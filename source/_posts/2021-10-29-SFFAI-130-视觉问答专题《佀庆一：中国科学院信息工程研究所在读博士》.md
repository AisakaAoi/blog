---
title: SFFAI 130 | 视觉问答专题《佀庆一：中国科学院信息工程研究所在读博士》
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: cf7c6a4b
date: 2021-10-29 02:11:15
tags:
---

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=466649172&bvid=BV1eL411K7Pi&cid=513865537&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

<!--more-->

SFFAI论坛网站已开放注册，详情点击查看：https://bbs.sffai.com/d/312

关注公众号：【人工智能前沿讲习】，回复【SFFAI130】获取讲者PPT资料，入交流群，推荐论文下载。

虽然复杂的视觉问答(VQA)模型取得了显著的成功，但它们倾向于只根据问题和答案之间的表面关联来回答问题。最近已经有不少解决这个语言偏见(language priors)现象的工作。然而，他们中的大多数往往直接输出最高概率的答案作为预测结果，而不再对输出答案的真实性进行检查。此外，他们只研究了图像和问题之间的交互，忽略了候选答案的语义。本期论坛我们邀请到了来自中国科学院信息工程研究所的佀庆一同学，他提出了一个基于视觉语义蕴涵的先筛选后重排序的渐进式框架。

***

### 讲者介绍

**佀庆一：**中国科学院信息工程研究所在读博士，主要研关注视觉问答系统中的Out-of-Distribution问题。目前已在IJCAI和ACL会议上发表论文2篇。

**报告题目：**再次检查：基于视觉蕴含的渐进式视觉问答

**报告摘要：**在本文中，我们提出了一个基于视觉蕴涵的先筛选后重排序（Select-And-Rerank）的渐进式框架。具体来说，我们首先筛选出与问题或图像相关的一些答案，然后通过视觉蕴涵任务对候选答案进行重排序，该重排序过程用以验证给定图像是否语义蕴含问题和每个候选答案的合成陈述。实验结果表明了我们提出的框架的有效性，它在语言偏见敏感的Out-of-Distribution数据集VQA-CP v2上建立了一个新的SoTA，提高了7.55%。

**论文标题：**Check It Again: Progressive Visual Question Answering via Visual Entailment

**分享亮点：**
1. 本文提出了一个先筛选后重排序的简单有效的框架来解决语言偏见问题，并实证研究了该框架每个模块的一系列设计选择的影响；
2. 本文将VQA任务重新规范成一个视觉蕴涵问题，令图像、问题和候选答案三者间的信息充分交互；
3. 本文提出的框架是普适的，可以很容易地与现有的VQA模型结合，进一步提高它们的能力。本框架以显著的性能提升建设了新SoTA。

***

### 论文推荐

1. Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases. [Clark et al., 2019]
    这篇论文是解决VQA语言偏见问题的ensemble-based methods中最有代表性的一篇工作，大幅度提升了多种任务下模型的OOD性能。

2. Counterfactual vqa: A cause-effect look at language bias. [Niu et al. 2021]
    这篇论文从因果效应的角度出发，将语言偏见作为问题对答案的直接因果效应（无图像参与），并通过从总因果效应（有图像参与）中减去直接语言效应来克服语言偏见。

3. Overcoming language priors with self-supervised learning for visual question answering. [Zhu et al. 2020]
    这篇论文是Data Augmentation methods中非常简单有效的一种方案。通过构造出图像和文本不匹配的pair引入辅助任务，达到平衡数据的目的。

4. Counterfactual samples synthesizing for robust visual question answering. [Chen et al. 2020]
    这篇文章通过对图像中关键object或文本中关键词的遮盖来构造新的反事实样本，平衡数据，是一种能够即插即用的Data Augmentation method。

5. Learning to contrast the counterfactual samples for robust visual question answering. [Liang et al., 2020]
    这篇工作是在上篇工作的改进，通过对比学习更好的利用构造出的样本与原样本之间的关系。

6. Lxmert: Learning cross-modality encoder representations from transformers. [Tan and Bansal 2019]
    这篇工作是在VQA领域最为常用的跨膜态预训练模型。

***

### 参考资料

> <https://www.bilibili.com/video/BV1eL411K7Pi/>
