---
title: Information Fusion | HiCMAE：用于自监督视听情感识别的分层对比掩码自编码器
categories:
  - 🌙进阶学习
  - ⭐脑机接口与混合智能研究团队（BCI团队）
  - 💫学习报告
abbrlink: f6d60895
date: 2024-09-29 00:31:48
tags:
---

该论文发表于Information Fusion（中科院1区，IF=14.7）,题目为《HiCMAE: Hierarchical Contrastive Masked Autoencoder for self-supervised Audio-Visual Emotion Recognition》。

中科院的孙立才博士为此文第一作者，中科院的刘斌副研究员及清华大学的陶建华教授为此文共同通讯作者。

论文链接:

[HiCMAE:HierarchicalContrastiveMaskedAutoencoderforself-supervisedAudio-VisualEmotionRecognition](https://www.sciencedirect.com/science/article/pii/S156625352400160X)

<!--more-->

***

### 论文摘要

近年来，随着人们越来越重视制造能够感知情感的智能机器，视听情感识别领域得到了广泛关注。目前，这一领域的研究大多采用监督学习方法。尽管取得了显著进展，但由于长期存在的数据稀缺问题，监督学习正面临发展瓶颈。为了突破这一限制，本文在自监督学习的最新发展推动下，提出了一种名为分层对比掩码自编码器（HiCMAE）的新框架。该框架通过在大量未标记的视听数据上进行广泛的自监督预训练，以推动视听情感识别的发展。

与以往的自监督视听表示学习技术不同，HiCMAE采用了两种主要的自监督学习方法进行预训练：掩码建模和对比学习。不同于其他方法只关注顶层表示而忽略中间层的指导，HiCMAE采取了一种多管齐下的策略来促进分层视听特征的学习，并提高学习表示的整体质量。具体来说，首先，它结合了编码器和解码器之间的分层跳跃连接，以促进中间层学习更有意义的表示，并支持掩码视听内容的重建。其次，对中间表示应用了分层跨模态对比学习，以逐步缩小视听模态之间的差距，并促进随后的跨模态融合。最后，在下游任务的微调过程中，HiCMAE采用分层特征融合策略，全面整合来自不同层次的多级特征。

为了验证HiCMAE的有效性，本文对包括分类和维度化视听情感识别任务在内的9个数据集进行了广泛的实验。实验结果表明，本文的方法在多个数据集上都显著优于现有的最先进的监督和自监督视听方法，证明了HiCMAE是一个强大的视听情感表示学习器。

***

### 研究背景

在早期的视听情感识别研究中，学者们主要致力于开发针对音频和视频模式的手工特征。随着深度学习技术的兴起，直接从原始数据中端到端学习特征的深度监督神经网络成为了新的趋势。尽管监督学习已经取得了巨大的进步，但它在很大程度上受限于对大量标记数据的依赖，这使得在数据稀缺的视听情感识别领域难以取得更进一步的突破。最近，自监督学习作为一种新的深度学习方法，已经在多个研究领域引起了革命，它能够从大量未标记的数据中学习到强大的特征表示。特别是，在自监督视觉表示学习中，屏蔽数据建模和对比学习已经取得了显著的成果。

***

### 方法与结果分析

本文介绍了一种名为分层对比掩码自编码器（HiCMAE）的新型自监督视听情感表示学习框架，用于视听情感识别。HiCMAE的训练包括两个阶段：在大量未标记的视听情感数据上进行自监督预训练，以及在有限的标记视听情感识别数据上进行下游微调。如图1所示，HiCMAE的自监督预训练过程主要由两部分组成：特定模态编码器、跨模态融合编码器，以及两个轻量级特定模态解码器。HiCMAE采用掩码数据建模（即掩码视听重建）和对比学习这两种主要的自监督方法进行预训练。此外，它还引入了一种多管齐下的策略，以促进预训练和微调过程中的分层视听特征学习，包括编码器和解码器之间的分层跳跃连接、分层跨模态对比学习，以及用于下游微调的分层特征融合。

如图1所示，HiCMAE继承了MAE的结构，采用了非对称编码器-解码器架构，以实现高效的自监督视听预训练。原始的音频和视频输入首先被嵌入到Tokens中，然后通过屏蔽机制过滤掉大部分Tokens。接下来，可见的（即未被屏蔽的）音频和视频Tokens由其对应的模态特定编码器和跨模态融合编码器进行处理。在特征编码完成后，可见的音频和视频Tokens会被填充可学习的屏蔽标记，然后通过轻量级模态特定解码器进行最终的音频和视频重建。与传统的屏蔽自动编码器不同，HiCMAE在视听编码器和解码器之间增加了分层跳跃连接，以更好地指导编码器在不同层次上的特征学习，并通过为解码器提供多级特征来加强屏蔽视听内容的重建。

{% asset_img 2.webp %}
<div align='center'>图1 HiCMAE模型的整体预训练流程</div>

***

### 编码器

在HiCMAE框架中，编码器由两部分组成：两个分别针对音频和视频的特定模态编码器，以及一个用于融合不同模态信息的跨模态编码器。其中，特定模态编码器的任务是挖掘并学习每种模态独有的特征，考虑到音频和视频信息的多样性，它们分别对各自的数据进行处理和分析。而跨模态融合编码器则是用来捕捉和理解不同模态之间的交互关系，它能够利用一种模态的信息去增强另一种模态的特征表示，从而在进行视听内容的屏蔽重建时，能够更加准确和全面。

{% asset_img 3.webp %}
<div align='center'>图2 编码器和解码器之间的分层跳跃连接的图示（以视频模态为例）</div>

***

### 解码器

在特征编码完成后，HiCMAE框架使用两个模态特定的解码器来完成最终的视听内容掩码重建。过去的研究已经证实，规模较小的模型就足以恢复被屏蔽的信息。因此，本研究也采用了这种策略，选择了基于Transformer的轻量级模型作为解码器，这些模型的层数比编码器少得多，从而在自监督预训练阶段大大减轻了计算负担。

如图2所示，与之前的视听屏蔽自动编码器相比，后者通常只处理最后一层编码器的输出，而忽略了其他层级的明确指导，HiCMAE则采用了编码器和中间层之间的分层跳跃连接。解码器层明确地指导了不同层级编码器特征的学习，并帮助解码器完成掩码视听内容的重建任务。具体来说，HiCMAE在解码器的每个Transformer层之前（第一层除外）都增加了MHCA层。

***

### 部分数据集上的对比实验

表1所示，在MAFW数据集上针对43种复合情绪的测试中，无论是视听模态，还是与目前最先进的T-MEP方法相比，本研究的方法在UAR（平均召回率）和WAR（加权平均召回率）方面都实现了略好的表现。在评估宏观平均F1分数和AUC（曲线下面积）时，本研究的最大模型相较于表现最好的T-ESFL方法有了显著的提升（即12.16% vs. 8.44%以及85.30% vs. 74.13%）。对于音频模态，本研究的方法在三个强大的基线方法上都取得了一致的适度改进。对于视觉模态，与之前的最佳结果相比，本研究的方法也取得了类似的性能提升。

<div align='center'>表1 与MAFW（43分类）上最先进的方法进行比较</div>
{% asset_img 4.webp %}

表2展示了本文在DFEW（7分类）数据集上与其他最新方法的性能对比。在视听结合的环境中，本文的HiCMAE-S模型明显超越了之前最佳的监督式方法T-MEP，平均提升率为+5.89%，加权平均提升率为+5.48%，同时在参数数量上减少了25%，而计算成本几乎持平。在单一模态的设置中，尽管本文的模型没有达到最先进的单一模态基线的性能，但仍然表现出了非常优秀的水平。特别值得注意的是，本文的方法在参数更少、计算量更小的情况下，更适合资源受限的环境。

<div align='center'>表2 与DFEW（7分类）最新方法的比较</div>
{% asset_img 5.webp %}

在表3中，本文展示了与CREMA-D（6分类）数据集上的最新方法的性能对比。在视听结合的设置中，本文首先发现本文的方法比两个自监督基线（MulTBase和Large）的性能要高出许多（+13%的WAR）。与两个版本的VQ-MAE-AV相比，本文的方法展现出了显著的性能提升，并且能够在单一阶段内完成训练。具体来说，即使是最小的HiCMAE-T版本也比最好的VQ-MAE-AV高出3.34%的WAR，同时使用的参数少了33%。随着模型规模的增大，HiCMAE-B将性能差距拉得更大（+4.49%的WAR），在该数据集上创造了新的最高标准。这些结果证明了本文旨在促进HiCMAE层次特征学习的多管齐下策略的有效性。最终，本文的方法也超越了高级的监督式基线（比如梯形网络）。

<div align='center'>表3 与CREMA-D（6分类）的最新方法进行比较</div>
{% asset_img 6.webp %}

如图3所示，本文的HiCMAE在多个数据集上显著优于现有的最先进监督或自监督视听方法。例如，在CREMAD（6分类）数据集上，HiCMAE以+4.49%的WAR超越了表现最佳的VQ-MAE-AV；在MAFW（11分类）数据集上，以+5.02%的WAR击败了最先进的T-MEP；在DFEW上，以+6.16%的WAR超越了最先进的T-MEP。此外，广泛的消融研究也验证了HiCMAE的多种设计选择的有效性。

{% asset_img 7.webp %}
<div align='center'>图3 在9个数据集上与最先进的视听方法进行比较。对于其他数据集，本文显示加权平均召回率（WAR）。</div>

本文使用了t-SNE技术来直观展示HiCMAE在CREMA-D数据集（包含6种情感类别）上学习到的特征嵌入空间。为了展现视听融合的效果，本文分别展示了单模态和多模态的特征嵌入空间。如图4所示，图中的每一行代表单模态或视听融合的嵌入空间，每一列则对应该数据集上的五个不同分组之一。通过观察这些图表，本文发现无论是纯音频还是纯视频模型，都能学习到很好的嵌入空间来区分不同种类的情感。同时，多模态嵌入空间相较于单模态的，具有更强的区分能力，这从它更为紧凑的同类聚集和更分离的异类分布就能明显看出。因此，这一比较结果从定性的角度验证了多模态融合在视听情感识别中的有效性。

{% asset_img 8.webp %}
<div align='center'>图4 CREMA-D（6分类）上的单模态和多模态嵌入空间可视化</div>

***

### 结论

本文提出了一个新颖的自监督框架——HiCMAE，作为利用大规模自监督预训练来解决当前监督方法所面临数据匮乏困境的初步尝试，并大幅推动了视听情感识别领域的发展。HiCMAE基于自监督学习的两种主要形式：遮蔽数据建模和对比学习。为了促进分层视听特征的学习，它采用了多管齐下的方法，包括编码器和解码器之间的分层跳跃连接、分层跨模态对比学习，以及用于下游任务微调的分层特征融合。通过9个涵盖分类和维度化任务的视听情感识别数据集的广泛实验，本文证明了HiCMAE的性能明显优于现有的最先进视听方法，展示了HiCMAE作为一个强大的视听情感表示学习器的实力。此外，广泛的消融研究和可视化分析也进一步证实了HiCMAE的有效性。本文希望这项工作能为视听情感识别领域的未来发展提供一些有价值的见解，并激发更多相关的研究工作。

***

### 原文链接

> <https://www.scholat.com/teamwork/showPostMessage.html?id=16250>
