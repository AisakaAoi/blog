---
title: SFFAI 33 | 呼奋宇：深度层次化图卷积神经网络
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 93907b52
date: 2019-06-16 07:16:49
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=57131318&bvid=BV1dx411d7JT&cid=99734531&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

首先介绍了弱监督图节点分类的任务和存在的挑战
接着分享了一些相关的经典论文
最后介绍了我们最新的IJCAI 2019的工作H-GCN：通过引入粗化和还原操作，设计了层次化的对称图神经网络，分别用9层和11层的网络取得了SOTA的结果。当训练数据极少时，相比之前方法至少有6个百分点的精度提升。

<!--more-->

***

### 讲者介绍

**呼奋宇：**中国科学院自动化研究所智能感知与计算研究中心17级直博生，目前的主要研究方向是图数据挖掘。已发表IJCAI2019论文一篇。

**报告题目：**Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification

**报告摘要：**节点分类是图数据挖掘中一个重要而基础的任务，已有的工作通常使用图卷积网络（GCNs）学出每个节点的表达并最终分类。但是大多数主流的图神经网络层数比较浅（仅有两层或三层），而且缺乏“图池化”机制，这使得每个节点只能获取有限的局部信息而无法感知全局信息，从而限制了模型的性能。现实中，由于海量的图数据量和高昂的标注成本，我们通常面临的是一种半监督节点分类的场景（即标记数据很少，待预测的节点和标记节点通常距离较远），这更加要求模型有足够大的感受野来感知到标记样本的信息。本次报告将以增大图网络的感受野为出发点，介绍我们在IJCAI2019上的最新工作。

**Spotlight：**
1. 通过对称的粗化（coarsening）和还原（refine）操作，设计了层次化的图神经网络，从而增大了模型的感受野。
2. 在半监督条件下效果显著，在基准数据集上取得state-of-the-art的结果；当训练数据极少时，相比之前方法至少有6个百分点的精度提升。

***

### 论文推荐

1. DeeperInsightsintoGraphConvolutionalNetworks forSemi-SupervisedLearning

    **推荐理由：**Kipf和Welling提出的GCN是一个经典的工作，它可以近似看成是把一阶邻居节点求和，进而得到中心节点的表达。但是GCN模型仅有2层，当层数加深后精度会明显下降。这篇文章证明了GCN本质上是一种拉普拉斯平滑，它会使得同一个连通分量里的所有节点的表达变得相似。因此当层数加深时，会造成“过度平滑”，即同一个连通分量里的节点会变得很难区分。

2. Hierarchical Graph Representation Learning with Differentiable Pooling

    **推荐理由：**这篇文章的想法很新颖，它的任务对对整体的图结构进行分类，为了得到全局的特征，它设计了一个层次化的图神经网络来逐层提取特征。本文提出了可微分池化的思想，对于每层神经网络得到的表达，它用学习的方式学出一个池化矩阵，并用该矩阵对每层得到的表达进行池化操作。

3. Hierarchical Representation Learning for Networks

    **推荐理由：**这篇文章的出发点也是为了增大感受野，因此设计了粗化机制逐层减小图数据的规模。对于粗化后得到的最小的特征图，作者先使用了传统的无监督学习的方法，如DeepWalk，Line得到每一个节点的表达，这样学出来的表达可以更多地捕获全局信息。然后，再使用对应的还原机制把特征图逐层还原，在每一次还原后都使用无监督方法继续更新节点的表达。在不同的实验数据库上均表明所提方法的有效性。

***

### 参考资料

> <https://www.bilibili.com/video/BV1dx411d7JT/>
> <https://bbs.sffai.com/d/82>
