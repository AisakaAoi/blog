---
title: >-
  BERTè®ºæ–‡é€æ®µç²¾è¯»-ã€ŠBERT: Pre-training of Deep Bidirectional Transformers for Language
  Understandingã€‹
categories:
  - ğŸŒ™è¿›é˜¶å­¦ä¹ 
  - â­è®ºæ–‡å¸¦è¯»
  - ğŸ’«ç²¾è¯»ç»å…¸
abbrlink: 72f6074f
date: 2021-11-19 02:46:11
tags:
---

### åŸæ–‡

{% pdf ./file/paper/2018-BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding.pdf %}

**BERT** è®ºæ–‡é“¾æ¥ï¼š<https://aclanthology.org/N19-1423.pdf>

BERT: è¿‘ 3 å¹´ NLP æœ€ç«

CV: å¤§æ•°æ®é›†ä¸Šçš„è®­ç»ƒå¥½çš„ NN æ¨¡å‹ï¼Œæå‡ CV ä»»åŠ¡çš„æ€§èƒ½ â€”â€” ImageNet çš„ CNN æ¨¡å‹

NLP: BERT ç®€åŒ–äº† NLP ä»»åŠ¡çš„è®­ç»ƒï¼Œæå‡äº† NLP ä»»åŠ¡çš„æ€§èƒ½

BERT å¦‚ä½•ç«™åœ¨å·¨äººçš„è‚©è†€ä¸Šçš„ï¼Ÿä½¿ç”¨äº†å“ªäº› NLP å·²æœ‰çš„æŠ€æœ¯å’Œæ€æƒ³ï¼Ÿå“ªäº›æ˜¯ BERT çš„åˆ›æ–°ï¼Ÿ

<!--more-->

***

### æ ‡é¢˜ + ä½œè€…

**BERT**: Pre-training of Deep Bidirectional Transformers for Language Understanding

pre-training: åœ¨ä¸€ä¸ªå¤§çš„æ•°æ®é›†ä¸Šè®­ç»ƒå¥½ä¸€ä¸ªæ¨¡å‹ pre-trainingï¼Œæ¨¡å‹çš„ä¸»è¦ä»»åŠ¡æ˜¯ç”¨åœ¨å…¶å®ƒä»»åŠ¡ training ä¸Šã€‚

deep bidirectional transformers: æ·±çš„åŒå‘ transformers

language understanding: æ›´å¹¿ä¹‰ï¼Œtransformer ä¸»è¦ç”¨åœ¨æœºå™¨ç¿»è¯‘ MT

BERT: ç”¨æ·±çš„ã€åŒå‘çš„ã€transformer æ¥åšé¢„è®­ç»ƒï¼Œç”¨æ¥åšè¯­è¨€ç†è§£çš„ä»»åŠ¡ã€‚

ä½œè€…ï¼šGoogle AI Languageï¼Œå†™ä½œæ—¶é—´çŸ­ï¼ˆå‡ ä¸ªæœˆï¼‰

***

### æ‘˜è¦

æ–°çš„è¯­è¨€è¡¨å¾æ¨¡å‹ BERT: **B**idirectional **E**ncoder **R**epresentations from **T**ransformersï¼ŒåŸºäº ELMo
Transformers æ¨¡å‹çš„åŒå‘ç¼–ç è¡¨ç¤º

ä¸ ELMo å’Œ GPT ä¸åŒï¼ŒBERT ä»æ— æ ‡æ³¨çš„æ–‡æœ¬ä¸­ï¼ˆjointly conditioning è”åˆå·¦å³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰é¢„è®­ç»ƒå¾—åˆ° æ— æ ‡æ³¨æ–‡æœ¬çš„ deep bidirectional representations

pre-trained BERT å¯ä»¥é€šè¿‡åŠ ä¸€ä¸ªè¾“å‡ºå±‚æ¥ fine-tuneï¼Œåœ¨å¾ˆå¤šä»»åŠ¡ï¼ˆé—®ç­”ã€æ¨ç†ï¼‰æœ‰ SOTA æ•ˆæœï¼Œè€Œä¸éœ€è¦å¯¹ç‰¹å®šä»»åŠ¡çš„åšæ¶æ„ä¸Šçš„ä¿®æ”¹ã€‚

GPT unidirectionalï¼Œä½¿ç”¨å·¦è¾¹çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ é¢„æµ‹æœªæ¥
BERT bidirectionalï¼Œä½¿ç”¨å·¦å³ä¾§çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

ELMo based on RNNs, down-stream ä»»åŠ¡éœ€è¦è°ƒæ•´ä¸€ç‚¹ç‚¹æ¶æ„
BERT based on Transformers, down-stream ä»»åŠ¡åªéœ€è¦è°ƒæ•´æœ€ä¸Šå±‚ã€‚
GPT, down-stream ä»»åŠ¡ åªéœ€è¦æ”¹æœ€ä¸Šå±‚ã€‚

æ‘˜è¦ç¬¬ä¸€æ®µï¼šå’Œå“ªä¸¤ç¯‡å·¥ä½œç›¸å…³ï¼ŒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
BERT æ˜¯åœ¨ GPT å’Œ ELMo çš„åŸºç¡€ä¸Šçš„æ”¹åŠ¨ã€‚

**æ‘˜è¦ç¬¬äºŒæ®µï¼šBERT çš„å¥½å¤„**
simple and empirically powerful, 11 NLP ä»»åŠ¡çš„SOTA, ç»å¯¹ç²¾åº¦ + ç›¸å¯¹ç²¾åº¦ï¼ˆæ¯”åˆ«äººå¥½å¤šå°‘ï¼‰

æ‘˜è¦å†™æ³•ï¼š
ç¬¬ä¸€æ®µï¼šæˆ‘å’Œå¦å¤– 2 ç¯‡ç›¸å…³å·¥ä½œçš„åŒºåˆ«ï¼Œæ”¹è¿›åœ¨å“ªé‡Œï¼Ÿ
ç¬¬äºŒæ®µï¼šæˆ‘çš„ç»“æœç‰¹åˆ«å¥½ï¼Œå¥½åœ¨ä»€ä¹ˆåœ°æ–¹ï¼Ÿ

Note: BERT è®ºæ–‡å†™ä½œå¥½ --> ç»å…¸ 
å·¥ä½œè´¨é‡ï¼šåˆ›æ–°æ€§ã€æ•ˆæœå¥½ -->  ç»å…¸

***

### å¯¼è¨€

å¯¼è¨€ç¬¬ä¸€æ®µï¼šæœ¬ç¯‡è®ºæ–‡å…³æ³¨çš„ç ”ç©¶æ–¹å‘çš„ä¸€äº›ä¸Šä¸‹æ–‡å…³ç³»
Language model pre-training å¯ä»¥æå‡ NLP ä»»åŠ¡çš„æ€§èƒ½ 
NLPä»»åŠ¡åˆ†ä¸¤ç±»ï¼šsentence-level tasks å¥å­æƒ…ç»ªè¯†åˆ«ã€ä¸¤ä¸ªå¥å­çš„å…³ç³»ï¼› token-level tasks NER (äººåã€è¡—é“å) éœ€è¦ fine-grained output

NLP é¢„è®­ç»ƒå¾ˆæ—©ä¹‹å‰å­˜åœ¨ï¼ŒBERT ä½¿ NLP é¢„è®­ç»ƒ å‡ºåœˆäº†ã€‚

å¯¼è¨€ç¬¬äºŒæ®µï¼šæ‘˜è¦ç¬¬ä¸€æ®µçš„æ‰©å……

pre-trained language representations ä¸¤ç±»ç­–ç•¥ï¼š
**åŸºäºç‰¹å¾çš„ ELMo** (æ„å»ºå’Œæ¯ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ç›¸å…³çš„ NN æ¶æ„ï¼›è®­ç»ƒå¥½çš„ç‰¹å¾ï¼ˆä½œä¸ºé¢å¤–çš„ç‰¹å¾ï¼‰ å’Œ è¾“å…¥ ä¸€èµ·æ”¾è¿›æ¨¡å‹)

**åŸºäºå¾®è°ƒå‚æ•°çš„ GPT**
æ‰€æœ‰çš„æƒé‡å‚æ•°æ ¹æ®æ–°çš„æ•°æ®é›†è¿›è¡Œå¾®è°ƒã€‚

ä»‹ç»åˆ«äººå·¥ä½œçš„ç›®çš„ï¼šé“ºå«è‡ªå·±æ–¹æ³•çš„å¥½

ELMo å’Œ GPT é¢„è®­ç»ƒæ—¶ ä½¿ç”¨ unidirectional langugage modelï¼Œä½¿ç”¨ç›¸åŒçš„ç›®æ ‡å‡½æ•°
è¯­è¨€æ¨¡å‹æ˜¯å•å‘çš„ã€é¢„æµ‹æœªæ¥ã€‚ä¸æ˜¯ç»™ç¬¬ ä¸€å¥ã€ç¬¬ä¸‰å¥ï¼Œé¢„æµ‹ç¬¬äºŒå¥

å¯¼è¨€ç¬¬ä¸‰æ®µï¼š
å½“å‰æŠ€æœ¯çš„å±€é™æ€§ï¼šæ ‡å‡†è¯­è¨€æ¨¡å‹æ˜¯ unidirectional å•å‘çš„ï¼Œé™åˆ¶äº†æ¨¡å‹æ¶æ„çš„é€‰æ‹©ã€‚

GPT ä»å·¦åˆ°å³çš„æ¶æ„ï¼Œåªèƒ½å°†è¾“å…¥çš„ä¸€ä¸ªå¥å­ä»å·¦çœ‹åˆ°å³ã€‚å¥å­æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡ï¼šä»å·¦çœ‹åˆ°å³ã€ä»å³çœ‹åˆ°å·¦ éƒ½åº”è¯¥æ˜¯åˆæ³•çš„ã€‚

token-level tasksï¼šé—®ç­” qa çœ‹å®Œæ•´ä¸ªå¥å­é€‰ç­”æ¡ˆï¼Œä¸æ˜¯ä»å·¦å¾€å³ä¸€æ­¥ä¸€æ­¥çœ‹ã€‚

å¦‚æœèƒ½ incorporate context from both directions çœ‹ä¸¤æ–¹å‘çš„ä¿¡æ¯ï¼Œèƒ½æå‡ ä»»åŠ¡æ€§èƒ½ã€‚

ç›¸å…³å·¥ä½œçš„å±€é™æ€§ï¼Œ+ è§£å†³å±€é™æ€§çš„æƒ³æ³• -- > å¯¼è¨€ç¬¬å››æ®µï¼š å¦‚ä½•è§£å†³ï¼Ÿ

BERT é€šè¿‡ MLM å¸¦æ©ç çš„è¯­è¨€æ¨¡å‹ ä½œä¸ºé¢„è®­ç»ƒçš„ç›®æ ‡ï¼Œæ¥å‡è½» è¯­è¨€æ¨¡å‹çš„å•å‘çº¦æŸã€‚inspired by the Close task 1953 

MLM å¸¦æ©ç çš„è¯­è¨€æ¨¡å‹åšä»€ä¹ˆå‘¢ï¼Ÿ
æ¯æ¬¡éšæœºé€‰è¾“å…¥çš„è¯æº tokens, ç„¶å mask å®ƒä»¬ï¼Œç›®æ ‡å‡½æ•°æ˜¯é¢„æµ‹è¢« masked çš„è¯ï¼›ç±»ä¼¼æŒ–ç©ºå¡«è¯ã€å®Œå½¢å¡«ç©ºã€‚

MLM å’Œ standard language model ï¼ˆåªçœ‹å·¦è¾¹çš„ä¿¡æ¯ï¼‰æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
MLM å¯ä»¥çœ‹ å·¦å³çš„ä¸Šä¸‹æ–‡ä¿¡æ¯, pre-train deep bidirectional transformer çš„åŸºç¡€ã€‚

BERT é™¤äº† MLM è¿˜æœ‰ä»€ä¹ˆï¼Ÿ 
NSP: next sentence prediction 
åˆ¤æ–­ä¸¤ä¸ªå¥å­æ˜¯éšæœºé‡‡æ ·çš„ or åŸæ–‡ç›¸é‚»ï¼Œå­¦ä¹  sentence-level çš„ä¿¡æ¯ã€‚

**æ–‡ç«  3ç‚¹ è´¡çŒ®ï¼š**
1.  bidirectional åŒå‘ä¿¡æ¯çš„é‡è¦æ€§
GPT åªç”¨äº† unidirectional ä¿¡æ¯ï¼›å¦å¤– Peter 2018 æŠŠä»å·¦çœ‹åˆ°å³ å’Œ ä»å³çœ‹åˆ°å·¦çš„æ¨¡å‹ç‹¬ç«‹è®­ç»ƒ + shallow concatenation æ‹¼åœ¨ä¸€èµ·ï¼›BERT åœ¨ bidirectional pre-training çš„åº”ç”¨æ›´å¥½

2. BERT é¦–ä¸ª å¾®è°ƒæ¨¡å‹ï¼Œåœ¨ sentence-level and token-level taskæ•ˆæœå¥½
å¥½çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä¸ç”¨å¯¹ç‰¹å®šä»»åŠ¡åšä¸€äº›æ¨¡å‹æ¶æ„çš„æ”¹åŠ¨

3. BERT å¼€æºï¼Œéšä¾¿ç”¨ã€‚

***

### ç»“è®º

è¿‘æœŸå®éªŒè¡¨æ˜ï¼Œéç›‘ç£çš„é¢„è®­ç»ƒæ¨¡å‹å¾ˆå¥½ï¼Œlow-resource ä»»åŠ¡ä¹Ÿèƒ½äº«å— benefit from æ·±çš„ç¥ç»ç½‘ç»œã€‚
æœ¬æ–‡è´¡çŒ®ï¼šæ‹“å±•å‰ä»»çš„ç»“æœåˆ° deep bidirectional architecturesï¼Œä½¿åŒæ ·çš„é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿå¤„ç†å¤§é‡çš„ NLP ä»»åŠ¡

**æœ¬æ–‡æ•…äº‹ï¼š**

2ä¸ªç›¸å…³å·¥ä½œï¼šELMo ç”¨äº† bidirectional ä¿¡æ¯ï¼Œä½†æ¶æ„ RNN è€ï¼›GPT æ¶æ„ Transformer æ–°ï¼Œä½†åªç”¨äº† unidirectional ä¿¡æ¯ã€‚

BERT = ELMo çš„ bidirectional ä¿¡æ¯ + GPT çš„æ–°æ¶æ„ transformer

How?
Language model ä»»åŠ¡ï¼šä¸æ˜¯é¢„æµ‹æœªæ¥ï¼Œè€Œæ˜¯å®Œå½¢å¡«ç©ºã€‚

å†™ä½œï¼šä¸¤ä¸ªç®—æ³•çš„ç»“åˆï¼Œä¸»è¦å·¥ä½œ -- è¯æ˜ åŒå‘æœ‰ç”¨

A + B ç¼åˆå·¥ä½œ or C æŠ€æœ¯è§£å†³ D é¢†åŸŸçš„é—®é¢˜ï¼Œä¸è¦è§‰å¾—æƒ³æ³•å°ã€ä¸å€¼å¾—å†™å‡ºæ¥ï¼›ç®€å•æœ´å®çš„å†™å‡ºæ¥ã€‚ç®€å•å¥½ç”¨ è¯´ä¸å®šä¼šå‡ºåœˆ

***

### ç›¸å…³å·¥ä½œ

2.1 Unsupervised Feature-based approaches
éç›‘ç£çš„åŸºäºç‰¹å¾è¡¨ç¤ºçš„å·¥ä½œï¼šè¯åµŒå…¥ã€ELMoç­‰

2.2 Unsupervised Fine-tuning approaches
éç›‘ç£çš„åŸºäºå¾®è°ƒçš„å·¥ä½œï¼šGPTç­‰

2.3 Transfer Learning from Supervised Data
åœ¨æœ‰æ ‡å·çš„æ•°æ®ä¸Šåšè¿ç§»å­¦ä¹ ã€‚

NLP æœ‰æ ‡å· çš„å¤§æ•°æ®é›†ï¼šnatural language inference and machine translation

CVåšçš„è¿˜ä¸é”™ï¼ŒImageNet è®­ç»ƒå¥½ã€å†åšè¿ç§»ã€‚

NLP è¡¨ç°ä¸é‚£ä¹ˆå¥½ï¼šCV å’Œ NLP ä»»åŠ¡çš„åŒºåˆ«ï¼ŒNLP æ•°æ®çš„ä¸è¶³ã€‚

**BERT çš„ä½œç”¨ï¼š**
NLP ä¸­ï¼Œåœ¨æ— æ ‡å·çš„å¤§é‡æ•°æ®é›†ä¸Šè®­ç»ƒçš„æ¨¡å‹æ•ˆæœ > æœ‰æ ‡å·ã€ä½†æ•°æ®é‡å°‘ä¸€äº›çš„æ•°æ®é›†ä¸Šè®­ç»ƒæ•ˆæœ

CV é‡‡ç”¨ BERT çš„æƒ³æ³•å˜›ï¼Ÿ
Yesï¼Œåœ¨å¤§é‡æ— æ ‡å·çš„å›¾ç‰‡ä¸Šè®­ç»ƒçš„æ¨¡å‹ï¼Œå¯èƒ½æ¯” æœ‰æ ‡å·çš„ ImageNet ç™¾ä¸‡å›¾ç‰‡ æ•ˆæœæ›´å¥½ã€‚

***

### BERT æ¨¡å‹

BERT æœ‰å“ªä¸¤æ­¥ï¼Ÿé¢„è®­ç»ƒ + å¾®è°ƒ
pre-training: ä½¿ç”¨ unlabeled data è®­ç»ƒ 
fine-tuning: å¾®è°ƒçš„ BERT ä½¿ç”¨ é¢„è®­ç»ƒçš„å‚æ•° åˆå§‹åŒ–ï¼Œæ‰€æœ‰çš„æƒé‡å‚æ•°é€šè¿‡ ä¸‹æ¸¸ä»»åŠ¡çš„ labeled data è¿›è¡Œå¾®è°ƒã€‚
æ¯ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¼šåˆ›å»ºä¸€ä¸ª æ–°çš„ BERT æ¨¡å‹ï¼Œï¼ˆç”±é¢„è®­ç»ƒå‚æ•°åˆå§‹åŒ–ï¼‰ï¼Œä½†æ¯ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡ä¼šæ ¹æ®è‡ªå·±ä»»åŠ¡çš„ labeled data æ¥å¾®è°ƒè‡ªå·±çš„ BERT æ¨¡å‹ã€‚

é¢„è®­ç»ƒå’Œå¾®è°ƒä¸æ˜¯ BERT çš„åˆ›æ–°ï¼ŒCVé‡Œç”¨çš„æ¯”è¾ƒå¤šã€‚

**ä½œè€…å…³äºé¢„è®­ç»ƒå’Œå¾®è°ƒçš„ä»‹ç» å¥½å—ï¼Ÿ**
å¥½ï¼å¦‚æœå‡è®¾è¯»è€…éƒ½çŸ¥é“è®ºæ–‡çš„æŠ€æœ¯ï¼Œè€Œåªä¸€ç¬”å¸¦è¿‡ï¼ˆç»™Refï¼‰ï¼Œä¸å¤ªå¥½ã€‚è®ºæ–‡å†™ä½œè¦è‡ªæ´½ï¼Œç®€å•çš„è¯´æ˜å°±å¥½ï¼Œé¿å…è¯»è€…ä¸çŸ¥é“é¢„è®­ç»ƒå’Œå¾®è°ƒï¼Œå¢åŠ ç†è§£æ–‡ç« çš„éšœç¢ã€‚

{% asset_img 1.webp %}

é¢„è®­ç»ƒçš„è¾“å…¥ï¼šunlabelled sentence pair
è®­ç»ƒ BERT çš„æƒé‡

ä¸‹æ¸¸ä»»åŠ¡ï¼šåˆ›å»ºåŒæ ·çš„ BERT çš„æ¨¡å‹ï¼Œæƒé‡çš„åˆå§‹åŒ–å€¼æ¥è‡ªäº é¢„è®­ç»ƒå¥½ çš„æƒé‡ã€‚
MNLI, NER, SQuAD ä¸‹æ¸¸ä»»åŠ¡æœ‰ è‡ªå·±çš„ labeled data, å¯¹ BERT ç»§ç»­è®­ç»ƒï¼Œå¾—åˆ°å„ä¸ªä¸‹æ¸¸ä»»åŠ¡è‡ªå·±çš„çš„ BERT ç‰ˆæœ¬ã€‚

**Model Architecture**

multi-layer bidirectional Transformer encoder 
ä¸€ä¸ªå¤šå±‚åŒå‘ Transformer çš„è§£ç å™¨ï¼ŒåŸºäº transfomer çš„è®ºæ–‡å’Œä»£ç ã€‚

å†™ä½œï¼šç¬¬ä¸‰ç« è¿™é‡Œä¸è®²å¯ä»¥ï¼›åœ¨ç¬¬äºŒç« ç›¸å…³å·¥ä½œåšä¸€å®šçš„ä»‹ç», i.e., L H

æ¨¡å‹è°ƒäº†å“ª 3 ä¸ªå‚æ•°?
L: transform blocksçš„ä¸ªæ•°
H: hidden size éšè—å±‚å¤§å°
A: è‡ªæ³¨æ„åŠ›æœºåˆ¶ multi-head ä¸­ head å¤´çš„ä¸ªæ•°

è°ƒäº† BERT_BASE ï¼ˆ1äº¿å‚æ•°ï¼‰å’Œ BERT_LARGE ï¼ˆ3.4äº¿å‚æ•°ï¼‰

Large æ¨¡å‹ å±‚æ•° L ç¿»å€ 12 -- 24ï¼›å®½åº¦ H 768 -- 1024
BERT æ¨¡å‹å¤æ‚åº¦å’Œå±‚æ•° L æ˜¯ linear, å’Œå®½åº¦ H æ˜¯ å¹³æ–¹å…³ç³»ã€‚
å› ä¸º æ·±åº¦ å˜æˆäº† ä»¥å‰çš„ä¸¤å€ï¼Œåœ¨å®½åº¦ä¸Šé¢ä¹Ÿé€‰æ‹©ä¸€ä¸ªå€¼ï¼Œä½¿å¾—è¿™ä¸ªå¢åŠ çš„å¹³æ–¹å¤§æ¦‚æ˜¯ä¹‹å‰çš„ä¸¤å€ã€‚

H = 16ï¼Œå› ä¸ºæ¯ä¸ª head çš„ç»´åº¦éƒ½å›ºå®šåœ¨äº†64ã€‚å› ä¸ºä½ çš„å®½åº¦å¢åŠ äº†ï¼Œæ‰€ä»¥ head æ•°ä¹Ÿå¢åŠ äº†ã€‚

BERT_base çš„å‚æ•°é€‰å– å’Œ GPT å·®ä¸å¤šï¼Œæ¯”è¾ƒæ¨¡å‹ï¼›BERT_large åˆ·æ¦œã€‚

**è¶…å‚æ•°æ¢ç®—æˆå¯å­¦ä¹ å‚æ•°çš„å¤§å°ï¼Œtransformeræ¶æ„çš„å›é¡¾**

å¯å­¦ä¹ å‚æ•°çš„æ¥æºï¼šåµŒå…¥å±‚ 30k * Hã€transformerå— L * H^2 * 12

åµŒå…¥å±‚ï¼š è¾“å…¥æ˜¯è¯çš„å­—å…¸å¤§å° 30kï¼Œè¾“å‡ºæ˜¯ H
å‚æ•°ï¼š30k ï¼ˆå­—å…¸å¤§å°ï¼‰ * H ï¼ˆhidden sizeï¼‰

åµŒå…¥å±‚çš„è¾“å‡ºä¼šè¿›å…¥ transformer å—ã€‚

transformer blocksï¼ˆH^2 * 12ï¼‰: self-attention mechanism ï¼ˆH^2 * 4ï¼‰+ MLPï¼ˆH^2 * 8ï¼‰

self-attention mechanism æœ¬èº«æ— å¯å­¦ä¹ å‚æ•°; multi-head self-attention mechanism è¦å¯¹ q, k, v åšæŠ•å½±ï¼Œæ¯ä¸€æ¬¡æŠ•å½±ç»´åº¦=64 --> A * 64 = Hã€‚
æ¯ä¸€ä¸ª q, k, v éƒ½æœ‰è‡ªå·±çš„æŠ•å½±çŸ©é˜µï¼Œåˆå¹¶æ¯ä¸ª head çš„æŠ•å½±çŸ©é˜µ --> q, k, v åˆ†åˆ«çš„ H * H çŸ©é˜µã€‚

å¾—åˆ°è¾“å‡ºåè¿˜ä¼šæœ‰ä¸€æ¬¡ H * H çš„æŠ•å½±ã€‚

Transformer block é‡Œçš„ self-attention å¯å­¦ä¹ å‚æ•° = H^ 2 * 4

MLP çš„ 2ä¸ªå…¨è¿æ¥å±‚ï¼š
ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚è¾“å…¥æ˜¯ Hï¼Œè¾“å‡ºæ˜¯ 4 * Hï¼›
ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚è¾“å…¥æ˜¯ 4 * Hï¼Œè¾“å‡ºæ˜¯ Hã€‚

æ¯ä¸€ä¸ªå‚æ•°çŸ©é˜µå¤§å° H * 4Hï¼ŒMLP ä¸­çš„å¯å­¦ä¹ å‚æ•° H^2 * 8

ä¸€ä¸ª transformer block çš„å‚æ•°é‡ H^2 * 12ï¼ŒL ä¸ª blocksï¼ŒL * H^2 * 12

**Input/Output Representations**

ä¸‹æ¸¸ä»»åŠ¡æœ‰å¤„ç†ä¸€ä¸ªå¥å­ or å¤„ç† 2 ä¸ªå¥å­ï¼ŒBERT èƒ½å¤„ç†ä¸åŒå¥å­æ•°é‡çš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œä½¿è¾“å…¥å¯ä»¥æ˜¯ a single sentence and a pair of sentences (Question answer)

a single sentence: ä¸€æ®µè¿ç»­çš„æ–‡å­—ï¼Œä¸ä¸€å®šæ˜¯çœŸæ­£ä¸Šçš„è¯­ä¹‰ä¸Šçš„ä¸€æ®µå¥å­ï¼Œå®ƒæ˜¯æˆ‘çš„è¾“å…¥å«åšä¸€ä¸ªåºåˆ— sequenceã€‚

A "sequence" åºåˆ—å¯ä»¥æ˜¯ä¸€ä¸ªå¥å­ï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸¤ä¸ªå¥å­ã€‚

BERT çš„è¾“å…¥å’Œ transformer åŒºåˆ«ï¼Ÿ
transformer é¢„è®­ç»ƒæ—¶å€™çš„è¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—å¯¹ã€‚ç¼–ç å™¨å’Œè§£ç å™¨åˆ†åˆ«ä¼šè¾“å…¥ä¸€ä¸ªåºåˆ—ã€‚
BERT åªæœ‰ä¸€ä¸ªç¼–ç å™¨ï¼Œä¸ºäº†ä½¿ BERT èƒ½å¤„ç†ä¸¤ä¸ªå¥å­çš„æƒ…å†µï¼Œéœ€è¦æŠŠä¸¤ä¸ªå¥å­å¹¶æˆä¸€ä¸ªåºåˆ—ã€‚

**BERT å¦‚ä½•åˆ‡è¯ï¼Ÿ**

WordPiece, æŠŠä¸€ä¸ªå‡ºç°æ¦‚ç‡ä½çš„è¯åˆ‡å¼€ï¼Œåªä¿ç•™ä¸€ä¸ªè¯å‡ºç°é¢‘ç‡é«˜çš„å­åºåˆ—ï¼Œ30k token ç»å¸¸å‡ºç°çš„è¯ï¼ˆå­åºåˆ—ï¼‰çš„å­—å…¸ã€‚
å¦åˆ™ï¼Œç©ºæ ¼åˆ‡è¯ --> ä¸€ä¸ªè¯æ˜¯ä¸€ä¸ª tokenã€‚æ•°æ®é‡æ‰“çš„æ—¶å€™ï¼Œè¯å…¸ä¼šç‰¹åˆ«å¤§ï¼Œåˆ°ç™¾ä¸‡çº§åˆ«ã€‚å¯å­¦ä¹ çš„å‚æ•°åŸºæœ¬éƒ½åœ¨åµŒå…¥å±‚äº†ã€‚

BERT çš„è¾“å…¥åºåˆ—å¦‚ä½•æ„æˆï¼Ÿ [ CLS ]  +  [ SEP ]

åºåˆ—å¼€å§‹: [ CLS ] è¾“å‡ºçš„æ˜¯å¥å­å±‚é¢çš„ä¿¡æ¯ sequence representation
BERT ä½¿ç”¨çš„æ˜¯ transformer çš„ encoderï¼Œself-attention layer ä¼šçœ‹è¾“å…¥çš„æ¯ä¸ªè¯å’Œå…¶å®ƒæ‰€æœ‰è¯çš„å…³ç³»ã€‚
å°±ç®— [ CLS ] è¿™ä¸ªè¯æ”¾åœ¨æˆ‘çš„ç¬¬ä¸€ä¸ªçš„ä½ç½®ï¼Œä»–ä¹Ÿæ˜¯æœ‰åŠæ³•èƒ½çœ‹åˆ°ä¹‹åæ‰€æœ‰çš„è¯ã€‚æ‰€ä»¥ä»–æ”¾åœ¨ç¬¬ä¸€ä¸ªæ˜¯æ²¡å…³ç³»çš„ï¼Œä¸ä¸€å®šè¦æ”¾åœ¨æœ€åã€‚

åŒºåˆ† ä¸¤ä¸ªåˆåœ¨ä¸€èµ·çš„å¥å­ çš„æ–¹æ³•ï¼š
- æ¯ä¸ªå¥å­å + [ SEP ] è¡¨ç¤º seperate
- å­¦ä¸€ä¸ªåµŒå…¥å±‚ æ¥è¡¨ç¤º æ•´ä¸ªå¥å­æ˜¯ç¬¬ä¸€å¥è¿˜æ˜¯ç¬¬äºŒå¥

[ CLS ] [Token1] â€¦â€¦ [Token n] [SEP] [Token1'] â€¦â€¦ [Token m]

æ¯ä¸€ä¸ª token è¿›å…¥ BERT å¾—åˆ° è¿™ä¸ª token çš„embedding è¡¨ç¤ºã€‚
å¯¹äº BERTï¼Œè¾“å…¥ä¸€ä¸ªåºåˆ—ï¼Œè¾“å‡ºä¸€ä¸ªåºåˆ—ã€‚

æœ€åä¸€ä¸ª transformer å—çš„è¾“å‡ºï¼Œè¡¨ç¤º è¿™ä¸ªè¯æº token çš„ BERT çš„è¡¨ç¤ºã€‚åœ¨åé¢å†æ·»åŠ é¢å¤–çš„è¾“å‡ºå±‚ï¼Œæ¥å¾—åˆ°æƒ³è¦çš„ç»“æœã€‚

{% asset_img 2.webp %}

For a given token, è¿›å…¥ BERT çš„è¡¨ç¤º = token æœ¬èº«çš„è¡¨ç¤º + segment å¥å­çš„è¡¨ç¤º + position embedding ä½ç½®è¡¨ç¤º

BERT åµŒå…¥å±‚ï¼šä¸€ä¸ªè¯æºçš„åºåˆ— --> ä¸€ä¸ªå‘é‡çš„åºåˆ— --> è¿›å…¥ transformer å—

Token embeddings:  è¯æºçš„embeddingå±‚ï¼Œæ•´æˆçš„embeddingå±‚ï¼Œ æ¯ä¸€ä¸ª token æœ‰å¯¹åº”çš„è¯å‘é‡ã€‚
Segement embeddings: è¿™ä¸ª token å±äºç¬¬ä¸€å¥è¯ Aè¿˜æ˜¯ç¬¬äºŒå¥è¯ Bã€‚
Position embeddings: è¾“å…¥çš„å¤§å° = è¿™ä¸ªåºåˆ—æœ€é•¿æœ‰å¤šé•¿ï¼Ÿ i.e., 1024 
Position embedding çš„è¾“å…¥æ˜¯ token è¯æºåœ¨è¿™ä¸ªåºåˆ— sequence ä¸­çš„ä½ç½®ä¿¡æ¯ã€‚ä»0å¼€å§‹ 1 2 3 4 --> 1024

{% asset_img 3.webp %}

BERT input representation = token embeddings + segment embeddings + position embeddings 

BERT çš„ segment embedding ï¼ˆå±äºå“ªä¸ªå¥å­ï¼‰å’Œ position embedding ï¼ˆä½ç½®åœ¨å“ªé‡Œï¼‰æ˜¯å­¦ä¹ å¾—æ¥çš„ï¼Œtransformer çš„ position embedding æ˜¯ç»™å®šçš„ã€‚

BERT å…³äº pre-train å’Œ fine-tune åŒæ ·çš„éƒ¨åˆ† == end

**3.1 Pre-training BERT**

é¢„è®­ç»ƒçš„ key factors: ç›®æ ‡å‡½æ•°ï¼Œé¢„è®­ç»ƒçš„æ•°æ®

**Task 1 MLM**
ä¸ºä»€ä¹ˆ bidirectional å¥½ï¼Ÿ MLM æ˜¯ä»€ä¹ˆï¼Ÿå®Œå½¢å¡«ç©º

ç”± WordPiece ç”Ÿæˆçš„è¯æºåºåˆ—ä¸­çš„è¯æºï¼Œå®ƒæœ‰ 15% çš„æ¦‚ç‡ä¼šéšæœºæ›¿æ¢æˆä¸€ä¸ªæ©ç ã€‚ä½†æ˜¯å¯¹äºç‰¹æ®Šçš„è¯æºä¸åšæ›¿æ¢ï¼Œi.e., ç¬¬ä¸€ä¸ªè¯æº [ CLS ] å’Œä¸­é—´çš„åˆ†å‰²è¯æº [SEP]ã€‚

å¦‚æœè¾“å…¥åºåˆ—é•¿åº¦æ˜¯ 1000 çš„è¯ï¼Œè¦é¢„æµ‹ 150 ä¸ªè¯ã€‚

MLM å¸¦æ¥çš„é—®é¢˜ï¼š**é¢„è®­ç»ƒå’Œå¾®è°ƒçœ‹åˆ°çš„æ•°æ®ä¸ä¸€æ ·**ã€‚é¢„è®­ç»ƒçš„è¾“å…¥åºåˆ—æœ‰ 15% [MASK]ï¼Œå¾®è°ƒæ—¶çš„æ•°æ®æ²¡æœ‰ [MASK].

15% è®¡åˆ’è¢« masked çš„è¯: 80% çš„æ¦‚ç‡è¢«æ›¿æ¢ä¸º [MASK], 10% æ¢æˆ random token,10% ä¸æ”¹å˜åŸ tokenã€‚ä½† T_i è¿˜æ˜¯è¢«ç”¨æ¥åšé¢„æµ‹ã€‚

80%, 10%, 10% çš„é€‰æ‹©ï¼Œæœ‰ ablation study in appendix

unchanged å’Œ å¾®è°ƒä¸­çš„æ•°æ®åº”è¯¥æ˜¯ä¸€æ ·çš„ã€‚

**Task 2 NSP Next Sentence Prediction**

åœ¨é—®ç­”å’Œè‡ªç„¶è¯­è¨€æ¨ç†é‡Œéƒ½æ˜¯**å¥å­å¯¹**ã€‚
å¦‚æœ BERT èƒ½å­¦ä¹ åˆ° sentence-level ä¿¡æ¯ï¼Œå¾ˆæ£’ã€‚

è¾“å…¥åºåˆ—æœ‰ 2 ä¸ªå¥å­ A å’Œ Bï¼Œ50% æ­£ä¾‹ï¼Œ50%åä¾‹
50% B åœ¨ A ä¹‹åï¼Œ50% æ˜¯ a random sentence éšæœºé‡‡æ ·çš„ã€‚

æ­£ä¾‹ï¼šè¿™ä¸ªäººè¦å»ä¸€ä¸ªå•†åº—ï¼Œç„¶åä»–ä¹°äº†ä¸€åŠ ä»‘çš„ç‰›å¥¶ã€‚IsNext
åä¾‹ï¼šè¿™ä¸ªäººå»äº†å•†åº—ï¼Œç„¶åä¼é¹…æ˜¯ä¸€ç§ä¸èƒ½é£çš„é¸Ÿã€‚NotNext

flight ## less, flightless å‡ºç°æ¦‚ç‡ä¸é«˜ï¼ŒWordPiece åˆ†æˆäº† 2 ä¸ªå‡ºç°é¢‘ç‡é«˜çš„å­åºåˆ—ï¼Œ## è¡¨ç¤º less æ˜¯ flightless çš„ä¸€éƒ¨åˆ†ã€‚

**Pre-training data**

2 ä¸ªæ•°æ®é›†ï¼šBooksCorpus (800 M) + English Wikipedia (2500 M)
ä½¿ç”¨ä¸€ç¯‡ä¸€ç¯‡æ–‡ç« ï¼Œè€Œä¸æ˜¯éšæœºæ‰“æ–­çš„å¥å­ã€‚ a document-level corpus rather than a shuffled sentence-level corpus

transformer å¯ä»¥å¤„ç†è¾ƒé•¿çš„åºåˆ—ï¼Œä¸€æ•´ä¸ªæ–‡æœ¬çš„è¾“å…¥ï¼Œæ•ˆæœä¼šå¥½ä¸€äº›ã€‚

**3.2 Fine-tuning BERT**

ç”¨ BERT åšå¾®è°ƒçš„ä¸€èˆ¬åŒ–çš„ä»‹ç»ã€‚

BERT å’Œä¸€äº›åŸºäºencoder-decoderçš„æ¶æ„ä¸ºä»€ä¹ˆä¸ä¸€æ ·ï¼Ÿtransformer æ˜¯encoder-decoderã€‚

æ•´ä¸ªå¥å­å¯¹è¢«æ”¾åœ¨ä¸€èµ·è¾“å…¥ BERTï¼Œself-attention èƒ½å¤Ÿåœ¨ä¸¤ä¸ªå¥å­ä¹‹é—´ç›¸äº’çœ‹ã€‚BERT æ›´å¥½ï¼Œä½†ä»£ä»·æ˜¯ ä¸èƒ½åƒ transformer åšæœºå™¨ç¿»è¯‘ã€‚

åœ¨encoder-decoderçš„æ¶æ„ï¼Œç¼–ç å™¨çœ‹ä¸åˆ°è§£ç å™¨çš„ä¸œè¥¿ã€‚

**BERT åš ä¸‹æ¸¸ä»»åŠ¡**

æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡ï¼Œè®¾è®¡æˆ‘ä»¬ä»»åŠ¡ç›¸å…³çš„è¾“å…¥å’Œè¾“å‡ºã€‚

å¥½å¤„ï¼šæ¨¡å‹ä¸æ€ä¹ˆå˜ï¼ŒåŠ ä¸€ä¸ªè¾“å‡ºå±‚ softmax å¾—åˆ° æ ‡å· label

**æ€ä¹ˆæ ·æŠŠè¾“å…¥æ”¹æˆæƒ³è¦çš„å¥å­å¯¹ï¼Ÿ**
- æœ‰ä¸¤ä¸ªå¥å­çš„è¯ï¼Œå½“ç„¶å°±æ˜¯å¥å­ A å’Œ Bã€‚
- åªæœ‰ä¸€ä¸ªå¥å­çš„è¯ï¼Œè¦åšå¥å­åˆ†ç±»çš„è¯ï¼Œ B æ²¡æœ‰ã€‚æ ¹æ®ä¸‹æ¸¸ä»»åŠ¡çš„è¦æ±‚ï¼Œè¦ä¹ˆæ˜¯ [CLS] representation is fed into an output layer for classification æ‹¿åˆ°ç¬¬ä¸€ä¸ªè¯æº [CLS] å¯¹åº”çš„è¾“å‡ºåšåˆ†ç±» such as entailment or sentiment analysisï¼Œæˆ–è€…æ˜¯ the token representations are fed into an output layer for token-level tasks æ‹¿åˆ°å¯¹åº”é‚£äº›è¯æºçš„è¾“å‡ºåš sequence tagging or question answering è¾“å‡ºã€‚

å¾®è°ƒæ¯”é¢„è®­ç»ƒä¾¿å®œã€‚TPU 1 hour, GPU a few hours.

**Section 4 å…·ä½“å¯¹æ¯ä¸€ä¸ªä¸‹æ¸¸ä»»åŠ¡æ˜¯æ€ä¹ˆæ ·æ„é€ è¾“å…¥è¾“å‡º**

***

### å®éªŒ

**4.1 GLUE General Language Understanding Evaluation**
- å¤šä¸ªæ•°æ®é›†
- sentence-level tasks

[CLS] çš„ BERT è¾“å‡ºè¡¨ç¤º + ä¸€ä¸ªè¾“å‡ºå±‚ Wï¼Œsoftmax åˆ†ç±»å¾—åˆ° label
log(softmax(CW^T)

**4.2 SQuAD v1.1**
Standford Question Answering Dataset

QA é—®ç­”ï¼šç»™ä¸€æ®µæ–‡å­—ï¼Œé—®ä¸€ä¸ªé—®é¢˜ï¼Œæ‘˜å½•ç­”æ¡ˆã€‚--> åˆ¤æ–­ç­”æ¡ˆçš„å¼€å§‹å’Œç»“å°¾ã€‚
å¯¹æ¯ä¸ªè¯æº tokenï¼Œåˆ¤æ–­æ˜¯ä¸æ˜¯ç­”æ¡ˆçš„å¼€å§‹orç»“å°¾

å­¦ 2 ä¸ªå‘é‡ S å’Œ Eï¼Œåˆ†åˆ«å¯¹åº”è¿™ä¸ªè¯æº token æ˜¯ç­”æ¡ˆå¼€å§‹è¯çš„æ¦‚ç‡ å’Œ æ˜¯ç­”æ¡ˆç»“å°¾è¯çš„æ¦‚ç‡ã€‚

å…·ä½“è®¡ç®— æ¯ä¸ª token æ˜¯ç­”æ¡ˆå¼€å§‹çš„æ¦‚ç‡ï¼Œç»“å°¾è¯ç±»ä¼¼ Eã€‚
S å’Œ ç¬¬äºŒå¥è¯çš„æ¯ä¸ªè¯æº token ç›¸ä¹˜ + softmaxï¼Œå¾—åˆ°å½’ä¸€åŒ–çš„æ¦‚ç‡ã€‚
P_i = e ^ ( S * T_i ) / \sigma_j ( e ^ ( S * T_j ) )

æœ¬æ–‡å¾®è°ƒæ—¶ï¼Œæ•°æ®æ‰«ä¸‰éï¼Œepochs = 3, lr = 5e-5, batch_size = 32

å¤§å®¶å®éªŒå‘ç°ï¼šç”¨ BERT åšå¾®è°ƒçš„æ—¶å€™ï¼Œç»“æœéå¸¸ä¸ç¨³å®šã€‚åŒæ ·çš„å‚æ•°ï¼ŒåŒæ ·çš„æ•°æ®é›†ï¼Œè®­ç»ƒ 10 éï¼Œvariance æ–¹å·®ç‰¹åˆ«å¤§ã€‚

å…¶å®å¾ˆç®€å•ï¼Œepochs ä¸å¤Ÿï¼Œ3 å¤ªå°äº†ï¼Œå¯èƒ½è¦å¤šå­¦ä¹ å‡ éä¼šå¥½ä¸€ç‚¹ã€‚

adam çš„ä¸å®Œå…¨ç‰ˆ åœ¨é•¿æ—¶é—´è®­ç»ƒçš„ BERT æ²¡é—®é¢˜ï¼Œè®­ç»ƒæ—¶é—´ä¸å¤Ÿï¼Œéœ€è¦ adam çš„å®Œå…¨ç‰ˆã€‚

**4.3 SQuAD v2.0  è¡¨ç°ä¹Ÿå¾ˆä¸é”™**

**4.4 SWAG**

Situations With Adversarial Generations åˆ¤æ–­ä¸¤ä¸ªå¥å­ä¹‹é—´çš„å…³ç³»ï¼ŒBERT å’Œä¹‹å‰çš„è®­ç»ƒæ²¡å¤šå¤§åŒºåˆ«ï¼Œæ•ˆæœå¥½ã€‚

æ€»ç»“ï¼šBERT åœ¨ä¸ä¸€æ ·çš„æ•°æ®é›†ä¸Šï¼Œç”¨èµ·æ¥å¾ˆæ–¹ä¾¿ï¼Œæ•ˆæœå¾ˆå¥½ã€‚
è¾“å…¥è¡¨ç¤ºæˆâ€œä¸€å¯¹å¥å­çš„å½¢å¼â€ï¼Œæœ€åæ‹¿åˆ° BERT å¯¹åº”çš„è¾“å‡ºï¼Œç„¶ååŠ ä¸€ä¸ªè¾“å‡ºå±‚ softmaxï¼Œå®Œäº‹äº†ã€‚

BERT å¯¹ NLP æ•´ä¸ªé¢†åŸŸçš„è´¡çŒ®éå¸¸å¤§ï¼Œæœ‰å¤§é‡çš„ä»»åŠ¡ç”¨ä¸€ä¸ªç›¸å¯¹ç®€å•ã€åªæ”¹æ•°æ®è¾“å…¥å½¢å¼å’Œæœ€ååŠ ä¸€ä¸ªè¾“å‡ºå±‚ï¼Œå°±å¯ä»¥æ•ˆæœå¾ˆä¸é”™ã€‚

**5 Ablation studies**

çœ‹ BERT æ¯ä¸€ä¸ªç»„æˆéƒ¨åˆ†çš„è´¡çŒ®ã€‚

æ²¡æœ‰ NSP
LTR ä»å·¦çœ‹åˆ°å³ï¼ˆæ—  MLM ï¼‰ & æ²¡æœ‰ NSP
LTR ä»å·¦çœ‹åˆ°å³ï¼ˆæ—  MLM ï¼‰ & æ²¡æœ‰ NSP + BiLSTM ï¼ˆä»ELMoæ¥çš„æƒ³æ³•ï¼‰

å»æ‰ä»»ä½•ä¸€ä¸ªç»„æˆéƒ¨åˆ†ï¼ŒBERTçš„æ•ˆæœéƒ½ä¼šæœ‰æ‰“æŠ˜ï¼Œç‰¹åˆ«æ˜¯ MRPCã€‚

**5.2 Effect of Model Size**

BERT_base 110 M å¯å­¦ä¹ å‚æ•°
BERT_large 340 M å¯å­¦ä¹ å‚æ•°

NLPç•Œè®¤ä¸º æ¨¡å‹è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ã€‚BERT é¦–å…ˆè¯æ˜äº†å¤§åŠ›å‡ºå¥‡è¿¹ï¼Œå¼•å‘äº†æ¨¡å‹â€œå¤§â€æˆ˜

ç°åœ¨ï¼šGPT-3 1000 äº¿å¯å­¦ä¹ å‚æ•°

**5.3 Feature-based Approach with BERT**

æ²¡æœ‰å¾®è°ƒçš„ BERTï¼Œå°†pre-trained å¾—åˆ°çš„ BERT ç‰¹å¾ä½œä¸ºä¸€ä¸ªé™æ€çš„ç‰¹å¾è¾“å…¥ï¼Œæ•ˆæœæ²¡æœ‰ + å¾®è°ƒå¥½

å–ç‚¹ï¼šç”¨ BERT éœ€è¦å¾®è°ƒã€‚

***

### è¯„è®º

å†™ä½œï¼š
- å…ˆå†™ BERT å’Œ ELMo (bidirectional + RNN)ã€GPT (unidirectional + transformer) çš„åŒºåˆ«
- ä»‹ç» BERT æ¨¡å‹
- BERT å®éªŒè®¾ç½®ã€æ•ˆæœå¥½
- ç»“è®ºçªå‡º 'bidirectional' è´¡çŒ®
- æ–‡ç«  1ä¸ªå–ç‚¹ï¼Œå®¹æ˜“è®°ã€‚

**ä½† BERT æ˜¯å¦è¦é€‰æ‹©  'bidirectional'  åŒå‘æ€§å‘¢ï¼Ÿ**
å¯ä»¥å†™ï¼Œä½†ä¹Ÿè¦å†™ åŒå‘æ€§å¸¦æ¥çš„ä¸è¶³æ˜¯ä»€ä¹ˆï¼Ÿ

é€‰æ‹©æœ‰å¾—æœ‰å¤±ã€‚
GPT ç”¨çš„æ˜¯ decoder
BERT ç”¨çš„æ˜¯ encoderï¼Œä¸å¥½åšgenerative tasksï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æœ¬æ‘˜è¦ã€‚

åˆ†ç±»é—®é¢˜åœ¨ NLP æ›´å¸¸è§ã€‚
NLP ç ”ç©¶è€…å–œæ¬¢ BERTï¼Œè¾ƒå®¹æ˜“çš„åº”ç”¨åœ¨ NLP ä¸­è‡ªå·±æƒ³è§£å†³çš„é—®é¢˜ã€‚

BERTï¼Œå®Œæ•´çš„è§£å†³é—®é¢˜çš„æ€è·¯ ---- å¤§å®¶å¯¹ DL çš„æœŸæœ›
è®­ç»ƒä¸€ä¸ªå¾ˆæ·±ã€å¾ˆå®½çš„æ¨¡å‹ï¼Œåœ¨ä¸€ä¸ªå¾ˆå¤§çš„æ•°æ®é›†ä¸Šé¢„è®­ç»ƒå¥½ï¼›è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°å¯ä»¥è§£å†³å¾ˆå¤šå°çš„é—®é¢˜ï¼Œé€šè¿‡å¾®è°ƒæå‡å°æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚

è¿™ä¸ªæ¨¡å‹æ‹¿å‡ºæ¥ä¹‹åå¯ä»¥ç”¨åœ¨å¾ˆå¤šå°çš„é—®é¢˜ä¸Šï¼Œèƒ½å¤Ÿé€šè¿‡å¾®è°ƒæ¥å…¨é¢æå‡è¿™äº›å°æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚è¿™ä¸ªåœ¨è®¡ç®—æœºè§†è§‰é‡Œé¢æˆ‘ä»¬ç”¨äº†å¾ˆå¤šå¹´äº†ã€‚

BERT æŠŠ CV çš„å¥—è·¯æ¬åˆ°äº† NLPï¼Œ1ä¸ª3äº¿å‚æ•°çš„æ¨¡å‹ï¼Œå±•ç¤ºï¼šæ¨¡å‹è¶Šå¤§ã€æ•ˆæœè¶Šå¥½ã€‚å¤§åŠ›å‡ºå¥‡è¿¹ã€‚

ä¸ºä»€ä¹ˆ BERT è¢«è®°ä½ï¼Ÿ
BERT ç”¨äº† ELMo, GPT æ›´å¤§çš„è®­ç»ƒæ•°æ®é›†ï¼Œæ•ˆæœæ›´å¥½ï¼›BERE ä¹Ÿè¢«æ›´å¤§çš„è®­ç»ƒæ•°æ®é›†å’Œæ›´å¤§çš„æ¨¡å‹è¶…è¶Šã€‚
BERT çš„å¼•ç”¨ç‡æ˜¯ GPT çš„ 10 å€ï¼Œå½±å“åŠ› âœ”

***

### å‚è€ƒèµ„æ–™

> <https://www.bilibili.com/video/BV1PL411M7eQ>
