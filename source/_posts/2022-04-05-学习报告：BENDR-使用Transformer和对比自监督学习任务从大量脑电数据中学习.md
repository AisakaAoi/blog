---
title: BENDR-使用Transformer和对比自监督学习任务从大量脑电数据中学习
categories:
  - 🌙进阶学习
  - ⭐脑机接口与混合智能研究团队（BCI团队）
  - 💫学习报告
abbrlink: b59d8b34
date: 2022-04-05 22:06:16
tags:
---

总结目前脑机接口取到的一些进展，大体上可以分为两方面：

硬件方面有芯片、脑电采集设备和边缘计算方面的进展；由于我是研究脑电信号算法方向的，所以更关注的是算法的发展，本文介绍了一个多伦多大学提出的一种脑电预训练模型-BENDR，作者在一个庞大的EEG数据集上构造预训练模型，并且在其各种下游任务都取得了比基线更好的结果。

了解深度学习的人应该都对预训练模型不陌生，因为预训练+微调的方式是现在许多领域比如NLP和CV的主流方法。其实，在一开始做脑电的时候，已经很疑惑了，为什么不用预训练模型的方法来做脑电呢，其实问题首先就在没有大规模的脑电数据，而更关键的是脑电数据是生理信号，不同的人或者同一个人不同时间的脑电信号都会有差异，不同设备采集的信号也会有所差异，所以很难能得到一个有效的能作为预训练的脑电大数据集。

所以，实际上训练预训练模型肯定已经有许多研究人员想过要做的工作了，但可能碍于上面种种问题。我了解到，论文作者的这项研究，使用的是天普大学提供的公开脑电数据集，这个数据集非常庞大，包含有30000个临床脑电图，解决了数据集不够的问题。

{% asset_img 1.webp %}

<!--more-->

在 BENDR: UsingTransformers and an Contrastive Self-Supervised Learning Task to Learn fromMassive Amount of EEG Data 一文中，研究人员将 DNN 从原始EEG序列提取有用的特征，并对这些特征进行分类。但是由于个体之间存在差异，导致EEG数据也存在很大的差异，所以不同模型类型的分类性能可能存在很大差异。

{% asset_img 2.webp %}
<div align='center'>图1 BENDR整体架构</div>

针对每个下游任务，使用传统的监督训练（留一被试）来训练模型。它们首先通过预训练开发的模型权重，再在具体的下游任务中微调模型，实验结果如下表。

{% asset_img 3.webp %}
<div align='center'>图2 实验结果</div>

但是，我很好奇他们是否有解决关于不同被试者脑电数据之间存在“数据鸿沟“的问题，或者只是说由于数据集实在过于庞大，这些差异相比大数据的庞大而显得足够小，换言之，大数据是否已经无形解决掉了数据差异或者噪声问题？这关乎我们研究的方向，就是需不需要研究被试的差异问题，还是说我们直接把大数据往模型一扔，然后就交给网络去解决算了。

如果未来有更多的人在他们的研究中证实预训练方法在脑电领域的有效性，这样脑电领域可能将迎来又一次的高速发展期，各种下游任务将有更好的提升，也会推动脑机接口的实用化发展。

虽然，预训练+微调的方式会让脑电领域迎来更好的发展，但是要谨防出现目前其他领域已经存在的一些问题，就是陷入大数据无法自拔，太过依赖于使用大数据来解决问题，陷入喂数据疯狂训练的境地。

***

### 原文链接

> <https://www.scholat.com/teamwork/showPostMessage.html?id=11471>
