---
title: SFFAI 71 | 泛化误差专题《骆轩源：论非凸学习下有噪声梯度方法的泛化误差上界》
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 973dd99b
date: 2020-07-05 05:13:45
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=669008381&bvid=BV1aa4y1E78s&cid=217232926&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<!--more-->

关注公众号：【人工智能前沿讲习】，回复【SFFAI71】获取讲者PPT资料，入交流群，推荐论文下载。

训练集往往只是数据总体的一部分，无法包含所有可能的情况，训练出的学习算法在训练集和非训练集上的表现会是不一样的，我们使用泛化误差来度量这一差距，这也是机器学习理论最重要的问题之一。本期论坛我们邀请到了来自清华大学的骆轩源同学，分享他在ICLR2020发表的一个关于泛化误差上界的工作。

***

### 讲者介绍

**骆轩源：**清华大学交叉信息研究院一年级硕士生，导师为交叉信息研究院李建副教授。本科毕业于清华大学计算机系。高中参加信息学竞赛，曾获NOI 2014金牌，进入国家集训队，保送清华大学。目前主要研究方向为机器学习理论。在ICLR 2020发表论文一篇。

**报告题目：**论非凸学习下有噪声梯度方法的泛化误差上界

**报告摘要：**泛化误差也即一个学习算法在训练集和真实未知数据集上表现的差距，是机器学习理论最重要的问题之一。基于该文新提出Bayes-Stability理论框架，作者得到了比前人更优的SGLD的期望泛化误差上界O(G/n)，其中G和n分别是训练路径上梯度的范数之和以及训练集大小。同时该上界对于非高斯噪音、动量加速、和滑动平均等扩展情况一样成立。除此之外，该文还证明了连续时间朗之万运动 (CLD) 任意时刻的Log-Sobolev不等式，基于该结论，作者证明了在加入l2正则化之后，CLD的期望泛化误差以O(1/n)的速度减小，并且该上界可以与训练时间无关。

**Spotlight：**
1. 本文提出了新的Bayes-Stability理论框架，基于此框架可以得到比前人更紧的界；
2. 本文提出的SGLD期望泛化上界严格紧于前人工作，同时本文的证明更简单，也能很容易地扩展到其他情况比如mini-batch, 动量加速，非高斯噪音等等变种；
3. 本文提出一个CLD的上界，收敛速度为O(1/n)，且不随训练时间增加而增长到正无穷。

***

### 论文推荐

#### 3篇领域经典

1. Understanding Deep Learning Requires Rethinking Generalization. (ICLR 2017)

    **推荐理由：**ICLR 2017的最佳会议论文。作者通过一些很有意思的实验（比如random label实验）来试图反驳之前的机器学习理论，认为之前的一些泛化上界并不能解释深度学习的成功。很有趣的一个文章，也被后来大部分做泛化的工作引用了。

2. Train faster, generalize better Stability of stochastic gradient descent. (ICML 2016)

    **推荐理由：**比较经典的关于SGD的泛化误差的论文。使用的是一个经典理论框架叫做algorithm stability。其结论为训练越快，泛化越好。

3. Neural Tangent Kernel Convergence and Generalization in Neural Networks.

    **推荐理由：**本文提出神经正切核（NTK）。NTK算是一个比较大的理论突破，其建立了over-parameterized neural network与Kernel Method的关系。而kernel相对于神经网络的复杂函数来说，性质更清晰一些，说不定更容易分析一些。后续有很多工作基于NTK，比如可以证明梯度下降能找到深度神经网路的全局最优解等等。
 

#### 3篇领域前沿

4. Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks.（ICML 2019）

    **推荐理由：**本文精细地分析了两层 (超级宽) 神经网络的优化和泛化。结论非常有意义。

5. Sharper Bounds for Uniformly Stable Algorithms (COLT 2020)

    **推荐理由：**本文把传统的uniform stability的high probability bound基本做到了最优。

6. Gradient Descent Maximizes the Margin of Homogeneous Neural Networks (ICLR 2020)

    **推荐理由：**本文的理论分析表明，离散的梯度下降和连续的梯度流在最小化齐次神经网络的逻辑损失或交叉熵损失的过程中，也会逐渐增大标准化分类间隔的一个光滑版变种。经过足够长的训练，标准化分类间隔及其光滑版变种还将收敛到同一极限，并且该极限和一个分类间隔最大化问题的KKT点处值相等。本文的结果极大地推广了前人在线性网络上得到的类似结果；相比于前人在齐次网络上的研究，也在使用的假设更弱的情况下给出了更量化的结果。

***

### 参考资料

> <https://www.bilibili.com/video/BV1aa4y1E78s/>
