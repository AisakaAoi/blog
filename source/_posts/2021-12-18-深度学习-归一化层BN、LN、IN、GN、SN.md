---
title: æ·±åº¦å­¦ä¹ -å½’ä¸€åŒ–å±‚BNã€LNã€INã€GNã€SN
categories:
  - ğŸŒ™è¿›é˜¶å­¦ä¹ 
  - â­äººå·¥æ™ºèƒ½ Artificial Intelligence
  - ğŸ’«æ·±åº¦å­¦ä¹ åŸºæœ¬æ¦‚å¿µ Deep Learning Basic Concepts
abbrlink: 928002b2
date: 2021-12-18 16:36:56
tags:
---

### ç»¼è¿°

å½’ä¸€åŒ–å±‚ï¼Œç›®å‰ä¸»è¦æœ‰è¿™å‡ ä¸ªæ–¹æ³•ï¼Œ**Batch Normalization**ï¼ˆ2015å¹´ï¼‰ã€**Layer Normalization**ï¼ˆ2016å¹´ï¼‰ã€**Instance Normalization**ï¼ˆ2017å¹´ï¼‰ã€**Group Normalization**ï¼ˆ2018å¹´ï¼‰ã€**Switchable Normalization**ï¼ˆ2018å¹´ï¼‰

å°†è¾“å…¥çš„å›¾åƒ shape è®°ä¸º [N, C, H, W]ï¼Œè¿™å‡ ä¸ªæ–¹æ³•ä¸»è¦çš„åŒºåˆ«å°±æ˜¯åœ¨ï¼š
- **BatchNorm**æ˜¯åœ¨ batch ä¸Šï¼Œå¯¹ NHW åšå½’ä¸€åŒ–ï¼Œå°±æ˜¯å¯¹æ¯ä¸ªå•ä¸€é€šé“è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œè¿™æ ·åšå¯¹å° batchsize æ•ˆæœä¸å¥½
- **LayerNorm**åœ¨é€šé“æ–¹å‘ä¸Šï¼Œå¯¹ CHW å½’ä¸€åŒ–ï¼Œå°±æ˜¯å¯¹æ¯ä¸ªæ·±åº¦ä¸Šçš„è¾“å…¥è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¸»è¦å¯¹ RNN ä½œç”¨æ˜æ˜¾
- **InstanceNorm**åœ¨å›¾åƒåƒç´ ä¸Šï¼Œå¯¹ HW åšå½’ä¸€åŒ–ï¼Œå¯¹ä¸€ä¸ªå›¾åƒçš„é•¿å®½å³å¯¹ä¸€ä¸ªåƒç´ è¿›è¡Œå½’ä¸€åŒ–ï¼Œç”¨åœ¨é£æ ¼åŒ–è¿ç§»
- **GroupNorm**å°† channel åˆ†ç»„ï¼Œæœ‰ç‚¹ç±»ä¼¼äº LNï¼Œåªæ˜¯ GN æŠŠ channel ä¹Ÿè¿›è¡Œäº†åˆ’åˆ†ï¼Œç»†åŒ–ï¼Œç„¶åå†åšå½’ä¸€åŒ–
- **SwitchableNorm**æ˜¯å°† BNã€LNã€IN ç»“åˆï¼Œèµ‹äºˆæƒé‡ï¼Œè®©ç½‘ç»œè‡ªå·±å»å­¦ä¹ å½’ä¸€åŒ–å±‚åº”è¯¥ä½¿ç”¨ä»€ä¹ˆæ–¹æ³•

{% asset_img 1.webp %}

<!--more-->

{% asset_img 6.webp %}

- å›¾ä¸­æ¯ä¸€ä¸ªæ­£æ–¹ä½“å—è¡¨ç¤ºä¸€ä¸ªæ•°æ®ï¼ˆæ¯”å¦‚è¯´è¿™é‡Œä¸€ä¸ªæ­£æ–¹ä½“å°±æ˜¯ä¸€ä¸ªå›¾åƒï¼‰
- æ¯ä¸€ä¸ªæ­£æ–¹ä½“ä¸­çš„**C**, **H**, **W**åˆ†åˆ«è¡¨ç¤º**channel**(é€šé“ä¸ªæ•°), **height**(å›¾åƒçš„é«˜), **weight**(å›¾åƒçš„å®½)
- Norm çš„æ–¹å¼ï¼Œå¦‚Layer Normä¸­**NHWC->N111**è¡¨ç¤ºæ˜¯å°†**åé¢çš„ä¸‰ä¸ªè¿›è¡Œæ ‡å‡†åŒ–**ï¼Œä¸ä¸ batch æœ‰å…³
- æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œåé¢çš„**LayerNorm**, **InstanceNorm**å’Œ**GroupNorm**è¿™ä¸‰ç§æ–¹å¼éƒ½æ˜¯å’Œ batch æ˜¯æ²¡æœ‰å…³ç³»çš„

***

### Batch Normalization

**è®ºæ–‡ï¼š**[Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf)

é¦–å…ˆï¼Œåœ¨è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œä¸€èˆ¬è¦å¯¹æ•°æ®åš**å½’ä¸€åŒ–**ï¼Œä½¿å…¶åˆ†å¸ƒä¸€è‡´ï¼Œä½†æ˜¯åœ¨æ·±åº¦ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šå¸¸ä»¥é€å…¥ç½‘ç»œçš„æ¯ä¸€ä¸ª batch è®­ç»ƒï¼Œè¿™æ ·æ¯ä¸ª batch å…·æœ‰ä¸åŒçš„åˆ†å¸ƒï¼›æ­¤å¤–ï¼Œä¸ºäº†è§£å†³ internal covarivate shift é—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜å®šä¹‰æ˜¯éšç€ batch normalizaiton è¿™ç¯‡è®ºæ–‡æå‡ºçš„ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ•°æ®åˆ†å¸ƒä¼šå‘ç”Ÿå˜åŒ–ï¼Œå¯¹ä¸‹ä¸€å±‚ç½‘ç»œçš„å­¦ä¹ å¸¦æ¥å›°éš¾ã€‚

æ‰€ä»¥ batch normalization å°±æ˜¯å¼ºè¡Œå°†æ•°æ®æ‹‰å›åˆ°**å‡å€¼ä¸º 0**ï¼Œ**æ–¹å·®ä¸º 1** çš„æ­£æ€åˆ†å¸ƒä¸Šï¼Œè¿™æ ·ä¸ä»…**æ•°æ®åˆ†å¸ƒä¸€è‡´**ï¼Œè€Œä¸”**é¿å…å‘ç”Ÿæ¢¯åº¦æ¶ˆå¤±**ã€‚

æ­¤å¤–ï¼Œinternal corvariate shiftå’Œcovariate shiftæ˜¯ä¸¤å›äº‹ï¼Œå‰è€…æ˜¯ç½‘ç»œå†…éƒ¨ï¼Œåè€…æ˜¯é’ˆå¯¹è¾“å…¥æ•°æ®ï¼Œæ¯”å¦‚æˆ‘ä»¬åœ¨è®­ç»ƒæ•°æ®å‰åšå½’ä¸€åŒ–ç­‰é¢„å¤„ç†æ“ä½œã€‚

{% asset_img 7.webp %}

**ç®—æ³•è¿‡ç¨‹ï¼š**
1. æ²¿ç€é€šé“è®¡ç®—æ¯ä¸ª batch çš„å‡å€¼ u
2. æ²¿ç€é€šé“è®¡ç®—æ¯ä¸ª batch çš„æ–¹å·® Ïƒ^2
3. å¯¹ x åšå½’ä¸€åŒ–ï¼Œxâ€™ = (x - u) / âˆš(Ïƒ^2 + Îµ)
4. åŠ å…¥ç¼©æ”¾å’Œå¹³ç§»å˜é‡ Î³ å’Œ Î² ï¼Œå½’ä¸€åŒ–åçš„å€¼ï¼Œy = Î³xâ€™ + Î²

åŠ å…¥ç¼©æ”¾å¹³ç§»å˜é‡çš„åŸå› æ˜¯ï¼šä¿è¯æ¯ä¸€æ¬¡æ•°æ®ç»è¿‡å½’ä¸€åŒ–åè¿˜ä¿ç•™åŸæœ‰å­¦ä¹ æ¥çš„ç‰¹å¾ï¼ŒåŒæ—¶åˆèƒ½å®Œæˆå½’ä¸€åŒ–æ“ä½œï¼ŒåŠ é€Ÿè®­ç»ƒã€‚è¿™ä¸¤ä¸ªå‚æ•°æ˜¯ç”¨æ¥å­¦ä¹ çš„å‚æ•°ã€‚

``` python
import numpy as np

def Batchnorm(x, gamma, beta, bn_param):

    # x_shape:[B, C, H, W]
    running_mean = bn_param['running_mean']
    running_var = bn_param['running_var']
    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(0, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(0, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta

    # å› ä¸ºåœ¨æµ‹è¯•æ—¶æ˜¯å•ä¸ªå›¾ç‰‡æµ‹è¯•ï¼Œè¿™é‡Œä¿ç•™è®­ç»ƒæ—¶çš„å‡å€¼å’Œæ–¹å·®ï¼Œç”¨åœ¨åé¢æµ‹è¯•æ—¶ç”¨
    running_mean = momentum * running_mean + (1 - momentum) * x_mean
    running_var = momentum * running_var + (1 - momentum) * x_var

    bn_param['running_mean'] = running_mean
    bn_param['running_var'] = running_var

    return results, bn_param
```

***

### Layer Normalizaiton

**è®ºæ–‡ï¼š**[Layer Normalizaiton](https://arxiv.org/pdf/1607.06450v1.pdf)

Batch Normalization å­˜åœ¨ä»¥ä¸‹ç¼ºç‚¹ï¼š
- å¯¹ batchsize çš„å¤§å°æ¯”è¾ƒæ•æ„Ÿï¼Œç”±äºæ¯æ¬¡è®¡ç®—å‡å€¼å’Œæ–¹å·®æ˜¯åœ¨ä¸€ä¸ª batch ä¸Šï¼Œæ‰€ä»¥å¦‚æœ batchsize å¤ªå°ï¼Œåˆ™è®¡ç®—çš„å‡å€¼ã€æ–¹å·®ä¸è¶³ä»¥ä»£è¡¨æ•´ä¸ªæ•°æ®åˆ†å¸ƒ
- BN å®é™…ä½¿ç”¨æ—¶éœ€è¦è®¡ç®—å¹¶ä¸”ä¿å­˜æŸä¸€å±‚ç¥ç»ç½‘ç»œ batch çš„å‡å€¼å’Œæ–¹å·®ç­‰ç»Ÿè®¡ä¿¡æ¯ï¼Œå¯¹äºå¯¹ä¸€ä¸ªå›ºå®šæ·±åº¦çš„å‰å‘ç¥ç»ç½‘ç»œï¼ˆDNNï¼ŒCNNï¼‰ä½¿ç”¨ BNï¼Œå¾ˆæ–¹ä¾¿ï¼›ä½†å¯¹äº RNN æ¥è¯´ï¼Œsequence çš„é•¿åº¦æ˜¯ä¸ä¸€è‡´çš„ï¼Œæ¢å¥è¯è¯´ RNN çš„æ·±åº¦ä¸æ˜¯å›ºå®šçš„ï¼Œä¸åŒçš„ time-step éœ€è¦ä¿å­˜ä¸åŒçš„ statics ç‰¹å¾ï¼Œå¯èƒ½å­˜åœ¨ä¸€ä¸ªç‰¹æ®Š sequence æ¯”å…¶ä»– sequence é•¿å¾ˆå¤šï¼Œè¿™æ · training æ—¶ï¼Œè®¡ç®—å¾ˆéº»çƒ¦ã€‚

ä¸ BN ä¸åŒï¼Œ**LN** æ˜¯é’ˆå¯¹æ·±åº¦ç½‘ç»œçš„**æŸä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒçš„è¾“å…¥**æŒ‰ä»¥ä¸‹å…¬å¼è¿›è¡Œ normalize æ“ä½œã€‚

{% asset_img 8.webp %}

BNä¸LNçš„åŒºåˆ«åœ¨äºï¼š
- LN ä¸­åŒå±‚ç¥ç»å…ƒè¾“å…¥æ‹¥æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·®ï¼Œä¸åŒçš„è¾“å…¥æ ·æœ¬æœ‰ä¸åŒçš„å‡å€¼å’Œæ–¹å·®ï¼›
- BN ä¸­åˆ™é’ˆå¯¹ä¸åŒç¥ç»å…ƒè¾“å…¥è®¡ç®—å‡å€¼å’Œæ–¹å·®ï¼ŒåŒä¸€ä¸ª batch ä¸­çš„è¾“å…¥æ‹¥æœ‰ç›¸åŒçš„å‡å€¼å’Œæ–¹å·®ã€‚

æ‰€ä»¥ï¼ŒLN **ä¸ä¾èµ–**äº **batch çš„å¤§å°**å’Œè¾“å…¥ **sequence çš„æ·±åº¦**ï¼Œå› æ­¤å¯ä»¥ç”¨äº batchsize ä¸º 1 å’Œ RNN ä¸­å¯¹è¾¹é•¿çš„è¾“å…¥ sequence çš„ normalize æ“ä½œã€‚

LN ç”¨äº **RNN æ•ˆæœæ¯”è¾ƒæ˜æ˜¾**ï¼Œä½†æ˜¯åœ¨ CNN ä¸Šï¼Œä¸å¦‚ BNã€‚

``` python
def ln(x, b, s):
    _eps = 1e-5
    output = (x - x.mean(1)[:,None]) / tensor.sqrt((x.var(1)[:,None] + _eps))
    output = s[None, :] * output + b[None,:]
    return output
```

ç”¨åœ¨å››ç»´å›¾åƒä¸Šï¼š

``` python
def Layernorm(x, gamma, beta):

    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(1, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(1, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
```

***

### Instance Normalization

**è®ºæ–‡ï¼š**[Instance Normalization](https://arxiv.org/pdf/1607.08022.pdf)
**è®ºæ–‡å®ç°ï¼š**[https://github.com/DmitryUlyanov/texture_nets](https://github.com/DmitryUlyanov/texture_nets)

BN æ³¨é‡å¯¹æ¯ä¸ª batch è¿›è¡Œå½’ä¸€åŒ–ï¼Œä¿è¯æ•°æ®åˆ†å¸ƒä¸€è‡´ï¼Œå› ä¸ºåˆ¤åˆ«æ¨¡å‹ä¸­ç»“æœå–å†³äºæ•°æ®æ•´ä½“åˆ†å¸ƒã€‚

ä½†æ˜¯**å›¾åƒé£æ ¼åŒ–**ä¸­ï¼Œ**ç”Ÿæˆç»“æœ**ä¸»è¦ä¾èµ–äº**æŸä¸ªå›¾åƒå®ä¾‹**ï¼Œæ‰€ä»¥å¯¹æ•´ä¸ª batch å½’ä¸€åŒ–ä¸é€‚åˆå›¾åƒé£æ ¼åŒ–ä¸­ï¼Œå› è€Œ**å¯¹ HW åšå½’ä¸€åŒ–**å¯ä»¥åŠ é€Ÿæ¨¡å‹æ”¶æ•›ï¼Œå¹¶ä¸”**ä¿æŒæ¯ä¸ªå›¾åƒå®ä¾‹ä¹‹é—´çš„ç‹¬ç«‹**ã€‚

``` python
def Instancenorm(x, gamma, beta):

    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(2, 3), keepdims=True)
    x_var = np.var(x, axis=(2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
```

***

### Group Normalization

**è®ºæ–‡ï¼š**[Group Normalization](https://arxiv.org/pdf/1803.08494.pdf)

ä¸»è¦æ˜¯é’ˆå¯¹ Batch Normalization å¯¹å° batchsize æ•ˆæœå·®ï¼ŒGN å°† **channel æ–¹å‘åˆ† group**ï¼Œç„¶åæ¯ä¸ª **group å†…åšå½’ä¸€åŒ–**ï¼Œç®— (C // G) * H * W çš„å‡å€¼ï¼Œè¿™æ ·**ä¸ batchsize æ— å…³ï¼Œä¸å—å…¶çº¦æŸ**ã€‚

{% asset_img 9.webp %}

``` python
def GroupNorm(x, gamma, beta, G=16):

    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5
    x = np.reshape(x, (x.shape[0], G, x.shape[1]/16, x.shape[2], x.shape[3]))

    x_mean = np.mean(x, axis=(2, 3, 4), keepdims=True)
    x_var = np.var(x, axis=(2, 3, 4), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
```

***

### Switchable Normalization

**è®ºæ–‡ï¼š**[Switchable Normalization](https://arxiv.org/pdf/1806.10779.pdf)
**è®ºæ–‡å®ç°ï¼š**[https://github.com/switchablenorms/Switchable-Normalization](https://github.com/switchablenorms/Switchable-Normalization)

æœ¬ç¯‡è®ºæ–‡ä½œè€…è®¤ä¸ºï¼š
1. å½’ä¸€åŒ–è™½ç„¶æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›ï¼Œç„¶è€Œå½’ä¸€åŒ–å±‚çš„æ“ä½œæ˜¯äººå·¥è®¾è®¡çš„ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè§£å†³ä¸åŒçš„é—®é¢˜åŸåˆ™ä¸Šéœ€è¦è®¾è®¡ä¸åŒçš„å½’ä¸€åŒ–æ“ä½œï¼Œå¹¶æ²¡æœ‰ä¸€ä¸ªé€šç”¨çš„å½’ä¸€åŒ–æ–¹æ³•èƒ½å¤Ÿè§£å†³æ‰€æœ‰åº”ç”¨é—®é¢˜ï¼›
2. ä¸€ä¸ªæ·±åº¦ç¥ç»ç½‘ç»œå¾€å¾€åŒ…å«å‡ åä¸ªå½’ä¸€åŒ–å±‚ï¼Œé€šå¸¸è¿™äº›å½’ä¸€åŒ–å±‚éƒ½ä½¿ç”¨åŒæ ·çš„å½’ä¸€åŒ–æ“ä½œï¼Œå› ä¸ºæ‰‹å·¥ä¸ºæ¯ä¸€ä¸ªå½’ä¸€åŒ–å±‚è®¾è®¡æ“ä½œéœ€è¦è¿›è¡Œå¤§é‡çš„å®éªŒã€‚

å› æ­¤ä½œè€…æå‡ºè‡ªé€‚é…å½’ä¸€åŒ–æ–¹æ³• Switchable Normalizationï¼ˆSNï¼‰ æ¥è§£å†³ä¸Šè¿°é—®é¢˜ã€‚ä¸å¼ºåŒ–å­¦ä¹ ä¸åŒï¼Œ**SN ä½¿ç”¨å¯å¾®åˆ†å­¦ä¹ **ï¼Œä¸ºä¸€ä¸ªæ·±åº¦ç½‘ç»œä¸­çš„æ¯ä¸€ä¸ªå½’ä¸€åŒ–å±‚ç¡®å®šåˆé€‚çš„å½’ä¸€åŒ–æ“ä½œã€‚

``` python
def SwitchableNorm(x, gamma, beta, w_mean, w_var):
    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5

    mean_in = np.mean(x, axis=(2, 3), keepdims=True)
    var_in = np.var(x, axis=(2, 3), keepdims=True)

    mean_ln = np.mean(x, axis=(1, 2, 3), keepdims=True)
    var_ln = np.var(x, axis=(1, 2, 3), keepdims=True)

    mean_bn = np.mean(x, axis=(0, 2, 3), keepdims=True)
    var_bn = np.var(x, axis=(0, 2, 3), keepdims=True)

    mean = w_mean[0] * mean_in + w_mean[1] * mean_ln + w_mean[2] * mean_bn
    var = w_var[0] * var_in + w_var[1] * var_ln + w_var[2] * var_bn

    x_normalized = (x - mean) / np.sqrt(var + eps)
    results = gamma * x_normalized + beta
    return results
```

***

### åœ¨Pytorchä¸Šçš„å®ç°

#### BatchNorm

``` python
torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
```

| å‚æ•° | è§£é‡Š |
| -------- | --- |
| **num_features** | æ¥è‡ªæœŸæœ›è¾“å…¥çš„ç‰¹å¾æ•°ï¼Œè¯¥æœŸæœ›è¾“å…¥çš„å¤§å°ä¸º batch_size x num_features [x width] |
| **eps** | ä¸ºä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼ˆåˆ†æ¯ä¸èƒ½è¶‹è¿‘æˆ–å–0ï¼‰ï¼Œç»™åˆ†æ¯åŠ ä¸Šçš„å€¼ã€‚é»˜è®¤ä¸º1e-5ã€‚ |
| **momentum** | åŠ¨æ€å‡å€¼å’ŒåŠ¨æ€æ–¹å·®æ‰€ä½¿ç”¨çš„åŠ¨é‡ã€‚é»˜è®¤ä¸º0.1ã€‚ |
| **affine** | å¸ƒå°”å€¼ï¼Œå½“è®¾ä¸ºtrueï¼Œç»™è¯¥å±‚æ·»åŠ å¯å­¦ä¹ çš„ä»¿å°„å˜æ¢å‚æ•°ã€‚ |
| **track_running_stats** | å¸ƒå°”å€¼ï¼Œå½“è®¾ä¸ºtrueï¼Œè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡å€¼å’Œæ–¹å·®ã€‚ |

**å®ç°å…¬å¼ï¼š**
{% asset_img 2.webp %}

***

#### LayerNorm

``` python
torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)
```

| å‚æ•° | è§£é‡Š |
| -------- | --- |
| **normalized_shape** | è¾“å…¥å°ºå¯¸ [âˆ—Ã—normalized_shape[0] Ã— normalized_shape[1] Ã— â€¦ Ã— normalized_shape[âˆ’1]] |
| **eps** | ä¸ºä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼ˆåˆ†æ¯ä¸èƒ½è¶‹è¿‘æˆ–å–0ï¼‰ï¼Œç»™åˆ†æ¯åŠ ä¸Šçš„å€¼ã€‚é»˜è®¤ä¸º1e-5ã€‚ |
| **elementwise_affine** | å¸ƒå°”å€¼ï¼Œå½“è®¾ä¸ºtrueï¼Œç»™è¯¥å±‚æ·»åŠ å¯å­¦ä¹ çš„ä»¿å°„å˜æ¢å‚æ•°ã€‚ |

**å®ç°å…¬å¼ï¼š**
{% asset_img 3.webp %}

***

#### InstanceNorm

``` python
torch.nn.InstanceNorm1d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
torch.nn.InstanceNorm3d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
```

| å‚æ•° | è§£é‡Š |
| -------- | --- |
| **num_features** | æ¥è‡ªæœŸæœ›è¾“å…¥çš„ç‰¹å¾æ•°ï¼Œè¯¥æœŸæœ›è¾“å…¥çš„å¤§å°ä¸º batch_size x num_features [x width] |
| **eps** | ä¸ºä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼ˆåˆ†æ¯ä¸èƒ½è¶‹è¿‘æˆ–å–0ï¼‰ï¼Œç»™åˆ†æ¯åŠ ä¸Šçš„å€¼ã€‚é»˜è®¤ä¸º1e-5ã€‚ |
| **momentum** | åŠ¨æ€å‡å€¼å’ŒåŠ¨æ€æ–¹å·®æ‰€ä½¿ç”¨çš„åŠ¨é‡ã€‚é»˜è®¤ä¸º0.1ã€‚ |
| **affine** | å¸ƒå°”å€¼ï¼Œå½“è®¾ä¸ºtrueï¼Œç»™è¯¥å±‚æ·»åŠ å¯å­¦ä¹ çš„ä»¿å°„å˜æ¢å‚æ•°ã€‚ |
| **track_running_stats** | å¸ƒå°”å€¼ï¼Œå½“è®¾ä¸ºtrueï¼Œè®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„å‡å€¼å’Œæ–¹å·®ã€‚ |

**å®ç°å…¬å¼ï¼š**
{% asset_img 4.webp %}

***

#### GroupNorm

``` python
torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)
```

| å‚æ•° | è§£é‡Š |
| -------- | --- |
| **num_groups** | éœ€è¦åˆ’åˆ†ä¸ºçš„groups |
| **num_features** | æ¥è‡ªæœŸæœ›è¾“å…¥çš„ç‰¹å¾æ•°ï¼Œè¯¥æœŸæœ›è¾“å…¥çš„å¤§å°ä¸º batch_size x num_features [x width] |
| **eps** | ä¸ºä¿è¯æ•°å€¼ç¨³å®šæ€§ï¼ˆåˆ†æ¯ä¸èƒ½è¶‹è¿‘æˆ–å–0ï¼‰ï¼Œç»™åˆ†æ¯åŠ ä¸Šçš„å€¼ã€‚é»˜è®¤ä¸º1e-5ã€‚ |
| **momentum** | åŠ¨æ€å‡å€¼å’ŒåŠ¨æ€æ–¹å·®æ‰€ä½¿ç”¨çš„åŠ¨é‡ã€‚é»˜è®¤ä¸º0.1ã€‚ |
| **affine** | å¸ƒå°”å€¼ï¼Œå½“è®¾ä¸ºtrueï¼Œç»™è¯¥å±‚æ·»åŠ å¯å­¦ä¹ çš„ä»¿å°„å˜æ¢å‚æ•°ã€‚ |

**å®ç°å…¬å¼ï¼š**
{% asset_img 5.webp %}

***

### æ€»ç»“

#### ç±»ä¼¼æƒ…å†µ

- å½“ **GroupNorm** ä¸­groupçš„æ•°é‡æ˜¯ **1** çš„æ—¶å€™ï¼Œæ˜¯ä¸ **LayerNorm** æ˜¯ç­‰ä»·çš„
- **InstanceNorm** ç­‰ä»·äºå½“ GroupNorm æ—¶ **num_groups** çš„æ•°é‡ç­‰äº **num_channel** çš„æ•°é‡

#### æ¯ä¸€ç§æ–¹å¼é€‚åˆçš„åœºæ™¯

- BatchNorm æ˜¯åœ¨ batch ä¸Šï¼Œå¯¹å° batchsize æ•ˆæœä¸å¥½
- LayerNorm åœ¨é€šé“æ–¹å‘ä¸Šï¼Œä¸»è¦å¯¹ RNN ä½œç”¨æ˜æ˜¾
- InstanceNorm åœ¨å›¾åƒåƒç´ ä¸Šï¼Œç”¨åœ¨é£æ ¼åŒ–è¿ç§»
- GroupNorm å°† channel åˆ†ç»„ï¼Œç„¶åå†åšå½’ä¸€åŒ–, åœ¨ batchsize < 16 çš„æ—¶å€™, å¯ä»¥ä½¿ç”¨è¿™ç§å½’ä¸€åŒ–

{% asset_img 10.webp %}

***

### å‚è€ƒèµ„æ–™

> <https://blog.csdn.net/shanglianlm/article/details/85075706>
> <https://zhuanlan.zhihu.com/p/395855181>
> <https://blog.csdn.net/liuxiao214/article/details/81037416>
