---
title: TMLR 2022 | 基于解耦变分推理的少样本学习
categories:
  - 🌙进阶学习
  - ⭐脑机接口与混合智能研究团队（BCI团队）
  - 💫学习报告
abbrlink: fc2cc356
date: 2025-05-21 04:38:28
tags:
---

{% asset_img 1.webp %}

该论文2022年发表于机器学习领域新兴期刊Transactions on Machine Learning Research (TMLR)，荷兰代尔夫特理工大学的Anuj Singh是第一作者，荷兰皇家壳牌集团技术公司的Hadi Jamali-Rad是通讯作者。该文章在paper with code上是FSL领域的sota。

<!--more-->

***

### 论文概要

背景：人类具备从极少样本中快速学习的能力，而深度学习通常需要海量数据才能达到良好性能。为应对样本匮乏的问题，少样本分类（Few-Shot Classification）成为研究热点。作者提出了一种将图像表示解耦为语义潜变量和标签潜变量的方法，同时设计了基于注意力的AttFEX模块，来捕捉跨样本的特征。

方法：

{% asset_img 2.webp %}
<div align='center'>图1 整体架构</div>

***

### 研究方法

（1）变分推断：

将特征分开学习，通过将特征建模为标签潜变量Zl和语义潜变量Zs，签潜变量Zl需要同时负责重构和判断标签。在分类时以Zs辅助去掉无关分类的信息。

{% asset_img 3.webp %}

此外，为了找到近似后验，通过ELBO推导出证据下界：

{% asset_img 4.webp %}

（2）AttFEX

使用注意力捕捉支持集和查询集的特征。先通过1*1卷积扩展通道，再通过自注意力捕捉跨样本特征，并与原始特征相乘，得到任务感知图。

{% asset_img 5.webp %}
{% asset_img 6.webp %}
{% asset_img 7.webp %}

（3）元学习训练策略：

借鉴经典的MAML，通过元更新加速新类别的适应。

伪代码部分如下，分为在基类上训练和在新类上直推：

{% asset_img 8.webp %}
{% asset_img 9.webp %}

***

### 实验结果

作者一共在三种数据集上进行实验（miniImagenet、tieredImagenet、cross），结果表明，所提出的方法取得了很好的性能，远超第二名。且作者仅使用性能低下的特征提取器ConV4就超越了其他所有方法。

{% asset_img 10.webp %}

***

### 可视化

作者对两个变分变量的方法进行了可视化处理，证明确实可以让特征更加可分，Zl的区分度明显提高。

{% asset_img 11.webp %}

***

### 结论

在本文中，作者提出了一种将特征通过变分网络解耦并通过注意力捕捉跨样本特征的小样本方法。该方法很好地解耦了潜在特征，使其能有效提高小样本的分类性能。本文自2022年发表以来一直是paper with code上的sota，表明了其方法的先进性。

***

### 原文链接

> <https://www.scholat.com/teamwork/showPostMessage.html?id=17014>
