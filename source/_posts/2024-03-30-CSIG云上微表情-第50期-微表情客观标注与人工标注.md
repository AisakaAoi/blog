---
title: CSIG云上微表情-第50期-微表情客观标注与人工标注
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 3eff7a1b
date: 2024-03-30 05:28:40
tags:
---

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=1352322425&bvid=BV1Lz421f7mQ&cid=1487543440&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

<!--more-->

微表情是一种短暂的、微弱的、无意识的面部微表情，持续时间往往在0.5s内，能够揭示人类试图隐藏的真实情绪。微表情识别的研究旨在让机器有足够的智能，能够从人脸视频序列中识别人类的真实情绪。然而由于微表情持续时间短、面部肌肉运动强度低，对其进行准确的表征与识别是一项极具挑战性的任务。为了促进心理学领域和计算机视觉领域针对微表情的进一步研究，由中国图象图形学学会（CSIG）和中国科学院心理研究所举办、CSIG机器视觉专业委员会、CSIG情感计算与理解专业委员会和中科院青促会心理所小组联合承办，中国科学院心理研究所的王甦菁博士组织一系列云上微表情的学术活动。
第五十期云上微表情于2024年03月30日晚上7点进行，由中国科学院心理研究所王甦菁老师团队的李婧婷博士主持。此次讲座邀请到来自中科院心理所微表情应用研究中心深度参与微表情编码工作的张建行与刘一婷同学围绕“微表情客观标注与人工标注”主题做报告。

讲者简介：

张建行，江苏科技大学计算机学院研究生三年级在读，导师黄树成教授。实习于中国科学院心理研究所微表情应用研究中心，在王甦菁副研究员的指导下开展微表情标注相关研究。
报告题目：面周肌电数据采集设备
报告摘要：
由于微表情是一种时间短暂、动作微小且发生在局部的面部表情。微表情的标注者需要通过慢放或者回放等操作对视频逐帧观察进行标注，使得微表情数据的标注工作费时费力。针对这一问题，本研究提出了一种新的解决方案：设计并实现了一款肌电采集设备。该方案通过将电极放置在面部周围，同步采集肌电和视频数据。标注人员可以通过肌电数据发生波动的时间快速定位到视频中微表情的起止时间，进行更为细致和精确的标注工作。这种方法不仅大幅降低了标注人员的工作负担，而且通过半自动化的标注过程，显著提高了微表情数据标注的效率和准确性。

参考文献：
[1] Zhang, J., Huang, S., Li, J., Wang, Y., Dong, Z., & Wang, S. J. (2023). A Perifacial EMG Acquisition System for Facial-Muscle-Movement Recognition. Sensors, 23(21), 8758.

刘一婷，首都师范大学与中国科学院心理研究所联合培养学生，她于2021年毕业于北京联合大学，获得管理学学士学位，目前在首都师范大学应用心理专业研究生就读，与中国科学院心理研究所联合培养，在微表情数据标注方面有扎实的理论基础和丰富实践经验，多次参与微表情数据标注工作，并且考取了FACS证书。
报告题目：微表情的诱发与人工标注
报告摘要：
微表情是非常重要的非语言交流线索，它可以揭示真实的情绪和个人的心理状态。作为谎言识别的重要线索之一，微表情的有效性甚至显著高于言语内容、语音、语调、身体姿势等其他线索，可以被广泛地应用于国家安全、司法实践、临床诊断、学生教育、卫生防疫等领域。然而，微表情具备的三个特征：持续时间短，动作强度低和只存在局部运动，使得微表情的相关特征很难提取。同时，微表情样本的人工标注也十分困难，是典型的小样本问题。因此，我们主要针对微表情的诱发方法及人工编码过程方面进行介绍与讲解。

参考文献：
[1]E Friesen and Paul Ekman. 1978. Facial action coding system: a technique for the measurement of facial movement. Palo Alto3, 2 (1978), 5.
[2]Li, J., et al. (2022). CAS (ME) 3: A third generation facial spontaneous micro-expression database with depth information and high ecological validity. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3), 2782-2800.
[3]Dong, Z., et al.. (2021, October). A brief guide: Code for spontaneous expressions and micro-expressions in videos. In Proceedings of the 1st Workshop on Facial Micro-Expression: Advanced Techniques for Facial Expressions Generation and Spotting (pp. 31-37).
