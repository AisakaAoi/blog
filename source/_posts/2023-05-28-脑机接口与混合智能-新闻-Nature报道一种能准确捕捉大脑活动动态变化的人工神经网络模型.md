---
title: 脑机接口与混合智能-新闻-Nature报道一种能准确捕捉大脑活动动态变化的人工神经网络模型
categories:
  - 🌙进阶学习
  - ⭐脑机接口与混合智能研究团队（BCI团队）
  - 💫新闻
abbrlink: 9e0bf1bb
date: 2023-05-28 03:39:04
tags:
---

现代神经科学研究使数据收集的速度很快，但我们了解构成神经系统的复杂非线性系统的能力仍然有限。直到最近，神经科学家们才能够记录大规模的行为和神经活动。然而，一直缺乏联合考虑这些数据的工具。尽管机器学习技术已经取得了巨大的进步，但关于如何分析现实世界中的连续和离散时间序列数据的问题仍然存在。因此，我们需要创新的计算工具来理解可以观察到的信息（尽管仍然有限）和想要理解的隐藏动态之间的复杂关系。

为了解决这个问题，在2023年5月，来自洛桑联邦理工学院（EPFL）的Mathis和她的团队在《Nature》杂志上发表了一项研究《Learnable latent embeddings for joint behavioural and neural analysis》，提出了一种基于数学的新型机器学习算法，被称为CEBRA，这种算法学习高维数据如何在低维空间（称为潜空间）中排列（嵌入），从而使相似的数据点靠近，而更多不同的数据点相距更远。这个空间中的嵌入可以用来推断数据中的潜在关系和结构。他们将这种对比学习方法扩展到带有离散或连续 "标签 "的样本数据。这些标签可以是在同一时间收集的其他数据，或者只是时间本身。他们团队引入了三种对比学习的变体：有监督的（使用用户定义的注释）、自我监督的（只使用时间标签）和混合变体。对比学习的优势在于它能够共同考虑神经数据和行为标签：这些标签可以是测量的动作、抽象的标签（如 "奖励"）或解构的感官特征（如图像的颜色或纹理）。CEBRA的一个主要特点是鼓励用户首先使用自我监督学习（发现驱动的科学），然后用标签做监督学习（假设测试），以了解什么有助于嵌入。

<!--more-->

他们提出的CEBRA算法在恢复原始真实数据方面的表现大大优于其他降维算法，如t-SNE、UMAP和变量自动编码器。论文中进行了几项神经科学分析，以突出CEBRA在生命科学领域的应用。首先，在使用两种技术记录的神经数据中发现了高度相似的潜在结构，解决了关于用不同方法收集的数据的可比性的重要开放问题。其次，使用CEBRA对小鼠观看视频时在一个叫做视觉皮层的大脑区域记录的活动进行解码，从而实现了对该视频的高性能解码。这表明，即使是初级视觉皮层（通常被认为仅是相对基本的视觉处理的基础）也可以被用来进行脑机接口式的视频解码。

CEBRA算法可用于压缩时间序列，以揭示数据可变性中原本隐藏的结构。它擅长处理同时记录的行为和神经数据，它可以对小鼠大脑视觉皮层的活动进行解码，以重建观看的视频。Mathis和她的团队在研究中，观察了50只老鼠在观看一段30秒的电影片段时的大脑活动。他们让这些老鼠看了九遍该电影片段。然后，研究人员训练了CEBRA，将大脑数据与电影片段联系起来。

{% asset_img 1.webp %}
<div align='center'>CEBRA编码器和kNN解码器的示意图。</div>

最后，该团队将该电影片段播放了第十次，并测试了CEBRA，以利用大脑活动数据测试片段内帧节的顺序。在Mathis和她的团队公开的一段视频中，一个单独的屏幕展示了CEBRA重建的老鼠所看到的片段，尽管视频会间歇性地出现卡顿，但重建的画面与原画面几乎一样。

{% asset_img 2.webp %}
<div align='center'>原始帧（上）和使用kNN解码从CEBRA嵌入的V1钙记录中解码的帧（下）的例子。</div>

CEBRA从原始神经数据中学到的信息可以在解码训练后进行测试，是一种用于脑机接口的方法，Mathis的团队的研究人员已经证明，预训练的CEBRA模型可以在毫秒内进行动物脑神经的解码。此外，CEBRA并不局限于视觉皮层神经元，甚至大脑数据。Mathis和她的团队的研究还表明，CEBRA可以用来预测灵长类动物手臂的运动。CEBRA算法可以用于研究发展、动物行为和基因表达数据。CEBRA的优势之一是它可以结合不同模式的数据，并有助于限制细微差别（如数据的变化取决于数据的收集方式）。

***

### 原文链接

> <https://www.scholat.com/teamwork/showPostMessage.html?id=13769>
> Schneider S, Lee J H, Mathis M W. Learnable latent embeddings for joint behavioural and neural analysis[J]. Nature, 2023: 1-9.
