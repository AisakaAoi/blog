---
title: >-
  è®ºæ–‡é˜…è¯»-CLIPè®ºæ–‡é€æ®µç²¾è¯»-ã€ŠLearning Transferable Visual Models From Natural Language
  Supervisionã€‹
categories:
  - ğŸŒ™è¿›é˜¶å­¦ä¹ 
  - â­è®ºæ–‡å¸¦è¯»
  - ğŸ’«ç²¾è¯»ç»å…¸
abbrlink: e17abfce
date: 2022-08-17 11:05:26
tags:
---

### åŸæ–‡

{% pdf ./file/paper/2021-Learning-Transferable-Visual-Models-From-Natural-Language-Supervision.pdf %}

ä»Šå¤©ä»‹ç»ä¸€ç¯‡OpenAIçš„ç¥ä½œCLIPï¼Œæ–‡ç« å‘è¡¨åœ¨ICML-2021ï¼Œäº2021å¹´3æœˆæŒ‚åœ¨arXivä¸Šçš„ã€‚

è®ºæ–‡é“¾æ¥ï¼š<https://arxiv.org/pdf/2103.00020.pdf>
Blogä¼ é€é—¨ï¼š<https://openai.com/blog/clip/>
Codeä¼ é€é—¨ï¼š<https://github.com/openai/CLIP>

<!--more-->

<iframe src="//player.bilibili.com/player.html?aid=851425715&bvid=BV1SL4y1s7LQ&cid=505919491&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

***

### Abstract

ï¼ˆæ­¤éƒ¨åˆ†ç¿»è¯‘ä¸ºä¸»ï¼‰

å½“å‰çš„è®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰æ¨¡å‹é€šå¸¸è¢«è®­ç»ƒç”¨äºé¢„æµ‹æœ‰é™çš„ç‰©ä½“ç±»åˆ«ã€‚è¿™ç§ä¸¥æ ¼çš„ç›‘ç£è®­ç»ƒæ–¹å¼é™åˆ¶äº†æ¨¡å‹çš„æ³›åŒ–æ€§å’Œå®ç”¨æ€§ï¼Œå› ä¸ºè¿™æ ·çš„æ¨¡å‹é€šå¸¸è¿˜éœ€è¦é¢å¤–çš„æ ‡æ³¨æ•°æ®æ¥å®Œæˆè®­ç»ƒæ—¶æœªæ›¾è§è¿‡çš„è§†è§‰â€œæ¦‚å¿µâ€ã€‚ç›´æ¥ä»å›¾ç‰‡çš„æè¿°æ–‡æœ¬ä¸­å­¦ä¹ æ˜¯ä¸€ä¸ªæœ‰æ½œåŠ›çš„é€‰æ‹©ï¼Œå› ä¸ºè¿™æ ·æˆ‘ä»¬å¯ä»¥è·å–æ›´å¤šçš„ç›‘ç£ä¿¡å·ã€‚è¿™ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†åˆ©ç”¨ä¸€ä¸ªç®€å•çš„é¢„è®­ç»ƒä»»åŠ¡ï¼ˆå³é¢„æµ‹å“ªä¸ªæ–‡æœ¬æè¿°å¯¹åº”å½“å‰å›¾åƒï¼‰åœ¨ä¸€ä¸ªä»äº’è”ç½‘ä¸Šæœé›†çš„4äº¿ä¸ªï¼ˆå›¾åƒï¼Œæ–‡æœ¬ï¼‰å¯¹çš„æ•°æ®é›†ä¸Šå¯ä»¥å–å¾—SOTAçš„å›¾åƒè¡¨å¾ã€‚é¢„è®­ç»ƒå®Œä¹‹åï¼Œåœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ç”¨è‡ªç„¶è¯­è¨€ï¼ˆæ–‡æœ¬ï¼‰åŒ¹é…è§†è§‰æ¦‚å¿µï¼ˆå›¾åƒï¼‰ä»è€Œå®ç°zero-shot transferã€‚æˆ‘ä»¬åœ¨30ä¸ªä¸åŒç±»å‹çš„ä¸‹æ¸¸CVä»»åŠ¡ä¸Šè¿›è¡Œäº†åŸºå‡†æµ‹è¯•ï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬æ¨¡å‹å¼ºå¤§çš„è¿ç§»èƒ½åŠ›ï¼Œå…¶åœ¨å¾ˆå¤šä¸‹æ¸¸ä»»åŠ¡ä¸Šä¸éœ€è¦ä»»ä½•é¢å¤–çš„æ•°æ®ä¹Ÿèƒ½æ¯”æ‹Ÿå®Œå…¨supervisedçš„æ¨¡å‹ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬çš„æ¨¡å‹åœ¨ImageNetä¸Šçš„zero-shot accuracyèƒ½è¾¾åˆ°åœ¨ImageNetä¸Šå…¨ç›‘ç£è®­ç»ƒçš„ResNet-50çš„æ€§èƒ½ã€‚

***

### Motivation

åœ¨NLPä¸­ï¼Œé¢„è®­ç»ƒçš„æ–¹æ³•ç›®å‰å…¶å®å·²ç»è¢«éªŒè¯å¾ˆæˆåŠŸäº†ï¼ŒåƒBERTå’ŒGPTç³»åˆ—ä¹‹ç±»çš„ã€‚å…¶ä¸­ï¼ŒGPT-3ä»ç½‘ä¸Šæœé›†äº†400 billion byte-pair-encoded tokensè¿›è¡Œé¢„è®­ç»ƒç„¶åå¯ä»¥åœ¨å¾ˆå¤šä¸‹æ¸¸ä»»åŠ¡ä¸Šå®ç°SOTAæ€§èƒ½å’Œzero-shot learningã€‚è¿™å…¶å®è¯´æ˜ä»web-scaleçš„æ•°æ®ä¸­å­¦ä¹ æ˜¯å¯ä»¥è¶…è¿‡é«˜è´¨é‡çš„äººå·¥æ ‡æ³¨çš„NLPæ•°æ®é›†çš„ã€‚

ç„¶è€Œï¼Œå¯¹äºCVé¢†åŸŸï¼Œç›®å‰é¢„è®­ç»ƒæ¨¡å‹åŸºæœ¬éƒ½æ˜¯åŸºäºäººå·¥æ ‡æ³¨çš„ImageNetæ•°æ®é›†ï¼ˆå«æœ‰1400å¤šä¸‡å¼ å›¾åƒï¼‰ï¼Œé‚£ä¹ˆå€Ÿé‰´NLPé¢†åŸŸçš„GPT-3ä»ç½‘ä¸Šæœé›†å¤§é‡æ•°æ®çš„æ€è·¯ï¼Œæˆ‘ä»¬èƒ½ä¸èƒ½ä¹Ÿä»ç½‘ä¸Šæœé›†å¤§é‡å›¾åƒæ•°æ®ç”¨äºè®­ç»ƒè§†è§‰è¡¨å¾æ¨¡å‹å‘¢ï¼Ÿ

ä½œè€…å…ˆæ˜¯å›é¡¾äº†å¹¶æ€»ç»“äº†å’Œä¸Šè¿°ç›¸å…³çš„ä¸¤æ¡è¡¨å¾å­¦ä¹ è·¯çº¿ï¼š
1. æ„å»ºimageå’Œtextçš„è”ç³»ï¼Œæ¯”å¦‚åˆ©ç”¨å·²æœ‰çš„ï¼ˆimageï¼Œtextï¼‰pairæ•°æ®é›†ï¼Œä»textä¸­å­¦ä¹ imageçš„è¡¨å¾ï¼›
2. è·å–æ›´å¤šçš„æ•°æ®ï¼ˆä¸è¦æ±‚é«˜è´¨é‡ï¼Œä¹Ÿä¸è¦æ±‚full labeledï¼‰ç„¶ååšå¼±ç›‘ç£é¢„è®­ç»ƒï¼Œå°±åƒè°·æ­Œä½¿ç”¨çš„JFT-300Mæ•°æ®é›†è¿›è¡Œé¢„è®­ç»ƒä¸€æ ·ï¼ˆåœ¨JFTæ•°æ®é›†ä¸­ï¼Œç±»åˆ«æ ‡ç­¾æ˜¯æœ‰å™ªå£°çš„ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼ŒJFTä¸­ä¸€å…±æœ‰18291ä¸ªç±»åˆ«ï¼Œè¿™èƒ½æ•™æ¨¡å‹çš„æ¦‚å¿µæ¯”ImageNetçš„1000ç±»è¦å¤šå¾—å¤šï¼Œä½†å°½ç®¡å·²ç»æœ‰ä¸Šä¸‡ç±»äº†ï¼Œå…¶æœ€åçš„åˆ†ç±»å™¨å…¶å®è¿˜æ˜¯é™æ€çš„ã€æœ‰é™çš„ï¼Œå› ä¸ºä½ æœ€åè¿˜æ˜¯å¾—å›ºå®šåˆ°18291ä¸ªç±»åˆ«ä¸Šè¿›è¡Œåˆ†ç±»ï¼Œé‚£ä¹ˆè¿™æ ·çš„ç±»åˆ«é™åˆ¶è¿˜æ˜¯é™åˆ¶äº†æ¨¡å‹çš„zero-shotèƒ½åŠ›ã€‚
è¿™ä¸¤æ¡è·¯çº¿å…¶å®éƒ½å±•ç°äº†ç›¸å½“çš„æ½œåŠ›ï¼Œå‰è€…è¯æ˜paired text-imageå¯ä»¥ç”¨æ¥è®­ç»ƒè§†è§‰è¡¨å¾ï¼Œåè€…è¯æ˜æ‰©å……æ•°æ®èƒ½æå¤§æå‡æ€§èƒ½ï¼Œå³ä½¿æ•°æ®æœ‰noiseã€‚äºæ˜¯high-levelä¸Šï¼Œä½œè€…è€ƒè™‘ä»ç½‘ä¸Šçˆ¬å–å¤§é‡çš„ï¼ˆtextï¼Œimageï¼‰pairä»¥æ‰©å……æ•°æ®ï¼ŒåŒæ—¶è¿™æ ·çš„pairsæ˜¯å¯ä»¥ç”¨æ¥è®­ç»ƒè§†è§‰è¡¨å¾çš„ã€‚ä½œè€…éšå³åœ¨äº’è”ç½‘ä¸Šé‡‡é›†äº†4äº¿ä¸ªï¼ˆtextï¼Œimageï¼‰å¯¹ï¼Œå‡†å¤‡å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚

***

### Model

#### Objective

æµ·é‡çš„ï¼ˆimageï¼Œtextï¼‰æ•°æ®æœ‰äº†ï¼Œé—®é¢˜æ˜¯æ€ä¹ˆè®¾è®¡å¹¶é«˜æ•ˆåœ°è®­ç»ƒæ¨¡å‹ã€‚ä½œè€…æå‡ºCLIPçš„æ¨¡å‹ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯ConVIRT[1]çš„ç®€åŒ–ç‰ˆã€‚è¿™é‡Œå…ˆç®€å•å›é¡¾ä¸‹ConVIRT (å’‹ä¸€çœ‹æ˜¯ä¸æ˜¯è§‰å¾—CLIPå’ŒConVIRTä¸€æ¨¡ä¸€æ ·)

{% asset_img 1.webp %}

VonVIRTç”¨ï¼ˆimageï¼Œtextï¼‰å¯¹æ¥è®­ç»ƒæ¨¡å‹ï¼Œå…¶æœ‰ä¸€ä¸ªimage encoderå’Œä¸€ä¸ªtext encoderï¼Œè®­ç»ƒç›®æ ‡æ˜¯è®©ä¸¤è·¯çš„representationå°½å¯èƒ½å¾—ä¸€è‡´ï¼ˆå¯¹å¶åœ°æœ€å¤§åŒ–è¡¨å¾çš„agreementï¼‰ï¼Œå…¶ä¸­gvå’Œguå‡½æ•°æ˜¯ä¸€ä¸ªnon-linearå¾—projection headï¼Œè´Ÿè´£åˆ†åˆ«å°†å›¾åƒå’Œæ–‡æœ¬è¡¨å¾æŠ•å½±åˆ°ä¸€ä¸ªsharedçš„ç©ºé—´ï¼Œä»è€Œè®¡ç®—è·ç¦»ã€‚

{% asset_img 2.webp %}

{% asset_img 3.webp %}

å…¶å®å°±æ˜¯æ„é€ äº†ä¸€ä¸ªå¯¹ç§°çš„contrastive lossï¼Œåœ¨ä¸€ä¸ªbatchå†…é¢„æµ‹è°æ˜¯æ­£æ ·æœ¬ã€‚

åŸºäºConVIRTï¼ŒCLIPä¸»è¦åšå‡ºäº†ä»¥ä¸‹ç®€åŒ–ï¼š
- ConVIRTä¸­çš„image encoderçš„å‚æ•°æ˜¯ImageNetåˆå§‹åŒ–çš„ï¼Œè€ŒCLIPç›´æ¥ç”¨randomåˆå§‹åŒ–
- ConVIRTçš„projection headæ˜¯non-linearçš„ï¼Œè€ŒCLIPé‡‡ç”¨linearçš„projection
- CLIPå»æ‰äº†ConVIRTä¸­text transformationï¼ˆæŒ‡å‡åŒ€ä»textä¸­é‡‡æ ·å¥å­ï¼‰ï¼Œå› ä¸ºCLIPæ•°æ®é›†ä¸­å¾ˆå¤šæŒ‡å‡ºè¿‡ä¸€æ¬¡çš„ï¼ˆimageï¼Œtextï¼‰
- CLIPçš„image transformationåªç”¨äº†resizeå’Œsquared crop
- CLIP lossä¸­çš„temperatureå‚æ•°Ï„æ˜¯å¯å­¦çš„

äºæ˜¯CLIPçš„é¢„è®­ç»ƒæ¨¡å‹å°±æœ‰äº†ï¼š

{% asset_img 4.webp %}

ä¸€ä¸ªbatché‡Œæœ‰Nå¯¹ï¼ˆimageï¼Œtextï¼‰ï¼Œç„¶åå’ŒConVIRTä¸€æ ·åšå¯¹ç§°çš„contrastive learningï¼Œä¼ªä»£ç å¦‚ä¸‹ï¼š

{% asset_img 5.webp %}

***

#### Inference / Zero-shot prediction

ä¸€æ—¦CLIPè®­ç»ƒå¥½äº†ï¼Œæˆ‘ä»¬å°±å¯ä»¥åšzero-shot predictionäº†ï¼Œå¦‚å›¾æ‰€ç¤ºï¼š

{% asset_img 6.webp %}

æ­¥éª¤å¯ä»¥æ•´ç†æˆä¸‹é¢è¿™æ ·ï¼š

- Sampleæ‰€æœ‰Nä¸ªclassï¼Œå¾—åˆ°Nä¸ªinput textï¼Œéƒ½ç»è¿‡text encoderç¼–ç å¾—åˆ°å¯¹åº”çš„Nä¸ªclass text embeddingï¼ˆæˆ‘è¿™é‡Œä¹‹æ‰€å«embeddingè€Œä¸å«representationæ˜¯æƒ³è¯´æ˜è¿™ä¸ªç‰¹å¾æ˜¯ç»è¿‡encodingå’Œprojectionå¾—åˆ°çš„ï¼‰
- Sampleä¸€ä¸ªè¦é¢„æµ‹çš„imageï¼Œå¾—åˆ°å…¶image embedding
- ä»¥Nä¸ªtext embeddingä¸ºkeyï¼Œä»¥å½“å‰image embeddingä¸ºqueryï¼Œç®—cosineç›¸ä¼¼åº¦ï¼Œç›¸ä¼¼åº¦æœ€é«˜çš„å³ä¸ºTop-1çš„prediction class

é¢„æµ‹è¿‡ç¨‹çš„ä»£ç å¦‚ä¸‹ï¼š

``` python
import os
import clip
import torch
from torchvision.datasets import CIFAR100

# Load the model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load('ViT-B/32', device)

# Download the dataset
cifar100 = CIFAR100(root=os.path.expanduser("~/.cache"), download=True, train=False)

# Prepare the inputs
image, class_id = cifar100[3637]
image_input = preprocess(image).unsqueeze(0).to(device)
text_inputs = torch.cat([clip.tokenize(f"a photo of a {c}") for c in cifar100.classes]).to(device)

# Calculate features
with torch.no_grad():
    image_features = model.encode_image(image_input)
    text_features = model.encode_text(text_inputs)

# Pick the top 5 most similar labels for the image
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
values, indices = similarity[0].topk(5)

# Print the result
print("\nTop predictions:\n")
for value, index in zip(values, indices):
    print(f"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%")
```

***

#### Training

##### text encoder

ä½œè€…ç»Ÿä¸€é‡‡ç”¨GPT-2é‡Œçš„Transformerï¼ˆ63M-parameter 12-layer 512-wide model with 8 attention headsï¼‰ã€‚è¾“å…¥å¥å­çš„æœ€å¤§é•¿åº¦ä¸º76ã€‚

##### image encoder

è¿™é‡Œä½œè€…ä¸€å…±è®­ç»ƒäº†8ä¸ªä¸åŒçš„image encoderï¼ˆ5 ResNets & 3 ViTsï¼‰ï¼Œåˆ†åˆ«å¦‚ä¸‹ï¼š

``` python
_MODELS = {
    "RN50": "https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt",
    "RN101": "https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt",
    "RN50x4": "https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt",
    "RN50x16": "https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt",
    "RN50x64": "https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt",
    "ViT-B/32": "https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt",
    "ViT-B/16": "https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt",
    "ViT-L/14": "https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt",
}
```

å…¶ä¸­ResNetsåšäº†ä¸€ä¸ªå°çš„ä¿®æ”¹ï¼šå°†ResNetç¼–ç å‡ºæ¥çš„ç»“æœå†ç»è¿‡ä¸€ä¸ªattention poolingï¼ˆæ¯”å¦‚ä¸€ä¸ª2048x7x7çš„featureï¼Œç”¨attention poolingæˆä¸€ä¸ª2048x1çš„featureï¼‰ï¼›å¯¹äºViTsä¹Ÿåšäº†ä¸€ä¸ªå°çš„ä¿®æ”¹ï¼šåœ¨tokensï¼ˆpatch tokenså’Œpos tokensç›¸åŠ ï¼‰è¢«é€åˆ°Transformerä¹‹å‰ï¼Œè®©tokenså…ˆç»è¿‡ä¸€ä¸ªlayer normå±‚ï¼Œæ­¤å¤–å‚æ•°çš„åˆå§‹åŒ–å’ŒåŸæ¥çš„ViTsä¹Ÿæœ‰å¾®å°çš„ä¸åŒã€‚

##### Other configuration

- optimizerï¼šAdam
- training epochsï¼š32
- batch sizeï¼š32768
- precisionï¼šhalf-precision

***

### Experiment

å®éªŒéƒ¨åˆ†è¿™é‡Œé‡ç‚¹focusåœ¨CVç›¸å…³éƒ¨åˆ†ã€‚

#### Zero-shot CLIP v.s. Linear Probe on ResNet50

{% asset_img 7.webp %}

CLIPçš„èƒœç‡åœ¨16/27ï¼Œå·²ç»å¾ˆå¼ºäº†ï¼Œå› ä¸ºCLIPæ˜¯zero-shotçš„ï¼Œå³æ²¡æœ‰ç”¨ä¸‹æ¸¸ä»»åŠ¡çš„æ•°æ®ï¼Œè€Œlinear probed ResNet50ç”¨äº†ä¸‹æ¸¸æ•°æ®è¿›è¡Œfinetuneé€»è¾‘å›å½’åˆ†ç±»å™¨çš„å‚æ•°ã€‚

***

#### Prompt engineering and ensembling

ä½œè€…é»˜è®¤promptæ¨¡æ¿æ˜¯ï¼š"A photo of a {label}."ï¼Œä½†ä½œè€…å‘ç°è¿™æ ·çš„æ¨¡æ¿è¿˜æ˜¯æœ‰ç‚¹ç²—ç³™ï¼Œå¯ä»¥è€ƒè™‘åŠ ä¸€äº›contextæ¯”å¦‚"A photo of a {label}, a type of pet."ã€‚å¯¹äºä¸åŒç±»å‹ä»»åŠ¡ï¼Œä½œè€…åšäº†ä¸€äº›æ‰‹åŠ¨çš„ã€ç‰¹å®šçš„promptå·¥ç¨‹ã€‚

ä»å¦ä¸€ä¸ªè§’åº¦ï¼Œä¸€å¼ å›¾çš„textæè¿°å…¶å®æœ‰å¾ˆå¤šç§çš„ï¼Œåªè¦textçš„æ ¸å¿ƒè¯­ä¹‰å’Œimageç›¸åŒå°±è¡Œï¼Œé‚£ä¹ˆæˆ‘ä»¬è¿˜å¯ä»¥åšä¸€äº›ensembleï¼Œæ¯”å¦‚ensembleä¸€ä¸‹"A photo of a big {label}."å’Œ"A photo of a small {label}."ã€‚

{% asset_img 8.webp %}

å¯ä»¥å‘ç°ï¼Œé‡‡ç”¨Prompt engineering+ensemblingçš„æ•ˆæœæ¯”åªç”¨æ²¡æœ‰ä¸Šä¸‹æ–‡çš„ç±»åˆ«åå¥½å¾—å¤šã€‚

ï¼ˆPSï¼šä½œè€…è¿™é‡Œçš„å‘ç°ç›´æ¥motivateäº†ä¹‹åçš„CoOp[2]ï¼ŒCoCoOp[3]ä¹‹ç±»learnable promptingçš„å·¥ä½œï¼Œåé¢æœ‰æ—¶é—´æˆ‘ä¼šä¸“é—¨å†™ä¸€æœŸå…³äºè¿™ä¸ªçš„ã€‚ï¼‰

***

#### Few-shot CLIP v.s. SOTA (ImageNet) SSL methods

{% asset_img 9.webp %}

y: 20ä¸ªæµ‹è¯•æ•°æ®é›†ä¸Šçš„å¹³å‡å¾—åˆ†; x: shots

- Zero-shot CLIPçš„æ€§èƒ½å’Œ4-shot CLIPå·®ä¸å¤š
- Few-shot CLIPçš„performanceè¿œé«˜äºä¹‹å‰çš„SOTAæ¨¡å‹

***

#### How many shots is needed for achieving zero-shot performance

{% asset_img 10.webp %}

Few-shot (linear probing) CLIP ï¼ˆä¿æŒCLIP encoder å‚æ•°fixedï¼ŒåŠ ä¸€å±‚é€»è¾‘å›å½’åˆ†ç±»å™¨å¾®è°ƒï¼‰å¹³å‡éœ€è¦20.8-shotsæ‰èƒ½match zero-shot CLIPæ€§èƒ½ã€‚è¿™é‡Œç›¸å½“äºä¿æŒäº†the same CLIP feature spaceä¸Šï¼Œè§‚å¯Ÿfew-shot finetuningå’Œzero-shotçš„æ€§èƒ½å·®å¼‚ã€‚è¿™é‡Œå…¶å®è¯´æ˜é€šè¿‡è‡ªç„¶è¯­è¨€å­¦åˆ°çš„è§†è§‰æ¦‚å¿µæ¯”å°‘é‡æ ·æœ¬finetuneå­¦åˆ°çš„å¥½ã€‚

***

#### Linear probing CLIP performance

è¿™é‡Œä¸å†æ˜¯few-shot linear probingäº†ï¼Œè€Œæ˜¯å…¨é‡æ•°æ®çš„linear probingï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹å…¶è·Ÿzero-shotæ€§èƒ½çš„å¯¹æ¯”ï¼š

{% asset_img 11.webp %}

æ€»ä½“ä¸Šï¼Œä¸¤è€…çš„æ€§èƒ½æ˜¯æ­£ç›¸å…³çš„ï¼Œæ­¤å¤–ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸‹linear probingçš„æ€§èƒ½è¦å¥½ä¸å°‘ã€‚

å†æ¥ä¸€ä¸ªlinear probingçš„å¤©æ¢¯å›¾ï¼š

{% asset_img 12.webp %}

CLIP GOATï¼ï¼ï¼

***

#### Robustness to Natural Distribution Shift

ä½œè€…åœ¨ImageNetçš„7ä¸ªshift datasetsä¸Šè§‚å¯Ÿå„æ¨¡å‹çš„å¹³å‡æ€§èƒ½ã€‚

{% asset_img 13.webp %}

è¯´å®è¯ï¼Œåšdomain adaptationï¼ˆDAï¼‰/generalizationï¼ˆDGï¼‰çš„äººçœ‹åˆ°è¿™é‡Œåº”è¯¥æŒºå…´å¥‹ï¼Œæ–°çš„é²æ£’ç‰¹å¾æ¥å•¦ã€‚ä¸è¿‡é—®é¢˜æ¥äº†ï¼Œå·¦è¾¹è¿™å¼ å›¾æ˜¯ä¸æ˜¯ä¹Ÿåæ˜ äº†representation learningæ¯”DAã€DG techniqueæ›´é‡è¦å‘¢ï¼Ÿï¼ˆé‚£ä¹ˆï¼Œæˆ‘ä»¬çœŸçš„éœ€è¦èŠ±é‚£ä¹ˆå¤§åŠ›æ°”å»å·DAå˜›... è¯´ä¸å®šé€šè¿‡è¿™ç§å¤§è§„æ¨¡pretrainingå°±èƒ½å¾ˆå¤§ç¨‹åº¦ä¸Šè§£å†³domain shiftçš„é—®é¢˜ã€‚ä½†å¦ä¸€æ–¹é¢ï¼ŒDAã€DGä¹Ÿå¯ä»¥åœ¨è¿™äº›pretrainingå¾—åˆ°çš„è¡¨å¾ä¸Šé”¦ä¸Šæ·»èŠ±ã€‚æ€ä¹ˆè¯´éƒ½æœ‰é“ç†ï¼Œä½†æˆ‘æ›´prefer to CLIPè¿™ç±»è¡¨å¾å­¦ä¹ çš„æ„ä¹‰ã€‚

CLIPçš„å®éªŒéå¸¸ä¸°å¯Œï¼Œè¿™é‡Œåªæ˜¯æŠ›ç –å¼•ç‰åœ°æŒ‘äº†å‡ ä¸ªæˆ‘ä¸ªäººè§‰å¾—æ¯”è¾ƒæœ‰æ„æ€çš„å®éªŒè®²ï¼Œå…·ä½“åœ°è¿˜æ˜¯æ¨èå¤§å®¶å»çœ‹åŸæ–‡ã€‚

***

### Limitation

è¿™ä¸ªéƒ¨åˆ†å¾€å¾€å®¹æ˜“è¢«äººå¿½ç•¥ï¼Œä½†å…¶å®ä¸ªäººè§‰å¾—ï¼Œlimitationå’Œconclusionéƒ¨åˆ†å¾€å¾€æœ‰ä½œè€…ä»¬æ›´æ·±å…¥çš„æ€è€ƒï¼Œè¿™é‡Œç®€å•æ€»ç»“ä¸‹CLIPçš„limitationï¼š

- CLIPçš„zero-shotæ€§èƒ½è™½ç„¶æ€»ä½“ä¸Šæ¯”supervised baseline ResNet-50è¦å¥½ï¼Œä½†å…¶å®åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šæ¯”ä¸è¿‡SOTA methodsï¼Œå› æ­¤CLIPçš„transfer learningæœ‰å¾…æŒ–æ˜
- CLIPåœ¨è¿™å‡ ç§taskä¸Šzero-shotæ€§èƒ½ä¸å¥½ï¼šfine-grainedåˆ†ç±»ï¼ˆèŠ±çš„åˆ†ç±»ã€è½¦çš„åˆ†ç±»ä¹‹ç±»çš„ï¼‰ã€æŠ½è±¡çš„ä»»åŠ¡ï¼ˆå¦‚è®¡ç®—å›¾ä¸­objectçš„ä¸ªæ•°ï¼‰ä»¥åŠé¢„è®­ç»ƒæ—¶æ²¡è§è¿‡çš„taskï¼ˆå¦‚åˆ†å‡ºç›¸é‚»è½¦è¾†çš„è·ç¦»ï¼‰ã€‚BTWï¼Œåœ¨è¿™äº›ä»»åŠ¡ä¸Šzero-shotæ€§èƒ½ä¸å¥½ï¼Œä¸ä»£è¡¨CLIP pretrained encoderså°±æ²¡ç”¨äº†ï¼ŒCLIP encodersè¿˜æ˜¯èƒ½æä¾›å¾ˆå¼ºçš„è§†è§‰å…ˆéªŒçš„
- Zero-shot CLIPåœ¨çœŸæ­£æ„ä¹‰ä¸Šçš„out-of-distribution dataä¸Šæ€§èƒ½ä¸å¥½ï¼Œæ¯”å¦‚åœ¨OCRä¸­
- å°½ç®¡CLIP zero-shot classifierèƒ½åœ¨å¾ˆå¹¿æ³›çš„ä»»åŠ¡ä¸Šworkï¼Œä½†ç©¶å…¶æœ¬è´¨CLIPè¿˜æ˜¯åœ¨æœ‰é™çš„ç±»åˆ«ä¸­è¿›è¡Œå¯¹æ¯”ã€æ¨ç†ï¼Œè€Œä¸èƒ½åƒimage captioné‚£æ ·å®Œå…¨çš„flexibleåœ°ç”Ÿæˆæ–°çš„æ¦‚å¿µï¼ˆå¦‚ï¼šè¯ï¼‰ï¼Œè¿™æ˜¯CLIPåŠŸèƒ½ä¸Šçš„ç¼ºé™·ï¼ŒCLIPç»ˆç©¶ä¸æ˜¯ç”Ÿæˆæ¨¡å‹
- CLIPä»ç„¶æ²¡æœ‰è§£å†³æ·±åº¦å­¦ä¹ poor data efficiencyçš„é—®é¢˜ï¼Œç»“åˆCLIPå’Œself-trainingå¯èƒ½æ˜¯ä¸€ä¸ªèƒ½æé«˜data efficiencyçš„æ–¹å‘
- CLIPçš„æ–¹æ³•è®ºä¸Šä¹Ÿå­˜åœ¨å‡ ä¸ªç¼ºé™·ï¼šåœ¨è®­ç»ƒå’ŒæŒ‘é€‰CLIPæ¨¡å‹æ—¶ï¼Œä½œè€…é‡‡ç”¨åœ¨å‡ ä¸ªæ•°æ®çš„validation performanceæ¥åšæŒ‡å¯¼ï¼Œè¿™å…¶å®æ˜¯ä¸å‡†ç¡®çš„ï¼Œå› ä¸ºå®ƒä¸èƒ½å®Œå…¨ä»£è¡¨CLIPçš„zero-shotæ€§èƒ½ã€‚å¦‚æœï¼Œè®¾è®¡ä¸€å¥—æ¡†æ¶æ¥evaluate zero-shot performanceå¯¹äºä¹‹åçš„ç ”ç©¶æ˜¯å¾ˆé‡è¦çš„
- CLIPçš„è®­ç»ƒæ•°æ®æ˜¯ä»ç½‘ä¸Šé‡‡é›†çš„ï¼Œè¿™äº›image-text pairsæ²¡æœ‰åšdata clearå’Œde-biasï¼Œè¿™å¯èƒ½ä¼šä½¿æ¨¡å‹æœ‰ä¸€äº›social biases
- å¾ˆå¤šè§†è§‰ä»»åŠ¡å¾ˆéš¾ç”¨textæ¥è¡¨è¾¾ï¼Œå¦‚ä½•ç”¨æ›´é«˜æ•ˆçš„few-shot learningæ–¹æ³•ä¼˜åŒ–CLIPä¹Ÿå¾ˆé‡è¦

åˆ°æ­¤ï¼ŒCLIPåŸºæœ¬è®²å®Œï¼Œæ€»ä½“æ¥è¯´ï¼Œå¯¹äºæ·±åº¦å­¦ä¹ æ¥è¯´æ˜¯ä¼˜åŒ–æ—¶ä»£æ„ä¹‰çš„ï¼Œè¿™å¯èƒ½æ ‡å¿—ç€æˆ‘ä»¬å³å°†è¿æ¥data-centric deep learningæ—¶ä»£ï¼Œå°è¯äº†Andrew Ngçš„ä¸€å¥åè¨€ï¼šâ€œYour model is good enough. Foucs on the data!â€(å¤§æ¦‚è¿™ä¸ªæ„æ€ï¼Œè¯å¥ä¸å®Œå…¨å‡†ç¡®)ã€‚

***

### å‚è€ƒé“¾æ¥

> [1] [Zhang Y, Jiang H, Miura Y, et al. Contrastive learning of medical visual representations from paired images and text[J]. arXiv preprint arXiv:2010.00747, 2020.](https://arxiv.org/pdf/2010.00747.pdf)
> [2] [Zhou K, Yang J, Loy C C, et al. Learning to prompt for vision-language models[J]. arXiv preprint arXiv:2109.01134, 2021.](https://arxiv.org/pdf/2109.01134.pdf)
> [3] [Zhou K, Yang J, Loy C C, et al. Conditional Prompt Learning for Vision-Language Models[J]. arXiv preprint arXiv:2203.05557, 2022.](https://arxiv.org/pdf/2203.05557.pdf)

***

### åŸæ–‡é“¾æ¥

> <https://zhuanlan.zhihu.com/p/486857682>
