---
title: SFFAI 105 | 强化学习专题《翁文涛：双Q-学习的均方误差》
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: a02ff8b3
date: 2021-05-09 22:19:53
tags:
---

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=464151869&bvid=BV1kL411u78Z&cid=443894019&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

<!--more-->

***

### 讲者介绍

**翁文涛：**清华大学姚班大四在读。主要研究兴趣为大规模随机系统中的算法与设计，研究问题包括云上的调度问题、按需服务平台、以及强化学习理论。

**报告题目：**双Q-学习的均方误差

**报告摘要：**本文在理论上严格比较了双Q-学习和Q-学习均方误差。基于最优策略的唯一性和算法的收敛性假设，本文基于线性随机近似理论对Q-学习和双Q-学习的Lyapunov方程进行了分析。该分析对表格型和带线性函数近似的情况均成立。我们证明了当双Q-学习的学习率为Q-学习的学习率的两倍，且输出其两个值函数估计的平均值时，渐近意义上双Q-学习的均方误差与Q-学习的均方误差是一致的。我们使用仿真实验进一步提供了该理论结果的实际意义。

**论文标题：**The Mean-Squared Error of Double Q-Learning
 
**Spotlight：**
1. 本文首次严格给出双Q学习准确的渐进意义上的均方误差；
2. 本文提供了以渐进均方误差为指标比较两种强化学习方法的理论框架；
3. 本文根据文中的理论发现，提出了改进双Q学习均方误差的简单方法。

***

### 论文推荐

#### 领域经典

1. The ODE method for convergence of stochastic approximation and reinforcement learning

    **推荐理由：**使用ODE方法研究强化学习收敛性的开创文章。是这一领域的必读文献。

2. An Analysis of Temporal-Difference Learning with Function Approximation

    **推荐理由：**该文章分析了带函数近似的TD学习的收敛性。其中的许多建模与证明思想对现在的强化学习理论产生了深远的影响。

3. Finite-Time Error Bounds For Linear Stochastic Approximation and TD Learning

    **推荐理由：**该文章将控制论中的Lyapunov分析思想巧妙地引入到了强化学习的理论分析中，开启了一大片研究方向。

#### 领域前沿

4. Q-learning with Uniformly Bounded Variance

    **推荐理由：**该文章全面的介绍了如何使用随机近似理论分析Q-学习，并提出了改进Q-学习均方误差的方法。

5. Explicit Mean-Square Error Bounds for Monte-Carlo and Linear Stochastic Approximation

    **推荐理由：**该文章介绍了如何从理论上精准地计算线性随机近似方法的均方误差。

6. Finite-Time Analysis for Double Q-learning

    **推荐理由：**该文章首次刻画了表格型双Q-学习在有限时间内均方误差的收敛速度。

***

### 参考资料

> <https://www.bilibili.com/video/BV1kL411u78Z/>
