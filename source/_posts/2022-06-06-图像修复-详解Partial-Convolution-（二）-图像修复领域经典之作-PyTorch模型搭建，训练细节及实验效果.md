---
title: å›¾åƒä¿®å¤-è¯¦è§£Partial Convolution ï¼ˆäºŒï¼‰ | å›¾åƒä¿®å¤é¢†åŸŸç»å…¸ä¹‹ä½œ | PyTorchæ¨¡å‹æ­å»ºï¼Œè®­ç»ƒç»†èŠ‚åŠå®éªŒæ•ˆæœ
categories:
  - ğŸŒ™è¿›é˜¶å­¦ä¹ 
  - â­äººå·¥æ™ºèƒ½ Artificial Intelligence
  - ğŸ’«ç ”ç©¶é¢†åŸŸ Research Area
  - ğŸ›°ï¸è®¡ç®—æœºè§†è§‰ Computer Vision
  - â˜„ï¸å›¾åƒä¿®å¤ Image Inpainting
abbrlink: 9d77de93
date: 2022-06-06 05:16:33
tags:
---

### è®ºæ–‡ä¿¡æ¯

- è®ºæ–‡æ ‡é¢˜ï¼šã€ŠImage Inpainting for Irregular Holes Using Partial Convolutionsã€‹
- è®ºæ–‡é“¾æ¥ï¼š[ECCV 2018 Open Access Repository](https://openaccess.thecvf.com/content_ECCV_2018/html/Guilin_Liu_Image_Inpainting_for_ECCV_2018_paper.html)

<!--more-->

***

### PyTorchæ¨¡å‹æ­å»º

#### PConvå·ç§¯å±‚æ­å»º

ä¸Šä¸€ç¯‡Partial Convolutionï¼ˆPConvï¼‰åšæ–‡ä¸­[1]ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†PConvçš„è¿ä½œæœºåˆ¶ã€‚å…¶å…¬å¼å¯ä»¥å†™ä¸ºï¼š

{% asset_img 1.webp %}

å…¶ä¸­åŒ…æ‹¬ä¸¤ä¸ªæ­¥éª¤ï¼šPConvå·ç§¯è¿ç®—å’ŒMaskæ›´æ–°è¿‡ç¨‹ã€‚æˆ‘ä»¬çœ‹[NVIDIAçš„æºä»£ç ](https://github.com/NVIDIA/partialconv)æ˜¯å¦‚ä½•å®ç°PConvå·ç§¯çš„:

``` python
class PartialConv2d(nn.Conv2d):
    def __init__(self, *args, **kwargs):
        # whether the mask is multi-channel or not
        if 'multi_channel' in kwargs:
            self.multi_channel = kwargs['multi_channel']
            kwargs.pop('multi_channel')
        else:
            self.multi_channel = False  
        if 'return_mask' in kwargs:
            self.return_mask = kwargs['return_mask']
            kwargs.pop('return_mask')
        else:
            self.return_mask = False
        super(PartialConv2d, self).__init__(*args, **kwargs)
        if self.multi_channel:
            self.weight_maskUpdater = torch.ones(self.out_channels, self.in_channels, self.kernel_size[0], self.kernel_size[1])
        else:
            self.weight_maskUpdater = torch.ones(1, 1, self.kernel_size[0], self.kernel_size[1])
            
        self.slide_winsize = self.weight_maskUpdater.shape[1] * self.weight_maskUpdater.shape[2] * self.weight_maskUpdater.shape[3]
        self.last_size = (None, None, None, None)
        self.update_mask = None
        self.mask_ratio = None

    def forward(self, input, mask_in=None):
        assert len(input.shape) == 4
        if mask_in is not None or self.last_size != tuple(input.shape):
            self.last_size = tuple(input.shape)
            with torch.no_grad():
                if self.weight_maskUpdater.type() != input.type():
                    self.weight_maskUpdater = self.weight_maskUpdater.to(input)
                if mask_in is None:
                    # if mask is not provided, create a mask
                    if self.multi_channel:
                        mask = torch.ones(input.data.shape[0], input.data.shape[1], input.data.shape[2], input.data.shape[3]).to(input)
                    else:
                        mask = torch.ones(1, 1, input.data.shape[2], input.data.shape[3]).to(input)
                else:
                    mask = mask_in
                self.update_mask = F.conv2d(mask, self.weight_maskUpdater, bias=None, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=1)

                # for mixed precision training, change 1e-8 to 1e-6
                self.mask_ratio = self.slide_winsize/(self.update_mask + 1e-8)
                # self.mask_ratio = torch.max(self.update_mask)/(self.update_mask + 1e-8)
                self.update_mask = torch.clamp(self.update_mask, 0, 1)
                self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)
        raw_out = super(PartialConv2d, self).forward(torch.mul(input, mask) if mask_in is not None else input)
        if self.bias is not None:
            bias_view = self.bias.view(1, self.out_channels, 1, 1)
            output = torch.mul(raw_out - bias_view, self.mask_ratio) + bias_view
            output = torch.mul(output, self.update_mask)
        else:
            output = torch.mul(raw_out, self.mask_ratio)
        if self.return_mask:
            return output, self.update_mask
        else:
            return output
```

æˆ‘ä»¬çœ‹åˆ°å…¶å®æ˜¯ç»§æ‰¿äº†PyTorchä¸­nn.Conv2dçš„ç±»ï¼Œåœ¨å…¶åŸºç¡€ä¸Šè¿›è¡Œç¼–å†™ã€‚è°ƒç”¨æ–¹æ³•éå¸¸ç®€å•ï¼ŒåŸºæœ¬å’Œnn.Conv2dä¸€æ ·ï¼Œé™¤äº†å¤šäº†ä¸€ä¸ªmulti_channelçš„å¼€å…³ï¼Œä¸€èˆ¬è®¾ç½®ä¸ºmulti_channel = Trueã€‚é‚£ä¹ˆå®šä¹‰ä¸€å±‚3Ã—3ï¼Œæ­¥é•¿å’Œpaddingéƒ½ä¸º1çš„PConvå±‚å¯ä»¥å¦‚ä¸‹å®šä¹‰ï¼š

``` python
self.conv = PartialConv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=bias, multi_channel = True)
```

åœ¨æ‰§è¡Œforwardæ—¶ï¼Œç”±äºè¿˜æœ‰maskå‚ä¸è¿ç®—ï¼Œå› æ­¤æœ‰images, masksä¸¤ä¸ªå…¥å‚å’Œå‡ºå‚ï¼š

``` python
images, masks = self.conv(images, masks)
```

***

#### æ•´ä½“ç½‘ç»œæ­å»º

å®šä¹‰å¥½äº†PartialConv2dåï¼Œæˆ‘ä»¬ä¾¿å¾ˆå®¹æ˜“æ­å»ºç½‘ç»œï¼š

{% asset_img 2.webp %}
<div align='center'>Partial Convolutional U-Net ç½‘ç»œç»“æ„ã€‚å›¾ç‰‡å‡ºè‡ª[2]</div>

``` python
# --------------------------
# PConv-BatchNorm-Activation
# --------------------------
class PConvBNActiv(nn.Module):
    def __init__(self, in_channels, out_channels, bn=True, sample='none-3', activ='relu', bias=False):
        super(PConvBNActiv, self).__init__()
        if sample == 'down-7':
            self.conv = PartialConv2d(in_channels, out_channels, kernel_size=7, stride=2, padding=3, bias=bias, multi_channel = True)
        elif sample == 'down-5':
            self.conv = PartialConv2d(in_channels, out_channels, kernel_size=5, stride=2, padding=2, bias=bias, multi_channel = True)
        elif sample == 'down-3':
            self.conv = PartialConv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=bias, multi_channel = True)
        else:
            self.conv = PartialConv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=bias, multi_channel = True)
        if bn:
            self.bn = nn.BatchNorm2d(out_channels)
        if activ == 'relu':
            self.activation = nn.ReLU()
        elif activ == 'leaky':
            self.activation = nn.LeakyReLU(negative_slope=0.2)

    def forward(self, images, masks):
        images, masks = self.conv(images, masks)
        if hasattr(self, 'bn'):
            images = self.bn(images)
        if hasattr(self, 'activation'):
            images = self.activation(images)

        return images, masks

# ------------
# Double U-Net
# ------------
class PUNet(nn.Module):
    def __init__(self, in_channels=3, out_channels=3, up_sampling_node='nearest', init_weights=True):
        super(PUNet, self).__init__()
        self.freeze_ec_bn = False
        self.up_sampling_node = up_sampling_node
        self.ec_images_1 = PConvBNActiv(in_channels, 64, bn=False, sample='down-7')
        self.ec_images_2 = PConvBNActiv(64, 128, sample='down-5')
        self.ec_images_3 = PConvBNActiv(128, 256, sample='down-5')
        self.ec_images_4 = PConvBNActiv(256, 512, sample='down-3')
        self.ec_images_5 = PConvBNActiv(512, 512, sample='down-3')
        self.ec_images_6 = PConvBNActiv(512, 512, sample='down-3')
        self.ec_images_7 = PConvBNActiv(512, 512, sample='down-3')
        self.dc_images_7 = PConvBNActiv(512 + 512, 512, activ='leaky')
        self.dc_images_6 = PConvBNActiv(512 + 512, 512, activ='leaky')
        self.dc_images_5 = PConvBNActiv(512 + 512, 512, activ='leaky')
        self.dc_images_4 = PConvBNActiv(512 + 256, 256, activ='leaky')
        self.dc_images_3 = PConvBNActiv(256 + 128, 128, activ='leaky')
        self.dc_images_2 = PConvBNActiv(128 + 64, 64, activ='leaky')
        self.dc_images_1 = PConvBNActiv(64 + out_channels, out_channels, bn=False, sample='none-3', activ=None, bias=True)
        self.tanh = nn.Tanh()
        if init_weights:
            self.apply(weights_init())

    def forward(self, input_images, input_masks):
        ec_images = {}
        ec_images['ec_images_0'], ec_images['ec_images_masks_0'] = input_images, input_masks
        ec_images['ec_images_1'], ec_images['ec_images_masks_1'] = self.ec_images_1(input_images, input_masks)
        ec_images['ec_images_2'], ec_images['ec_images_masks_2'] = self.ec_images_2(ec_images['ec_images_1'], ec_images['ec_images_masks_1'])
        ec_images['ec_images_3'], ec_images['ec_images_masks_3'] = self.ec_images_3(ec_images['ec_images_2'], ec_images['ec_images_masks_2'])
        ec_images['ec_images_4'], ec_images['ec_images_masks_4'] = self.ec_images_4(ec_images['ec_images_3'], ec_images['ec_images_masks_3'])
        ec_images['ec_images_5'], ec_images['ec_images_masks_5'] = self.ec_images_5(ec_images['ec_images_4'], ec_images['ec_images_masks_4'])
        ec_images['ec_images_6'], ec_images['ec_images_masks_6'] = self.ec_images_6(ec_images['ec_images_5'], ec_images['ec_images_masks_5'])
        ec_images['ec_images_7'], ec_images['ec_images_masks_7'] = self.ec_images_7(ec_images['ec_images_6'], ec_images['ec_images_masks_6'])
        # --------------
        # images decoder
        # --------------
        dc_images, dc_images_masks = ec_images['ec_images_7'], ec_images['ec_images_masks_7']
        for _ in range(7, 0, -1):
            ec_images_skip = 'ec_images_{:d}'.format(_ - 1)
            ec_images_masks = 'ec_images_masks_{:d}'.format(_ - 1)
            dc_conv = 'dc_images_{:d}'.format(_)
            dc_images = F.interpolate(dc_images, scale_factor=2, mode=self.up_sampling_node)
            dc_images_masks = F.interpolate(dc_images_masks, scale_factor=2, mode=self.up_sampling_node)
            dc_images = torch.cat((dc_images, ec_images[ec_images_skip]), dim=1)
            dc_images_masks = torch.cat((dc_images_masks, ec_images[ec_images_masks]), dim=1)
            dc_images, dc_images_masks = getattr(self, dc_conv)(dc_images, dc_images_masks)
        outputs = self.tanh(dc_images)

        return outputs
```

***

### è®­ç»ƒç»†èŠ‚

#### æŸå¤±å‡½æ•°

Partial Convolutionçš„ä½œè€…é‡‡ç”¨äº†è‡ªç›‘ç£å­¦ä¹ çš„æ–¹æ³•ï¼Œè€Œæ²¡æœ‰é‡‡ç”¨GANä¸­çš„å¯¹æŠ—æŸå¤±ã€‚é‡‡ç”¨çš„æŸå¤±å‡½æ•°æœ‰ä»¥ä¸‹å‡ ç§ï¼š

- é¦–å…ˆè®¡ç®—å…¶L1 lossï¼š

{% asset_img 3.webp %}

- å†æ¬¡ï¼Œæ˜¯å’Œperceptual lossç±»ä¼¼çš„style loss:

{% asset_img 4.webp %}

- æœ€åæ˜¯ä¸€é¡¹total variation (TV) lossï¼š

{% asset_img 5.webp %}

- æ€»æŸå¤±ä¸ºä¸Šè¿°æŸå¤±çš„ç»¼åˆï¼š

{% asset_img 6.webp %}

***

#### å…¶å®ƒç»†èŠ‚

ä½œè€…é‡‡ç”¨Adamä¼˜åŒ–å™¨è®­ç»ƒï¼Œåœ¨ä¸€å¼  NVIDIA V100 GPU (16GB) ä¸Šä»¥ batch size ä¸º 6 è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒè¿‡ç¨‹åˆ†ä¸º Initial Training å’Œ Fine-Tuning ä¸¤ä¸ªé˜¶æ®µï¼šåœ¨ Initial Training é˜¶æ®µï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º0.0002ï¼Œå¯ç”¨ batch normalization; åœ¨ Fine-Tuning é˜¶æ®µï¼Œå­¦ä¹ ç‡è®¾ç½®ä¸º 0.00005ï¼Œå¹¶ä¸”å›ºå®šä½ç½‘ç»œ encoder éƒ¨åˆ†çš„ batch normalizationã€‚

åœ¨ Imagenet å’Œ Places2 ä¸Šï¼ŒInitial Training æŒç»­10å¤©ï¼›åœ¨ CelebA-HQï¼ŒInitial Training æŒç»­3å¤©ã€‚æ‰€æœ‰çš„ Fine-Tuning éƒ½åœ¨1å¤©å†…å®Œæˆã€‚

***

### å®éªŒæ•ˆæœ

åœ¨æœ¬æ–‡ä¸­ï¼Œé™¤äº†æå‡ºå°†partial convolutionåº”ç”¨åˆ°å›¾åƒä¿®å¤é¢†åŸŸï¼Œä½œè€…è¿˜æå‡ºäº†ï¼šå›¾åƒä¿®å¤åº”è¯¥æ˜¯ä¿®å¤ä¸è§„åˆ™çš„æŸåå›¾ç‰‡çš„ã€‚å› æ­¤ä½œè€…è¿˜æä¾›äº†ä»–ä»¬çš„ä¸è§„åˆ™maskæ•°æ®é›†Irregular Mask Datasetï¼Œå¹¶ä¸”hole-to-image area ratiosæœ‰6ç§ä¸åŒçš„å¤§å°: (0.01, 0.1], (0.1, 0.2], (0.2, 0.3], (0.3, 0.4], (0.4, 0.5], (0.5, 0.6]ã€‚åœ¨ä¹‹åçš„å›¾åƒä¿®å¤å·¥ä½œä¸­ï¼Œç»å¤§å¤šæ•°éƒ½é‡‡ç”¨äº†æœ¬æ–‡æä¾›çš„Irregular Mask Datasetè¿›è¡Œå¯¹æ¯”å®éªŒã€‚

{% asset_img 7.webp %}
<div align='center'>Irregular Mask Datasetä¸­ä¸€äº›æµ‹è¯•maskå›¾ç‰‡</div>

***

#### Qualitative Comparison

ä¸‹é¢æ˜¯è®ºæ–‡ä¸­æ‰€å±•ç¤ºçš„ä¸€äº›å®éªŒæ•ˆæœï¼š

{% asset_img 8.webp %}
<div align='center'>ImagNetæ•°æ®é›†ä¸Šå®éªŒæ•ˆæœå¯¹æ¯”</div>

{% asset_img 9.webp %}
<div align='center'>Places2æ•°æ®é›†ä¸Šå®éªŒæ•ˆæœå¯¹æ¯”</div>

å¦å¤–ä½œè€…è¿˜ç»™å‡ºäº†æ™®é€šå·ç§¯ä¸PConvçš„å¯¹æ¯”ï¼š

{% asset_img 10.webp %}
<div align='center'>æ™®é€šå·ç§¯ä¸PConvçš„æ•ˆæœå¯¹æ¯”</div>

***

#### Quantitative Comparison

ä½œè€…é‡‡ç”¨äº†L1 lossï¼ŒPSNR, SSIMå’ŒIScoreä½œä¸ºå¯¹æ¯”æŒ‡æ ‡ï¼Œå¹¶åœ¨ä¸ç”¨çš„mask ratiosä¸Šç»™å‡ºäº†ç»“æœï¼š

{% asset_img 11.webp %}

***

### å‚è€ƒèµ„æ–™

> <https://zhuanlan.zhihu.com/p/519446359>
> [1] [å¡å¡çŒ¡ç‰¹ï¼šè¯¦è§£Partial Convolution ï¼ˆä¸€ï¼‰ | å›¾åƒä¿®å¤é¢†åŸŸç»å…¸ä¹‹ä½œ | è¿ç®—æœºåˆ¶åŠæ¨¡å‹ç»“æ„](https://zhuanlan.zhihu.com/p/519446359)
> [2] [ææ–‡é‘«ï¼šPartial Convolutions for Image Inpainting](https://zhuanlan.zhihu.com/p/139405565)