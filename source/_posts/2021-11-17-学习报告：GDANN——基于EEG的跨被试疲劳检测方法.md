---
title: 学习报告：GDANN——基于EEG的跨被试疲劳检测方法
categories:
  - 🌙进阶学习
  - ⭐脑机接口与混合智能研究团队（BCI团队）
  - 💫学习报告
abbrlink: acb8882
date: 2021-11-17 15:11:57
tags:
---

本篇学习报告的内容为2021年发表在《Sensors》的一篇基于迁移学习的跨被试疲劳脑电分类的文章《An EEG-Based Transfer Learning Method for Cross-Subject Fatigue Mental State Prediction》。作者结合领域对抗神经网络 (Domain-Adversarial Neural Network, DANN) 和生成对抗网络 (Generative Adversarial Networks, GAN) 提出一种新型网络Generative-DANN (GDANN)。GAN在一定程度上解决了跨被试脑电图不同分布的问题，DANN则在迁移学习上有着出色的表现，特别是在文档分析和图像识别领域，这篇文章首次将其运用于基于EEG的跨被试疲劳检测中。实验的数据来自于作者采集的模拟驾驶数据。作者将DANN、支持向量机 (support vector machine, SVM) 和简单迁移学习 (Easy Transfer Learning , EasyTL) 等多个方法与GDANN比较，结果表明GDANN有着更好的性能，在跨被试测试中能够达到91.63%的平均准确率。

<!--more-->

***

### 研究背景和内容

精神疲劳是一种复杂的生理和心理状态，可导致警觉性、注意力和认知能力下降。全球每年约有130万人在交通事故中丧生，疲劳驾驶是其中的主导因素。因此，在驾驶时进行精神状态检测和预测对于减少疲劳驾驶造成的生命财产损失极为重要。目前已经有了很多疲劳检测的方法，它们可以分为三类：心理测量问卷、行为方法（例如，面部表情、头部运动、眨眼率和眼睛状态）和生理测量（EEG、心电图 (electrocardiogram, ECG)、眼电图 (electrooculogram, EOG) 和表面肌电图）。这些问卷，不仅主观性强，而且无法实时监测疲劳状态，而行为方法极易受到道路环境的干扰，造成一定的判断误差。在生理测量中，EEG 被认为是最有效的精神状态检测方法，因为 EEG 记录了人类大脑皮层神经细胞活动，可以直接反映大脑的即时状态，避免人为主观因素的影响。大多数现有的基于EEG的疲劳驾驶检测方法都集中在提取合适的特征和设计分类器上，这种方法虽然有效，但主要集中在同一被试情况下的脑电分析，这意味着跨被试检测能力仍然不足。如今，基于领域自适应方法的迁移学习模型被广泛应用于自然语言处理和图像分类等许多领域，并有着非常好的性能。DANN是一种典型的领域自适应方法，其最重要的特点是在没有标记目标样本的情况下对齐源域和目标域。它在跨被试的 EEG 分析中仍然存在一些缺点。首先，DANN要求源域和目标域之间的样本是平衡的，而对于跨被试EEG分析，源域中的样本通常远大于目标域中的样本，表现出严重的不平衡。其次，由于跨被试的EEG信号存在显著差异，一些源域样本可能与目标域的分布极为不一致，这将导致“负迁移”，并使DANN在跨域中的性能下降。

***

### 研究方法

该文在现有的DANN()基础上添加了GAN，其中，GAN由生成带有参数和损失值的假目标域数据的生成网络（如图1）和用于识别真实目标域数据和虚假目标域数据的分类网络（如图2）组成。各个损失函数分别为。

{% asset_img 1.webp %}
<div align='center'>图1 GAN的生成网络</div>

{% asset_img 2.webp %}
<div align='center'>图2 GAN的分类网络</div>

{% asset_img 3.webp %}

{% asset_img 4.webp %}
<div align='center'>图3 GAN的训练过程</div>

{% asset_img 5.webp %}
<div align='center'>图4 DANN的训练过程</div>

***

### 实验

脑电图由数字监测系统（Brain Products GmbH，Munich，Germany）记录，采样频率为200 Hz。所有EEG通道的阻抗保持在10 KΩ以下。之后，使用带通滤波器将 EEG 数据保持在1 Hz和30 Hz之间，并采用独立分量分析（ICA）去除由眨眼引起的伪影。去除伪影后，应用具有10%重叠的1 s滑动窗口将EEG信号分割成61个通道，并为每个对象获取每种条件的400个片段。然后，选择在0.1Hz到30 Hz的频率范围的功率谱密度 (PSD) 将三维时间序列片段转换为二维样本数据。PSD的详细示意图如图5所示。

{% asset_img 6.webp %}
<div align='center'>图5 EEG的PSD提取示意图</div>

本文使用DANN，GDANN，SVM和EasyTL四个模型进行交叉对象交叉验证对比，并重复5次得到平均结果。

为了获得稳定的性能，作者对参数λ和进行调整。当优化一个参数时，另一个参数保持不变，来观察它们的敏感性。这两个参数值由以下两个范围确定，分别是:{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9} λ和{1, 2, 3, 4, 5, 6 , 7, 8, 9, 10, 11, 12}为Ns。结果如图6，7所示：

{% asset_img 7.webp %}
<div align='center'>图6 GDANN的参数λ敏感性（Ns=12）</div>

{% asset_img 8.webp %}
<div align='center'>图6 GDANN的参数敏感性（λ =0.5）</div>

由此可知，随着λ的增加，GDANN的平均准确率趋于缓慢增加，为了更好的反映模型的稳健性，选择λ = 0.5；而随着Ns从1增加到9，平均准确率急剧增加，达到9后精度保持稳定，精度曲线可能会略有波动，因此选择设置。模型其他参数还有：Epoch = 50, Batch_Size = 64, Adam_Learning_Rate = 0.00001, Adam_b1 = 0.5, Adam_b2 = 0.999, SGD_Learning_Rate = 0.005 and SGD_Momentum = 0.9。

为了体现所提模型的改进，作者首先比较了GDANN和DANN的性能，然后利用上面得到的最优参数，对其Accuracy、Precision、F1Score、Recall等相关指标的性能进行了分析。在实验中，PSD被提取为训练 GDANN 和 DANN 的输入。相同的训练集和测试集分别用于训练和测试。表1显示了重复实验5次后的平均结果。

<div align='center'>表1 DANN与GDANN混淆矩阵指标对比结果</div>
{% asset_img 9.webp %}

为了验证方法的有效性，作者将其性能与SVM和EasyTL进行比较。SVM是最常见、最高效的有监督机器学习方法，可以用来突出迁移学习的强大性能。EasyTL是另一种有用的迁移学习方法，可用于将性能与GDANN进行比较。这里需要注意的是，在比较的过程中，EasyTL与GDANN有相同的训练集和测试集，而SVM不需要无标签数据的辅助训练，而是来自的训练集和来自的测试集。我们可以得到混淆矩阵对应的箱线图，以及它们配对的t检验的结果，如图7所示。通过比较这四个指标，可以看出我们的模型GDANN比其他三个模型有更好的表现。比如在准确率方面，改进后的模型GDANN平均准确率达到了91.63%，相比原始模型DANN有11.9%的性能提升，而EasyTL平均准确率为72.95%，SVM为67.77%，体现了GDANN 的良好表现。

{% asset_img 10.webp %}
<div align='center'>图7 混淆矩阵的箱线图 (a) Accuracy, (b) Precision, (c) Recall, and (d) F1Score</div>

此外，作者也比较了四种模型的测试时间如图8。该图比较了相应模型所需的平均测试时间，带有正负偏差误差线。可以看出，GDANN（122.80 s）比SVM（102.3 s）、EasyTL（27.44 s）和DANN（85.24 s）花费的时间更多。

{% asset_img 11.webp %}
<div align='center'>图8 四种模型测试时间</div>

***

### 总结

本文结合GAN和DANN提出性能优秀的GDANN，使用GAN生成大致符合目标域数据分布的假数据，在一定程度上解决了来源的问题，避免了某些被试之间相关性弱而导致负迁移的问题。但是相比其他模型，GDANN多了一个额外的生成辅助假数据的过程，这就使得它比其他模型需要花费更多的测试时间。未来可以通过尝试覆盖目标域的所有边缘数据进一步强化GAN的模拟能力，以提高模型的性能。

***

### 参考链接

> <https://www.scholat.com/teamwork/showPostMessage.html?id=10801>
> [1] Zeng, Hong, et al. "An EEG-Based Transfer Learning Method for Cross-Subject Fatigue Mental State Prediction." Sensors 21.7 (2021): 2369.
