---
title: SFFAI 50 | ICCV'19专场 场景解析 & CNN架构优化
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: b6625368
date: 2019-12-01 04:15:30
tags:
---

SFFAI50 - 付君

<iframe src="//player.bilibili.com/player.html?aid=77832706&bvid=BV1oJ411z766&cid=133142384&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<!--more-->

SFFAI50 - 丁霄汉

<iframe src="//player.bilibili.com/player.html?aid=77832706&bvid=BV1oJ411z766&cid=133151318&p=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

关注微信号：人工智能前沿讲习，回复“付君”，“丁霄汉”获取讲者PPT资料。
国际计算机视觉大会ICCV为计算机视觉方向的三大顶级会议之一，会议的论文集代表了计算机视觉领域最新的发展方向和水平。本期论坛我们邀请了两位ICCV 2019论文作者，来分享他们工作中的思考与创想。

***

### 讲者介绍

**付君：**中国科学院自动化研究所在读博士，主要的研究方向图像语义分割。

**报告题目：**Adaptive Context Network for Scene Parsing

**报告摘要：**场景解析中需要预测出场景图像中的像素点属于某一目标类或场景类。上下文的利用对于识别每个像素点十分关键。当前的方法主要是基于全卷积神经网络上将不同上下文融合到高层语义部特征中。具体来说，利用全局上下文来改善局部特征的歧义，利用浅层的局部上下文补充空间细节。这些方法在融合全局上下文和局部上下文到每个像素点时都是同等对待的，但是我们认为不同像素点对于全局上下文和局部上下文的需求是不同的。为此，我们提出了自适应上下文网络，通过联合考虑全局和局部上下文在每个像素区域的关联，提出像素感知的上下文捕获策略，自适应地融合全局上下文和局部上下文，从而获得更加有效的场景解析结果。最后实验表明了我们方法的有效性，此外该方法在多个公开的场景解析数据集上(Cityscapes,ADE20k等)均取得同期最好性能。

**Spotlight：**
1. 分享利用不同上下文的经典方法；
2. 本文提出像素感知的上下文捕获策略，能自适应地融合全局和局部上下文。

**丁霄汉：**清华大学在读博士生，主要研究方向为卷积神经网络的设计与优化。

**报告题目：**非对称卷积模块：零代价地提升卷积网络的精度

**报告摘要：**我们提出非对称卷积模块（Asymmetric Convolution Block），一种架构无关的卷积神经网络基本构件。这一模块使用一维卷积核来增强常用的二维卷积核。相比于常用的卷积层，这种结构在训练的时候可以达到更高的精度。在训练完成后，这种结构可以等效转换成普通的卷积层，所以部署的模型虽然精度更高了，但其结构和普通的网络完全相同。因此我们说，这种性能提升是“白给”的。

**Spotlight：**
1. 提出的是一种通用的卷积神经网络构件，可以被用到任何架构的卷积网络中去；
2. 非常实用和易用，只需要几行代码实现，甚至不需要调参；
3. 不引入任何推理开销（inference-time costs），将你的性能-开销曲线垂直上移。

***

### 论文推荐

1. PARSENET LOOKING WIDER TO SEE BETTER

    **推荐理由：**这篇文章很早采取了全局上下文特征去改进局部区域识别歧义，其文章中深入分析FCN网络引入全局上下文的必要性。此外该文章获得全局上下文特征的方式简单有效，是利用全局上下文特征代表工作，被后续很多分割文章引用。

2. Laplacian Pyramid Reconstruction and Reﬁnement for Semantic Segmentation

    **推荐理由：**这篇文章很早关注到了浅层特征有助于目标边缘细节的恢复，因此设计了拉普拉斯金字塔结构针对性补充边缘细节到高层特征中，逐步改进了目标边缘的分割。这篇文章对于浅层特征的分析以及相关的实验值得关注。

3. Squeeze-and-Excitation Networks

    **推荐理由：**这篇非常著名的文章提出的Squeeze-and-Excitation Block本质上是channel-wise attention，对卷积层的输出的不同channel乘以一个不同的系数，就能显著提升性能，而开销代价不多。而且这篇文章的写作非常好。

4. Soft Conditional Computation

    **推荐理由：**这篇文章提出的Soft Conditional Computation (SCC)用到了一个非常简单但是一直被忽视的原理：多个卷积核先各自卷积再相加，等于先把卷积核相加再卷积。只要在网络里放进数倍于原来的卷积层，并用全连接层做route function得到的系数对这些卷积层的kernel做加权相加，用得到的kernel去做卷积，就能提升性能。这样的网络虽然参数量增加了几倍，但是运算量没有增加很多。

5. Selective Kernel Networks

    **推荐理由：**Selective Kernel Network (SKNet)为了实现（近似）的动态感受野，将3×3卷积和5×5卷积的输出结合。这里的所谓结合也非常简单，也是用全连接层做route function得到系数然后加权相加。也可以提高性能。而且如果用3×3膨胀卷积来近似5×5非膨胀卷积的话，性能损失不大，而参数量和计算量可以大大降低。

***

### 参考资料

> <https://www.bilibili.com/video/BV1oJ411z766/>
