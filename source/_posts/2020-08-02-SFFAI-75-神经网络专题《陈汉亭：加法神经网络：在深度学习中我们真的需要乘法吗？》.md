---
title: SFFAI 75 | 神经网络专题《陈汉亭：加法神经网络：在深度学习中我们真的需要乘法吗？》
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: b3ab0bb9
date: 2020-08-02 05:19:42
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=329447714&bvid=BV16A411n71R&cid=231444108&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<!--more-->

关注微信公众号：【人工智能前沿讲习】，回复【SFFAI75】获取讲者PPT资料，入交流群，推荐论文下载。
随着机器学习所使用的网络愈加复杂，参数量和计算量都在爆炸式增长，训练网络所需要的庞大计算资源和产生的能源消耗已经开始引起研究人员的关注。为了提高性效比，将神经网络模型小型化，减小计算量是十分重要的。本期我们邀请了来自北京大学的陈汉亭同学将为大家分享他在CVPR2020发表的工作，去除网络中的乘法运算，用加法神经网络获得较高学习能力。

***

### 讲者介绍

**陈汉亭：**北京大学智能科学系博士三年级在读，同济大学学士，研究兴趣主要包括计算机视觉、机器学习和深度学习。在ICCV,AAAI,CVPR等会议发表论文数篇，目前主要研究方向为神经网络模型小型化。

**报告题目：**加法神经网络：在深度学习中我们真的需要乘法吗？

**报告摘要：**和廉价的加法操作相比，乘法操作需要更大的计算量。在深度神经网络中广泛应用的卷积计算了输入特征和卷积核的互相关，其中包含着大量乘法操作。我们提出了一种加法神经网络来将 CNN 中的大量乘法操作用加法来代替。具体的，我们将输出特征使用滤波器和输入特征的 L1 距离来度量。通过细致的分析，我们提出了基于全精度梯度的反向传播和自适应学习率来帮助加法神经网络的训练。实验结果表明，我们提出的加法神经网络能够取得和卷积神经网络近似的准确率，并且在卷积层中不含任何乘法。

**Spotlight：**
1. 本文提出了一种新型的加法神经网络，将卷积神经网络中的卷积运算使用加法代替，从而减少了其运算量；
2. 我们提出了改进的梯度和自适应学习率用于加法神经网络的反向传播和训练；
3. 我们提出的加法网络在图像分类大型数据集ImageNet上使用ResNet-50结构取得了74.9% Top-1和91.7% Top-5准确率，和原始卷积网络十分相近。

***

### 论文推荐

#### 3篇领域经典

1. GradientBased Learning Applied to Document Recognition

    **推荐理由：**最经典的的卷积神经网络LeNet-5，将卷积运算引入神经网络中。

2. ImageNet Classification with Deep Convolutional

    **推荐理由：**著名的AlexNet，在2012年的ImageNet图像分类比赛上使用卷积神经网络达到了最低的15.3%的Top-5错误率，比第二名低10.8个百分点。

3. Deep Residual Learning for Image Recognition

    **推荐理由：**将残差连接引入卷积神经网络中，让深层卷积神经网络的训练变得简单起来。

#### 3篇领域前沿

4. MobileNetV2 Inverted Residuals and Linear Bottlenecks

    **推荐理由：**将常规卷积运算拆分为深度可分离卷积，提高了卷积的运算速度。

5. GhostNet More Features from Cheap Operations

    **推荐理由：**提出了一种新的ghost模块，使用部分特征图生成其他的特征图来加速卷积运算。

6. DeepShift Towards Multiplication-Less Neural Networks

    **推荐理由：**使用shift操作来替换卷积中的乘法操作。

***

### 参考资料

> <https://www.bilibili.com/video/BV16A411n71R/>
