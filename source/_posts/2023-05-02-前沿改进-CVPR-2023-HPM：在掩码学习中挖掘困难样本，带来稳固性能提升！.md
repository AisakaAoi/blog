---
title: å‰æ²¿æ”¹è¿›-CVPR 2023 | HPMï¼šåœ¨æ©ç å­¦ä¹ ä¸­æŒ–æ˜å›°éš¾æ ·æœ¬ï¼Œå¸¦æ¥ç¨³å›ºæ€§èƒ½æå‡ï¼
categories:
  - ğŸŒ™è¿›é˜¶å­¦ä¹ 
  - â­äººå·¥æ™ºèƒ½ Artificial Intelligence
  - ğŸ’«å‰æ²¿æ”¹è¿› Frontier Improvement
abbrlink: bf832548
date: 2023-05-02 02:54:05
tags:
---

æœ¬æ–‡ä»‹ç»äº†ä¸€ç¯‡åœ¨è‡ªç›‘ç£æ©ç å­¦ä¹ ï¼ˆMasked Image Modelingï¼‰é¢†åŸŸçš„åŸåˆ›å·¥ä½œ HPM (Hard Patches Mining for Masked Image Modeling)ã€‚ 

å„ç§è‡ªç›‘ç£æ©ç å­¦ä¹ æ–¹æ³•çš„æ€§èƒ½å¼ºçƒˆä¾èµ–äºäººå·¥å®šä¹‰çš„æ©ç ç­–ç•¥ï¼Œè€Œæˆ‘ä»¬æå‡ºä¸€ç§æ–°çš„å›°éš¾æ ·æœ¬æŒ–æ˜ç­–ç•¥ï¼Œè®©æ¨¡å‹è‡ªä¸»åœ°æ©ç å›°éš¾æ ·æœ¬ï¼Œæå‡ä»£ç†ä»»åŠ¡çš„éš¾åº¦ï¼Œä»è€Œè·å¾—å¼ºå¤§çš„è¡¨å¾æå–èƒ½åŠ›ã€‚ç›®å‰ HPM å·²è¢« CVPR 2023 æ¥æ”¶ï¼Œç›¸å…³ä»£ç å·²å¼€æºã€‚

{% asset_img 1.webp %}

- è®ºæ–‡æ ‡é¢˜ï¼šHard Patches Mining for Masked Image Modeling
- å½•ç”¨ä¿¡æ¯ï¼šCVPR 2023, https://arxiv.org/abs/2304.05919
- ä»£ç å¼€æºï¼šhttps://github.com/Haochen-Wang409/HPM

<!--more-->

***

### Introduction

{% asset_img 2.webp %}
<p style="text-align:center">Figure 1. Comparison between conventional MIM pre-training paradigm and our proposed HPM.</p>

åœ¨å…¸å‹çš„ MIM æ–¹æ³•ä¸­ï¼Œæ¨¡å‹é€šå¸¸ä¸“æ³¨äºé¢„æµ‹ masked patches çš„æŸä¸€å½¢å¼çš„ target (ä¾‹å¦‚ BEiT[1]çš„ç¦»æ•£ tokenï¼ŒMAE[2] çš„ pixel RGB)ã€‚è€Œç”±äº CV ä¿¡å·çš„ç¨ å¯†æ€§ï¼ŒMIM æ–¹æ³•é€šå¸¸éœ€è¦é¢„å…ˆå®šä¹‰çš„æ©ç›–ç­–ç•¥ï¼Œä»¥æ„é€ å…·æœ‰æŒ‘æˆ˜æ€§çš„è‡ªç›‘ç£ä»£ç†ä»»åŠ¡ã€‚å¦åˆ™ï¼Œä¸€ä¸ªç®€å•çš„æ’å€¼å°±èƒ½å®Œæˆå¯¹äº masked patches çš„é‡å»ºã€‚æ€»çš„æ¥è¯´ï¼Œæ•´ä¸ª MIM çš„è¿‡ç¨‹å¯ä»¥è¢«è®¤ä¸ºæ˜¯è®­ç»ƒä¸€ä¸ªå­¦ç”Ÿï¼ˆæ¨¡å‹ï¼‰ï¼Œè§£å†³ç»™å®šçš„é—®é¢˜ï¼ˆé‡å»º masked patchesï¼‰ï¼Œå¦‚ä¸Šå›¾ (a) æ‰€ç¤ºã€‚

ç„¶è€Œï¼Œæˆ‘ä»¬è®¤ä¸ºï¼Œæ¨¡å‹ä¸åº”è¯¥åªä¸“æ³¨äºè§£å†³ç»™å®šçš„é—®é¢˜ï¼Œè¿˜åº”è¯¥ç«™åœ¨è€å¸ˆçš„ç«‹åœºä¸Šï¼Œå…·å¤‡è‡ªå·±å‡ºå…·æŒ‘æˆ˜æ€§é—®é¢˜çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ›é€ å…·æœ‰æŒ‘æˆ˜æ€§çš„é—®é¢˜å¹¶è§£å†³è¿™äº›é—®é¢˜ï¼Œæ¨¡å‹å¯ä»¥åŒæ—¶ç«™åœ¨å­¦ç”Ÿå’Œè€å¸ˆçš„ç«‹åœºä¸Šï¼Œå¯¹å›¾åƒå†…å®¹æœ‰ä¸€ä¸ªæ›´å…¨é¢çš„ç†è§£ï¼Œä»è€Œé€šè¿‡äº§ç”Ÿä¸€ä¸ªé«˜éš¾åº¦çš„ä»£ç†ä»»åŠ¡æ¥å¼•å¯¼è‡ªå·±æå–æ›´åŠ å¯æ‰©å±•çš„è¡¨å¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†Hard Patches Miningï¼ˆHPMï¼‰ï¼Œä¸€ä¸ªå…¨æ–°çš„MIMé¢„è®­ç»ƒæ¡†æ¶ï¼Œå¦‚ä¸Šå›¾ (b) æ‰€ç¤ºã€‚

å…·ä½“æ¥è¯´ï¼Œç»™å®šä¸€ä¸ªè¾“å…¥å›¾åƒï¼Œæˆ‘ä»¬ä¸æ˜¯åœ¨äººå·¥è®¾è®¡çš„æ ‡å‡†ä¸‹ç”Ÿæˆä¸€ä¸ª binary maskï¼Œè€Œæ˜¯é¦–å…ˆè®©æ¨¡å‹ä½œä¸ºä¸€ä¸ªè€å¸ˆï¼Œè‡ªä¸»äº§ç”Ÿæ©ç ï¼›ç„¶ååƒä¼ ç»Ÿæ–¹æ³•ä¸€æ ·ï¼Œè®©æ¨¡å‹ä½œä¸ºä¸€ä¸ªå­¦ç”Ÿï¼Œè®©å®ƒé‡å»º masked patchesã€‚

æ¥ä¸‹æ¥ï¼Œè¯¥é—®é¢˜å°±è½¬åŒ–ä¸ºäº†å¦‚ä½•è¯„åˆ¤æŸä¸€ä¸ª patch æ˜¯å¦ä¸ºå›°éš¾æ ·æœ¬ã€‚æˆ‘ä»¬è‡ªç„¶åœ°æƒ³åˆ°ï¼šã€Œå¦‚æœæŸä¸€ patch éš¾ä»¥é‡å»ºï¼Œå³é‡å»º loss è¾ƒå¤§ï¼Œåˆ™å®ƒä¸ºå›°éš¾æ ·æœ¬ã€ã€‚

{% asset_img 3.webp %}
<p style="text-align:center">Figure 2. For each tuple, we show the (a) input image, (b) patch-wise reconstruction loss averaged over 10 different masks, (c) predicted loss, and (d) masked images generated by the predicted loss.</p>

å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œæˆ‘ä»¬å‘ç°ä¸€å¼ å›¾ç‰‡ä¸­ discriminative çš„åŒºåŸŸï¼ˆå‰æ™¯ï¼‰å¾€å¾€æ˜¯éš¾ä»¥é‡å»ºçš„ã€‚å› æ­¤ï¼Œåªè¦è®©æ¨¡å‹ã€Œé¢„æµ‹æ¯ä¸ª patch çš„é‡å»ºæŸå¤±ã€ï¼Œè¿›è€Œ mask æ‰é‚£äº›é«˜é‡å»ºæŸå¤±çš„ patchï¼Œå°±å¾—åˆ°äº†æ›´åŠ å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»£ç†ä»»åŠ¡ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªè¾…åŠ©çš„ loss predictorï¼Œç”¨ä»¥é¢„æµ‹æ¯ä¸ª patch çš„é‡å»ºæŸå¤±ã€‚

***

### Method

{% asset_img 4.webp %}
<p style="text-align:center">Figure 3. Illustration of our proposed HPM.</p>

{% asset_img 5.webp %}

***

### å›¾åƒé‡å»º

{% asset_img 6.webp %}

***

### é‡å»ºæŸå¤±é¢„æµ‹

#### ç»å¯¹æŸå¤±

{% asset_img 7.webp %}

#### ç›¸å¯¹æŸå¤±

{% asset_img 8.webp %}

***

### æ©ç äº§ç”Ÿï¼šeasy-to-hard

{% asset_img 9.webp %}

æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹çš„ä¼ªä»£ç ä¸ºï¼š

{% asset_img 10.webp %}
{% asset_img 11.webp %}

***

### Experiments

#### æ¶ˆèå®éªŒ

è¿™ä¸€éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»¥ ViT-B/16 ä¸º backboneï¼Œä»¥ ImageNet-1K ä¸Š pre-train 200 epochs ä¸ºé»˜è®¤é…ç½®ã€‚

é‡å»ºç›®æ ‡çš„æ¶ˆèã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸ç®¡ä»¥ä»€ä¹ˆä¸ºé‡å»ºç›®æ ‡ï¼ŒåŠ å…¥ L_pred ä½œä¸ºé¢å¤–çš„æŸå¤±ï¼Œå¹¶åŸºäºæ­¤è¿›ä¸€æ­¥äº§ç”Ÿæ›´éš¾çš„ä»£ç†ä»»åŠ¡å‡èƒ½è·å¾—æ€§èƒ½æå‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä»…ä»…å¼•å…¥ä¸€ä¸ªé¢å¤–çš„ objective ä¹Ÿèƒ½å¤Ÿå¸¦æ¥ consistent çš„æ€§èƒ½æå‡ï¼Œè¡¨æ˜æŒ–æ˜å›°éš¾æ ·æœ¬çš„èƒ½åŠ›æœ¬èº«ï¼Œå°±èƒ½å¤Ÿä¿ƒä½¿ä¹ å¾—æ›´å¥½çš„ç‰¹å¾è¡¨ç¤ºã€‚è¿™ä¸€ç‚¹ä¸ä»…åœ¨åˆ†ç±»ä»»åŠ¡ä¸Šå¾—åˆ°ä½“ç°ï¼Œä¸‹æ¸¸ä»»åŠ¡ï¼ˆæ£€æµ‹åˆ†å‰²ï¼‰ä¹Ÿæœ‰ç›¸åº”çš„ä½“ç°ã€‚

{% asset_img 12.webp %}
{% asset_img 13.webp %}

**æ©ç ç­–ç•¥çš„æ¶ˆèã€‚**æˆ‘ä»¬å‘ç°ï¼Œéš¾åº¦å¤§çš„ä»£ç†ä»»åŠ¡ç¡®å®èƒ½å¤Ÿå¸¦æ¥æ€§èƒ½æå‡ï¼Œä½†ä¿ç•™ä¸€å®šçš„éšæœºæ€§ä¹Ÿæ˜¯åŒæ ·å¿…è¦çš„ã€‚è¿™äº›ç»“è®ºæ˜¯éå¸¸ç›´è§‚çš„ã€‚ç›´æ¥æ©ç›–é‚£äº›é¢„æµ‹æŸå¤±æœ€é«˜çš„ patch è™½ç„¶å¸¦æ¥äº†æœ€éš¾çš„é—®é¢˜ï¼Œä½†å›¾åƒ discriminative parts å‡ ä¹è¢«æ©ç›–äº†ï¼Œè¿™æ„å‘³ç€ visible patches å‡ ä¹éƒ½æ˜¯èƒŒæ™¯ï¼ˆè§å›¾2ï¼‰ã€‚åœ¨æ²¡æœ‰ä»»ä½•æç¤ºçš„æƒ…å†µä¸‹ï¼Œå¼ºè¿«æ¨¡å‹åªæ ¹æ®è¿™äº›èƒŒæ™¯æ¥é‡å»ºå‰æ™¯æ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚

{% asset_img 14.webp %}

è¿›ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬æ¢ç©¶ã€Œéš¾ã€çš„ä»£ç†ä»»åŠ¡å¯¹äº MIM æ˜¯å¦æœ‰å¸®åŠ©ã€‚å…¶ä¸­ï¼Œargmin è¡¨ç¤ºè¿™ä¸ªä»»åŠ¡ç”šè‡³ç®€å•äº random maskingï¼Œä¼šå¯¼è‡´æ€§èƒ½é€€åŒ–ã€‚ç±»ä¼¼çš„ï¼Œhard-to-easy ä¹Ÿä¼šå¯¼è‡´æ€§èƒ½é€€åŒ–ã€‚

{% asset_img 15.webp %}

**é¢„æµ‹æŸå¤±çš„ formulation**ã€‚MSE ç›¸è¾ƒäº baseline èƒ½å¤Ÿæœ‰æå‡ï¼Œä½† BCE æ˜¯ä¸€ä¸ªæ›´å¥½çš„é€‰æ‹©ã€‚

{% asset_img 16.webp %}

**Comparison with state-of-the-art methods**

{% asset_img 17.webp %}

ä¸‹é¢ï¼Œç»™å‡ºäº†ä¸€äº›é¢„æµ‹æŸå¤±çš„å¯è§†åŒ–ã€‚å¯ä»¥çœ‹å‡ºï¼Œé¢„æµ‹æŸå¤±å¾ˆå¥½çš„ååº”äº†ç‰©ä½“çš„å‰æ™¯éƒ¨åˆ†ã€‚

{% asset_img 18.webp %}
{% asset_img 19.webp %}

***

### å‚è€ƒèµ„æ–™

> <https://mp.weixin.qq.com/s?__biz=MzI1MjQ2OTQ3Ng==&mid=2247601717&idx=2&sn=6907577602ce8b491e59186f6f4704c0>
> [1] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. In International Conference on Learning Representations (ICLR), 2022
> [2] Kaiming He,Xinlei Chen,Saining Xie,Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022
> [3] Kaiming He,Xinlei Chen,Saining Xie,Yanghao Li, Piotr DollÃ¡r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022
