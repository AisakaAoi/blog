---
title: CSIG云上微表情-第69期-基于大模型的表情 / 微表情识别
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 7fef0e83
date: 2025-10-09 06:53:05
tags:
---

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=115344400326225&bvid=BV1bExCznEGv&cid=32947046677&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

<!--more-->

微表情是一种短暂的、微弱的、无意识的面部微表情，持续时间往往在0.5s内，能够揭示人类试图隐藏的真实情绪。微表情识别的研究旨在让机器有足够的智能，能够从人脸视频序列中识别人类的真实情绪。然而由于微表情持续时间短、面部肌肉运动强度低，对其进行准确的表征与识别是一项极具挑战性的任务。为了促进心理学领域和计算机视觉领域针对微表情的进一步研究，由中国图象图形学学会（CSIG）和中国科学院心理研究所举办、CSIG机器视觉专业委员会、CSIG情感计算与理解专业委员会和中科院青促会心理所小组联合承办，中国科学院心理研究所的王甦菁博士和李婧婷博士组织一系列云上微表情的学术活动。
第六十九期云上微表情于2025年10月09日晚7点进行，由中国科学院心理研究所王甦菁老师团队的李婧婷博士主持。此次讲座主题为“基于大模型的表情 / 微表情识别”，分别邀请来自东北大学的副教授兰星、中国科学技术大学的博士研究生刘世凤介绍相关研究工作，欢迎大家关注！


兰星，博士，东北大学计算机科学与工程学院副教授，博士生导师。主要研究方向包括多模态大模型、情感计算、具身智能等相关领域。2025年6月于中国科学院大学获工学博士学位，曾公派赴新加坡国立大学访问交流，获中国科学院院长奖、中国科学院朱李月华奖。在国际主流期刊与会议上发表论文20余篇，其中CCF-A/IEEE Trans.论文10余篇，授权国家发明专利2项。参与多项国家自然科学基金重点项目及国际合作重点项目。目前担任IJCV、IEEE TIP、IEEE TII、ACM ToMM等SCI期刊审稿人。
报告题目：基于多模态大模型的人脸表情思维链推理方法
报告摘要：表情是情感的一种外在表现，在人类沟通中扮演着关键角色。分析面部表情的内在机理对于准确识别面部表情至关重要。当前方法，通常预测面部动作单元 (AU) 名称和强度，但缺乏对 AU 与整体表情之间关系的深入分析。为了解决上述问题，我们提出了表情大模型 ExpLLM，生成用于面部表情识别的准确思维链（CoT）。我们从关键观察、整体情感分析和最终结论三个关键角度设计了CoT机制。关键观察描述了 AU 的名称、强度和相关情绪。整体情感分析提供基于多个 AU 及其交互的分析，识别主导情绪及其关系。最后，结论给出了根据前面的分析得出的最终表情标签。此外，我们还引入了 Exp-CoT 引擎，旨在构建此表达式 CoT 并生成用于训练 ExpLLM 的指令描述数据。对 RAF-DB 和 AffectNet 数据集的大量实验证明了 ExpLLM 的有效性。


刘世凤，中国科学技术大学人工智能与数据科学学院在读博士生，师从陈恩红教授。主要研究方向为：微反应情感分析、情感人机交互。
报告题目：MER-CLIP: 基于AU引导视觉-语言对齐的微表情识别方法
报告摘要：微表情（Micro-Expressions, MEs）作为一种关键的心理应激反应，是能够反映真实情绪的短暂、细微的面部运动。自动微表情识别在刑事侦查、心理诊断等领域具有重要应用价值。面部动作编码系统（Facial Action Coding System, FACS）通过识别特定面部动作单元（Action Units, AUs）的激活状态对表情进行编码，是微表情分析的核心参考依据。然而，当前微表情识别方法对AU的利用多局限于定义感兴趣区域（ROIs）或依赖特定先验知识，往往存在性能受限、泛化能力不足的问题。为解决这一挑战，本研究将CLIP模型强大的跨模态语义对齐能力融入微表情识别任务，提出一种基于AU引导视觉-语言对齐的微表情识别方法——MER-CLIP。具体而言，我们将AU标签转化为详细的面部肌肉运动文本描述，通过对齐视觉动态特征与基于AU的文本表征，实现细粒度的微表情时空特征学习。此外，本研究还设计了情绪推理模块（Emotion Inference Module），借助更高层次的语义理解捕捉微表情运动模式与情绪之间的细微关联。针对微表情数据稀缺易导致的过拟合问题，我们提出一种有效的数据增强策略LocalStaticFaceMix，该策略通过融合面部图像，在保留关键微表情运动的同时提升面部特征多样性。最后，在四个微表情基准数据集上的全面实验验证了MER-CLIP方法的优越性。
