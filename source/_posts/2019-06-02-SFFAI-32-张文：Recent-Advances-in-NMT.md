---
title: SFFAI 32 | 张文：Recent Advances in NMT
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 5c04c85d
date: 2019-06-02 07:11:23
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=56812331&bvid=BV1fx411o7Hw&cid=99207991&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

本次分享中，我们介绍一下近期的工作，分别以缓解上述三个问题为出发点，提出的三种方法：
1. 我们在基于RNN的编码器上层引入关系网络层，增强模型对源语言的表示学习能力；
2. 我们将立方体剪枝算法应用于NMT解码器中，在达到相同甚至更好翻译性能的情况下，比传统柱搜索算法提升了搜索效率。

<!--more-->

***

### 讲者介绍

**张文：**中国科学院计算技术研究所智能信息处理重点实验室在读博士生。研究方向为自然语言处理和机器翻译。博士期间在CWMT，COLING，EMNLP和ACL等会议上发表多篇论文。曾获中国科学院大学三好学生称号和易方达博士生奖学金。

**报告题目：**Recent advances in Neural Machine Translation

**报告摘要：**神经网络机器翻译（NMT）模型已经取得卓越的效果。但仍然存在一些问题和挑战：
1. 基于RNN的NMT模型使用RNN对源端序列编码时仅考虑了词语之间的正反顺序信息，忽略了词语与词语之间的关系，而且RNN通常倾向于忘记有用的历史信息，难以捕获语言学中的长距离依赖信息；
2. NMT模型的翻译效率仍然比较低，解码器在测试阶段的柱搜索过程中，通常需要在搜索空间和搜索效率（翻译质量和翻译效率）之间进行权衡，因此模型的优势无法得到充分发挥；
3. NMT模型大多是自回归的，即以源语言句子和目标端的历史序列作为条件逐词生成整个目标译文。在训练阶段，模型以目标端参考译文的历史序列作为上下文进行预测；而在测试阶段，模型只能使用自身预测出来的历史序列作为上下文。这种训练和测试输入目标端上下文的差异会不断地导致错误累积。此外，词级的训练过程要求所生成的序列与参考译文序列之间严格匹配，这导致对不同于参考译文但合理的翻译的过度校正。本次报告将以缓解上述三个问题为出发点，介绍一下近期提出的三种方法。

**Spotlight：**
1. 在基于RNN的编码器上层引入关系网络层，增强模型对源端句子的表示能力；
2. 使用立方体剪枝算法优化NMT解码器，在达到相同甚至更好翻译性能的情况下，比传统柱搜索算法提升了搜索效率；
3. 在NMT训练阶段，以一定的概率从前一步的参考译文词语和模型自身预测出的1-best词语之间采样，1-best词语的确定包括词语级别和句子级别的最优，从而使模型在训练时尽量考虑测试时的环境。

***

### 论文推荐

1. A simple neural network module for relational reasoning

    **推荐理由：**关系推理是一般智能行为的核心组成部分，但是学者们已经证明神经网络很难学习到关系信息。在本文中，作者提出关系网络（RN），并描述了如何使用RN作为一个简单的即插即用的模块从根本上解决依赖于关系推理的问题。他们在三个任务上测试了RN增强的网络：在视觉问答的数据集CLEVR上，他们提出的RN实现了state-of-the-art的性能；在基于文本的问题任务上，RN可以实现关于动态物理系统的复杂推理；然后，在名为Sort-of-CLEVR的数据集上，作者验证了强大的卷积网络不具备解决关系问题的能力，但RN增强的模型具有该能力。总之，他们的工作解释了配备RN模块的深度学习架构如何隐式发现并学习实体及其关系的推理。

2. Refining Source Representations with Relation Networks for Neural Machine Translation

    **推荐理由：**尽管使用编码器-解码器框架的NMT模型近来取得了巨大成功，但它仍然存在遗忘长距离依赖信息的问题，这是循环神经网络结构的固有缺点，并且在编码过程中忽略了源端序列中词语之间的关系。然而在实际应用当中，之前的历史信息和关系信息通常是有用的。为了缓解这些问题，我们在编码器中引入关系网络增强编码器的表示能力。关系网络通过将源端词语两两关联，进而增强RNN的记忆能力。在解码时带有源端关系信息的表示输入到注意力模块，我们的模型保持编码器-解码器框架不变。在几个数据集的实验表明，与传统的编码器-解码器模型相比，我们的方法可以显著提高翻译性能，甚至优于引入监督语法知识的方法。

3. Forest Rescoring:Faster_Decoding with Integrated Language Models

    **推荐理由：**如何高效解码一直是机器翻译面临的一个基本问题，特别是集成了语言模型之后，而语言模型通常对于实现良好的翻译质量是必不可少的。作者基于k-best句法分析的算法设计了更快解码方法，并且验证了所提出方法在基于短语和基于句法的翻译系统上的有效性。针对两个翻译系统，在获得相同搜索误差和翻译精度的情况下，他们的方法比传统柱搜索算法有十倍以上速度提升。

4. Speeding Up Neural Machine Translation Decoding by Cube Pruning

    **推荐理由：**虽然神经网络机器翻译（NMT）模型已经取得了可喜的成果，但它的解码效率还比较低。这所导致的后果就是我们必须在翻译质量和翻译速度之间进行权衡，因此翻译性能无法得到充分发挥。我们使用立方体剪枝（Cube Pruning）算法优化NMT解码效率。我们探索出解码器中比较耗时的计算单元有RNN和目标大词表上的归一化操作。在传统柱搜索算法中的每一步，我们通过组合类似的目标隐藏状态构造等价类，减少目标端的RNN扩展操作以及大词汇表上的归一化操作。实验表明，在达到相同甚至更好翻译性能的情况下，与传统柱搜索算法相比，我们的方法可以分别在GPU和CPU上提高解码效率3.3倍和3.5倍。

5. Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks

    **推荐理由：**给定一些输入，RNN模型可以输出一个词语序列，例如最近的机器翻译模型。当前模型训练的方法是在给定当前循环状态和之前词语序列的情况下最大化序列中每个词语的概率。而在测试阶段，之前参考词语序列是未知的，只能由模型自身预测出的历史序列取代。训练和测试之间的这种差异可能沿着所生成的序列快速地产生错误累积。针对这种现象，作者提出了一种课程学习的策略，在训练过程中，从完全由真实的先前词语序列指导模型训练转化为较少地由真实的先前词语序列指导，大多数情况下由模型本身生成的词语序列代替。在几个序列预测任务的实验表明，所提出的方法产生了显著的改进。

***

### 参考资料

> <https://www.bilibili.com/video/BV1fx411o7Hw/>
> <https://bbs.sffai.com/d/80>
