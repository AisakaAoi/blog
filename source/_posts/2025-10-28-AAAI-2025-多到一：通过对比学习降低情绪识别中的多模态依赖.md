---
title: AAAI 2025 | 多到一：通过对比学习降低情绪识别中的多模态依赖
categories:
  - 🌙进阶学习
  - ⭐脑机接口与混合智能研究团队（BCI团队）
  - 💫学习报告
abbrlink: 1a2aba53
date: 2025-10-28 09:14:04
tags:
---

{% asset_img 1.webp %}

该论文发表于The Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25，CCF-A)，题目为《Multi-to-Single: Reducing Multimodal Dependency in Emotion Recognition Through Contrastive Learning》。

上海交通大学的刘彦楷为此文的第一作者，上海交通大学的郑伟龙副教授为此文的通讯作者。

论文链接：https://doi.org/10.1609/aaai.v39i2.32134

<!--more-->

***

### 论文概要

该研究提出了一种名为“Multi-to-Single”（M2S）的情绪识别模型，旨在解决多模态情绪识别在实际应用中难以同时获取所有模态数据的挑战。研究的核心思想是通过对比学习，在预训练阶段最小化模态内情绪相关特征与情绪无关特征的互信息，最大化模态间情绪相关特征的互信息，并使用单模态数据进行微调，从而在测试阶段仅使用单一模态数据即可达到甚至超越传统多模态方法的性能，以此降低对多模态数据的依赖性。为实现此目标，该模型引入了两个创新模块：一个空间和时间稀疏（STS）注意力机制，用于增强编码器对EEG等信号的特征提取能力 ；以及一个新颖的多对多对比预测编码（M2M CPC）模块，用于学习和融合跨模态特征。在五个公开数据集上的大量实验证明，该模型在跨模态任务中取得了当前最优（state-of-the-art）的性能。为了应对这一挑战，研究人员开始探索跨模态学习方法，其目标是在训练和测试阶段使用不同的模态组合。尽管在计算机视觉等领域已有较多研究，但在情感脑机接口领域，相关工作仍处于早期阶段，且大多仅关注从眼动信号生成其他模态特征，未能实现任意模态间的转换和充分利用多模态特征。因此，本研究旨在开发一个能够用单模态数据实现多模态效果的模型，以解决上述局限性。

***

### 研究背景

多模态情绪识别是情感脑机接口领域的一个关键研究方向。通过融合来自EEG、EYE、ECG等多种生理信号的特征，多模态模型的性能显著优于单模态模型。然而，在实际应用场景中，同时获取高质量的多模态数据存在显著难。EEG信号对环境干扰和被试者的微小动作极为敏感，容易导致信号质量下降甚至数据不可用。对于EYE信号，虽然采集过程相对便捷，但对距离敏感，要求眼动仪与被试者保持特定距离，且被试者需持续注视屏幕，这在长时间采集中难以维持。ECG和PPS信号，也面临着电极采集带来的相似问题。

***

### 方法

{% asset_img 2.webp %}
<div align='center'>图1 多到一框架的总体概览。(a) 预训练阶段。(b) 模型微调阶段。(c) 时空稀疏注意力机制。</div>

***

#### 多到一情绪识别模型

模型的整体架构如图1（a）所示，包含预训练和微调两个阶段。在预训练阶段，模型使用成对但无标签的多模态数据。针对每种模态，模型设计了两个编码器：一个基于Transformer的情感相关（Emotion-Related, ER）编码器，用于提取情绪特征；一个基于MLP的情感无关（Emotion-Independent, EI）编码器，用于提取与情绪无关的特征。为了使两组特征尽可能独立，研究采用对比对数比上界（CLUB）方法来最小化ER和EI编码器输出特征之间的互信息 。CLUB损失函数定义为：

{% asset_img 3.webp %}

其中，zx和z’x分别是ER和EI的输出。q(z’x|zx)是参数θ近似p(z’x|zx)的变分分布。

为了验证编码器是否提取了数据的有效特征，我们为每种模态添加了一个解码器，并计算了每种模态的重构损失。模态X的重构损失定义为：

{% asset_img 4.webp %}

其中，DX是模态X的解码器。

为了学习和融合不同模态的特征，我们应用了两种对比学习的方法：InfoNCE损失和创新的M2M CPC模块。首先介绍InfoNCE损失的计算，M2M CPC会在下面进行介绍。对于任意嵌入zx，通过在时间维度进行平均池化和一层线性层，将嵌入投影到一个新的嵌入空间中，得到模态X的最终嵌入，其中S表示样本数量，Df表示最终嵌入维度。因此可以得到S个正样本对，S^2-S个负样本对。根据InfoNCE思想，可以将其视为一个S类别的分类任务。以EEG和EYE为例，可以定义真实标签GT为 [0, 1, …, S-1]，然后计算对比损失Lcontra：

{% asset_img 5.webp %}

***

#### 空间和时间稀疏注意力机制（STS）

像EEG、ECG和PPC等信号，它们都是通过位于人体固定位置的电极进行长时间采集的。因此，这些信号在空间和时间两个维度上都有很强的内部特征。为了充分利用信号的时空特征，我们设计了STS注意力机制。其核心思想是在计算注意力时，不仅考虑当前时间点的信息，还融合了前一时间点和初始时间点的数据，以捕捉时间序列的基本信息和动态变化。Query、Key、Value的计算方式如下：

{% asset_img 6.webp %}

其中，⊕表示相加和平均操作。注意力计算如下：

{% asset_img 7.webp %}

***

#### 多到多对比预测编码（M2M CPC）

{% asset_img 8.webp %}
<div align='center'>图2 M2M CPC模块结构，黄色和绿色分别表示不同模态。</div>

M2M CPC是本研究提出的一个新颖模块，其改进了传统的CPC。传统CPC通过自回归模型预测序列的未来信息。考虑到人的情绪变化通常会引发多种生理信号的同步改变，M2M CPC模块利用多种模态的当前嵌入向量来共同预测每一种模态的未来向量。以EEG和EYE为例，将它们输入情绪相关编码器后，可以得到嵌入向量：

{% asset_img 9.webp %}

其中，T表示时间窗口长度，S表示数据样本数量，D表示嵌入维度。作者使用一个双层LSTM作为每个模态的自回归模型。定义M为观测序列长度，N为预测步长，可以得到：

{% asset_img 10.webp %}

其中，D’是LSTM的隐藏层维度，且M+N≤T。将Zeeg和Zeye进行拼接，以预测每个模态未来的N个时间步。使用InfoNCE损失函数来优化该模块，包括四部分损失：同模态下预测向量与真实向量的损失，异模态下预测向量与真实向量的损失。定义ZA和ZB为包含负样本和一个正样本的集合，A，B∈{eeg, eye}。那么：

{% asset_img 11.webp %}

其中，[·]表示拼接操作。因此完整的M2M CPC损失定义为：

{% asset_img 12.webp %}

***

### 预训练和微调

预训练的损失为上述所有损失的加和：

{% asset_img 13.webp %}

在微调阶段，模型仅需输入单一模态数据。此时，对应模态的预训练ER编码器被冻结，仅需优化一个新添加的分类器。对于跨模态任务，模型使用一种模态进行微调，再用另一种模态进行测试。

***

### 实验结果

该研究在五个公开多模态情绪数据集上（SEED、SEED-IV、SEED-V、DEAP、DREAMER）进行了广泛实验。研究选取了balanced accuracies和kappa scores作为SEED系列数据集的评估指标，选取了balanced accuracies和F1 scores作为DEAP和DREAMER两个数据集的评估指标。

***

#### 与跨模态方法比较

实验结果如表1和表2所示，M2S模型在所有跨模态和单模态任务设置中均显著优于所有基线方法。在跨模态任务中，M2S的性能比次优方法普遍高出5个百分点以上，最大提升超过10个百分点（p < 0.05）。

{% asset_img 14.webp %}

表1 在 SEED、SEED-IV 和 SEED-V 数据集上，跨模态和单模态下的被试内（subject-dependent）准确率（%）和 Kappa 分数（%）。其中，“EEG → EYE” 表示使用 EEG 数据进行微调，并使用 EYE数据进行测试。

{% asset_img 15.webp %}

表2 在DEAP、FREAMER数据集上，跨模态和单模态下的被试内准确率（%）和F1 分数（%）。

***

#### 与多模态方法比较

实验结果如表3所示，仅使用单模态（EEG）进行微调和测试，M2S模型的性能也优于大多数监督学习下的多模态方法。同时，该研究还进行了对ER编码器的监督训练实验，该实验仅使用EEG单模态数据。其准确率略低于使用预训练模型的结果，但仍能达到多模态方法的性能。这些实验结果表明，预训练过程发挥了作用，并且对比学习模块的引入也实现了不同模态特征的有效融合。

{% asset_img 16.webp %}

表3 多模态方法与 M2S 方法在 SEED、SEED-IV 和 SEED-V 数据集上的平衡准确率（%）和 Kappa 分数（%）比较。其中，(S) 表示该方法使用了相同的编码器进行单模态监督学习。

***

### 消融实验

#### 损失函数

实验结果如表1所示，移除预训练中的任何一个损失函数（CLUB、Recon、Contra、CPC）都会导致模型性能不同程度的下降 。其中，移除对比学习相关的损失对跨模态任务的影响最为严重

***

#### STS注意力机制

实验结果如图3所示，与不使用注意力或使用标准多头注意力机制相比，采用STS注意力机制显著提升了模型在EEG单模态任务上的性能。

{% asset_img 17.webp %}
<div align='center'>图3 基于 EEG单模态微调的多种注意力机制在 SEED、SEED-IV 和 SEED-V 数据集上的性能结果。</div>

***

#### M2M CPC模块

如图4所示，将M2M CPC应用于其他现有模型，也能提升它们的性能，证明了该模块的通用性和有效性。如图 5 所示，在相同的超参数调整范围内，当时间窗口长度固定时，模型的准确率随着预测步数的增加，通常表现出先上升后下降的趋势。而当预测步长固定时，更长的时间窗口有助于模型获得更好的性能。

{% asset_img 18.webp %}
<div align='center'>图4 将 M2M CPC 模块添加到 CLIP 和 ECO-FET 模型后，它们在 SEED、SEED-IV 和 SEED-V 数据集上的跨模态任务 (EEG → EYE) 准确率。</div>

{% asset_img 19.webp %}
<div align='center'>图5 SEED 数据集上跨模态任务 (EEG → EYE) 的热力图。</div>

***

### 结论

本研究成功提出并验证了一种新颖的跨模态学习方法M2S，该方法能够有效降低情绪识别任务对多模态数据的依赖，仅用单一模态便可达到甚至超越多模态的性能。研究引入的STS注意力机制和M2M CPC模块被证明是提升模型性能的关键。特别是M2M CPC模块，它具有良好的通用性，可以被整合到其他模型中以学习和融合不同模态的特征，从而帮助现有方法取得更好的表现。该研究为解决情感脑机接口在真实场景下的数据采集难题提供了有效的解决方案。

***

### 原文链接

> <https://www.scholat.com/teamwork/showPostMessage.html?id=17459>
