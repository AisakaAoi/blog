---
title: SFFAI 128 | 机器翻译专题《张绍磊：多延时下的通用机器同声传译方法》
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 8633fc1b
date: 2021-11-23 02:08:40
tags:
---

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=681564930&bvid=BV1gS4y1V717&cid=510816716&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

<!--more-->

SFFAI论坛网站已开放注册，详情点击查看：https://bbs.sffai.com/d/312

关注公众号：【人工智能前沿讲习】，回复【SFFAI128】获取讲者PPT资料，入交流群，推荐论文下载。

传统机器翻译读入完整的源端句子之后一次性输出翻译，而机器同声传译 (Simultaneous Machine Translation) 在阅读整个源句子的同时进行翻译，因此其性能由翻译质量和延迟两方面来评估。机器同传在不同的场景下往往有不同的延时需求，例如实时直播往往需要更低延时以提供更加流畅的翻译，而正式会议则侧重更高的翻译质量而允许稍高的延时。本期我们邀请到了来自计算技术研究所的张绍磊同学，介绍其构建的通用机器同传模型，使用一个模型在多个延时下完成高质量的翻译。

***

### 讲者介绍

**张绍磊：**中国科学院计算技术研究所直博生，主要研究方向为机器翻译、同声传译。以第一作者在 EMNLP、AAAI 等国际会议上发表论文3篇；参加全球第二届同声传译测评比赛并在流式输入赛道获得冠军；担任中国中文信息学会青年工作委员会学生执委，组织开展各项学术活动。

**报告题目：**多延时下的通用机器同声传译方法

**报告摘要：**现有的机器同声传译方法通常需要为每个延时训练并维护多个同传模型，导致计算成本很高和难以部署。在本文中，我们提出多专家混合Wait-k 策略以构建通用机器同传模型，仅使用一个模型在多个延时下完成高质量的翻译。在三个数据集上的实验表明，我们的方法可以仅使用一个通用模型在不同延迟下优于所有强基线，并且在高效性和鲁棒性上取得可喜的结果。

**论文标题：**Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy

**分享亮点：**
1. 本文开发了一种能够在多延时下完成高质量翻译的通用机器同传方法；
2. 本文探索了通过混合专家模型（Mixture-of-Expert）划分神经元以提升模型表征能力的具体方法；
3. 本文提出的通用模型可以被当做一个机器同传内核，具有很强的拓展性。

***

### 论文推荐

1. STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework ∗

    **推荐理由：**本文定义了机器同传的基本框架并且提出了wait-k策略。wait-k策略目前是应用最为广泛的机器同传算法。我们的工作主要基于wait-k策略，所以强烈建议阅读。
 
2. Efficient Wait-k Models for Simultaneous Machine Translation

    **推荐理由：**Wait-k策略具有训练复杂度高的问题，本文提出了一种更加高效的wait-k策略，在训练效率上有所提高，并且保持了相当的翻译质量。
 
3. Future-Guided Incremental Transformer for Simultaneous Translation

    **推荐理由：**本文是我们发表与AAAI2021的工作，该文章分析了wait-k策略的不足并且引入未来信息指导训练。文章细致分析了wait-k策略的一些表现，阅读该文有助于更加深入了解wait-k策略。
 
4. MONOTONIC MULTIHEAD ATTENTION

    **推荐理由：**本文中提出的方法MMA取得了目前机器同传的最佳性能，也是本次分享的强baseline之一，所以建议了解其基本方法。

5. OUTRAGEOUSLY LARGE NEURAL NETWORKSTHE SPARSELY-GATED MIXTURE-OF-EXPERTS LAYER

    **推荐理由：**本文中，google首次提出专家混合模型（Mixture-of-experts），以充分利用网络中的神经元，增强模型表征能力。阅读本文有助于理解专家混合模型架构和基本思想。

6. A Mixture of h − 1 Heads is Better than h Heads

    **推荐理由：**本文首次将专家混合模型应用于Transformer结构中的multi-head attention，将multi-head attention表示为多个专家输出的混合的形式。该方法通过混合h-1个head输出，在翻译质量上取得了略微提升。

***

### 参考资料

> <https://www.bilibili.com/video/BV1gS4y1V717/>
