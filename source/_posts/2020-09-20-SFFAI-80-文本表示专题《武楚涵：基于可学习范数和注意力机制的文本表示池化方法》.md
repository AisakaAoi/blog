---
title: SFFAI 80 | 文本表示专题《武楚涵：基于可学习范数和注意力机制的文本表示池化方法》
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 28417e7d
date: 2020-09-20 05:27:22
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=293036982&bvid=BV1Wf4y1n71k&cid=409037731&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<!--more-->

关注公众号：【人工智能前沿讲习】，回复【SFFAI80】获取讲者PPT资料，入交流群，推荐论文下载。

池化是许多基于深度学习的NLP模型用来学习文本表示的重要技术。在已有的池化方法中（例如平均池化，最大池化和基于注意力机制的池化），输出的文本表示是输入特征的L1或L∞范数的加权和。但是，这些方法的池化范数是固定的，对于在不同任务中学习文本表示可能并非最优。另外，如最大池化和基于注意力机制的池化等许多常用的方法可能会过分强调某些特征，导致其他有用的信息没有被充分利用。

***

### 讲者介绍

**武楚涵：**清华大学电子工程系博士生，主要研究方向为自然语言处理，用户建模和推荐系统，目前已在ACL、IJCAI、KDD、EMNLP、WSDM、NAACL和CIKM等学术会议上发表论文若干。

**报告题目：**基于可学习范数和注意力机制的文本表示池化方法

**报告摘要：**我们提出了一种基于可学习范数和注意力机制的池化方法，用于文本表示学习。不同于已有的使用固定范数的池化方法，我们提出以端到端的方式学习池化的范数，以自动在不同的任务中找到用于文本表示的最佳范数。此外，我们提出了两种方法来确保模型训练的数值稳定性。第一是幅度限制，它重新缩放输入以确保其非负性，并减轻指数爆炸的风险。第二是公式重整，用于分解幂指数运算，以避免计算输入特征的实数幂，并加速池化运算。在四个基准数据集上的实验结果表明，我们的方法可以有效地提高基于注意力机制的池化方法的性能。

**Spotlight：**
1. 本文提出了一种基于可学习范数和注意力机制的文本表示池化方法；
2. 本文提出了两种方法来保证模型训练的数值稳定性并提升池化速度；
3. 本文提出的池化方法具有通用性，可以用于许多NLP领域的任务。

***

### 论文推荐

#### 经典论文

1. Convolutional Neural Networks for Sentence Classification

    **推荐理由：**基于卷积神经网络和最大值池化的文本表示方法，是使用卷积神经网络作为文本特征抽取器的经典工作。

2. Hierarchical Attention Networks for Document Classification

    **推荐理由：**一种基于层次化注意力机制的文本表示模型（HAN），是注意力机制在NLP领域的经典应用。

3. Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks

    **推荐理由：**一篇尝试设置不同范数进行特征池化的工作。与本文不同，该工作中的池化范数需要手动选取，而我们的方法则可以自动进行训练来寻找合适的范数。

#### 前沿论文

4. Attention is not Explanation

    **推荐理由：**一篇对注意力机制解释性的探讨，研究了注意力权重和特征重要性的关联，指出了基于注意力权重进行模型解释性分析的问题。

5. Attention is not not Explanation

    **推荐理由：**对上一篇论文的一项反驳，质疑了上一篇论文中的部分假设，并进一步讨论了使用注意力权重解释RNN等模型的合理性。

6. Hierarchical User and Item Representation with Three-Tier Attention for Recommendation

    **推荐理由：**一篇对层次化注意力机制的进一步扩展的工作。该工作在HAN的基础上提出增加了一级文档级别的注意力机制，从而形成了三级注意力机制，用于学习更好的用户和商品表示。

***

### 参考资料

> <https://www.bilibili.com/video/BV1Wf4y1n71k/>
