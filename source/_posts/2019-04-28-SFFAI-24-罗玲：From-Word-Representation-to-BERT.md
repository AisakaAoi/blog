---
title: SFFAI 24 | 罗玲：From Word Representation to BERT
categories:
  - 🌙进阶学习
  - ⭐讲座
abbrlink: 72d40aef
date: 2019-04-28 00:15:57
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=50772422&bvid=BV1Z4411b7dc&cid=88876278&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

本次分享主要是通过简要介绍预训练词向量研究历程(word2vec，glove，ELMo等)，重点介绍BERT的主要贡献。作为刷新GLUE榜单11项NLP任务（句子关系判断，分类任务，序列标注任务等）成绩的预训练模型，BERT不仅沿袭将词向量和下游任务结合在一起实现上下文相关的优点，并且通过masked语言模型实现了真正的深度双向模型。这使得BERT不仅能更好的处理sentence-level的任务

<!--more-->

***

### 参考资料

> <https://www.bilibili.com/video/BV1Z4411b7dc/>
