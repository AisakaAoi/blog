---
title: SFFAI 4 | 详解记忆增强神经网络
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: 481f8e8f
date: 2018-10-14 07:45:41
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=37093433&bvid=BV1Ht411X7rd&cid=65180886&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

视频PPT及文章资料，请关注微信公众号：人工智能前沿讲习班
PPT及文章链接：https://mp.weixin.qq.com/s/fqHropcxoP0AfQu2m1-wYQ
记忆增强神经网络（ Memory Augmented NeuralNetwork， MANN）是在传统的神经网络模型基础上增加存储模块以及相应的读写机制的一类模型。

<!--more-->

<iframe src="//player.bilibili.com/player.html?aid=37093433&bvid=BV1Ht411X7rd&cid=65180897&p=2" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<iframe src="//player.bilibili.com/player.html?aid=37093433&bvid=BV1Ht411X7rd&cid=65181667&p=3" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

***

### 讲者介绍

**王克欣**，中科院自动化研究所宗成庆老师组二年级硕士生。研究领域为自然语言处理，研究方向为记忆增强神经网络、无监督（句法）结构学习等。

**报告题目：**详解记忆增强神经网络

**报告摘要：**内存是计算的前提，目前广泛被采用的不具有外置内存（external memory）的机器学习模型（如LSTM）的记忆能力是受可训练参数数目影响的，使得其面对需要较大存储的任务时想达到较好性能代价较高；而具有外置内存（一般称为memory augmented）的模型能够一定程度上解决这一问题。本次演示分享主要关注（外置）记忆增强神经网络系列模型，从两种分类方式（读写角度以及从类比自动机理论角度）介绍其发展过程以及这类模型的核心设计思路；另着重介绍神经图灵机模型（neural turing machine），分析工作原理，并进一步简单探讨表达能力与学习能力的关系等。

**田正坤**：中国科学院自动化智能交互课题组，研究方向是语音识别，迁移学习，另外对于自然语言处理也有很大的兴趣。

**报告题目**：Seq2Seq模型在语音识别中的应用

**报告摘要**：Most ASR systems involve separately trained acoustic, pronunciation and language model components which are trained separately. A single end-to-end trained sequence-to-sequence model, which directly outputs words or graphemes, could greatly simplify the speech recognition pipeline. Attention-based sequence-to-sequence model has made significant progress in the filed of Neural Machine Translation. Speech Transcription and Machine Translation have many similarities. And Seq2Seq Model for speech recognition has become a research hotspot.

***

### 参考资料

> <https://www.bilibili.com/video/BV1Ht411X7rd/>
> <https://bbs.sffai.com/d/9/2>
> <https://bbs.sffai.com/d/11-seq2seq>
