---
title: CSIG云上微表情-第61期-结合深度信息的多模态微表情识别
categories:
  - 🌙逢坂杂谈与搬运
  - ⭐一些讲座
abbrlink: d0d5c16c
date: 2025-02-28 06:26:34
tags:
---

<iframe src="//player.bilibili.com/player.html?isOutside=true&aid=114081780472939&bvid=BV1KP93YEEMs&cid=28623044988&p=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>

<!--more-->

微表情是一种短暂的、微弱的、无意识的面部微表情，持续时间往往在0.5s内，能够揭示人类试图隐藏的真实情绪。微表情识别的研究旨在让机器有足够的智能，能够从人脸视频序列中识别人类的真实情绪。然而由于微表情持续时间短、面部肌肉运动强度低，对其进行准确的表征与识别是一项极具挑战性的任务。为了促进心理学领域和计算机视觉领域针对微表情的进一步研究，由中国图象图形学学会（CSIG）和中国科学院心理研究所举办、CSIG机器视觉专业委员会、CSIG情感计算与理解专业委员会和中科院青促会心理所小组联合承办，中国科学院心理研究所的王甦菁博士和李婧婷博士组织一系列云上微表情的学术活动。
第六十一期云上微表情于2025年02月28日晚7点进行，线上由中国科学院心理研究所王甦菁老师团队的李婧婷博士主持。此次讲座主题为“结合深度信息的多模态微表情识别”，邀请到北京邮电大学的博士研究生张人与中国科学院心理研究所联合培养硕士研究生钱雨带来学术报告，欢迎大家关注！

张人，北京邮电大学智能工程学院博士三年级在读，导师尹建芹教授，主要研究方向：计算机视觉，情感识别，主要研究兴趣为微表情分析与识别，多模态微表情分析
报告题目：Facial 3D Regional Structural Motion Representation Using Lightweight Point Cloud Networks for Micro-Expression Recognition
报告摘要：人机交互（HCI）依赖于理解和适应用户的情绪状态。微表情（ME）是情绪感知的关键组成部分，其特征是自发性、快速性、微妙性和难以控制性。它们经常揭示一个人的真实情感。为了有效地捕捉面部动态的细微差别，有必要对运动进行全面详细的表示。目前，运动表示方法主要局限于RGB图像中的2D分析，忽视了面部结构及其运动在传达情绪中的关键作用。为了克服这一局限性，我们引入了一种创新的面部运动表示方法，该方法包括3D面部结构、区域化RGB和结构运动特征。此外，我们将面部划分为八个不同的区域，仅选择最重要的运动点来描绘每个区域的主要运动特征。为了模拟关键面部运动区域之间的相互作用，我们采用了一种先进的、轻量级的点云和图卷积网络（Lite point GCN）。使用leave-one subject-out（LOSO）对CAS（ME）3数据集进行的全面测试表明，我们的方法优于现有的最先进方法。
参考文献：
Zhang, R.,et al. (2025). Facial 3D Regional Structural Motion Representation Using Lightweight Point Cloud Networks for Micro-Expression Recognition. IEEE Transactions on Affective Computing.

钱雨，中国科学院心理研究所微表情应用研究中心实习生，江苏科技大学计算机学院研究生二年级在读，导师黄树成。主要研究方向：计算机视觉，图像处理，情感分析。主要研究兴趣为微表情识别。
报告题目：Micro-expression recognition using dual-view self-supervised contrastive learning with intensity perception
报告摘要：微表情作为真实情感的表征，在医疗护理和公共安全领域具有重要应用价值。这类表情具有持续时间短、强度低、局部发生的特点，这些特性导致了微表情识别中的小样本问题，使得特征学习变得困难，并限制了识别性能的提升。为了解决这一问题，我们提出了一种基于动作单元（AU）强度感知的多模态对比学习预训练模型。我们通过实验确定了面部表情识别的最小感知阈值，并利用该阈值对大量无监督样本进行了筛选。在第一阶段，模型通过无监督的多模态对比学习，从不同模态的面部动作差异中提取特征；随后，模型利用少量标注数据在微表情识别任务上进行训练，从而克服了小样本的限制。通过在MEGC2019-CD和多模态数据集CAS(ME)³上的对比实验，验证了所提方法的优越性。本方法的代码已开源，地址为：https://github.com/MELABIPCAS/DVSCL.git
参考文献：
Li, J., Zhou, H., Qian, Y.,et al. (2025). Micro-expression recognition using dual-view self-supervised contrastive learning with intensity perception. Neurocomputing, 619, 129142.
