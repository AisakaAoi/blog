---
title: CSIG云上微表情-第11期（2020.12）-结合计算机视觉的微表情检测与识别
categories:
  - 🌙进阶学习
  - ⭐人工智能 Artificial Intelligence
  - 💫计算机视觉 Computer Vision
  - 🛰️微表情 Micro Expression
abbrlink: e78e09c0
date: 2022-07-08 21:37:19
tags:
---

<iframe src="//player.bilibili.com/player.html?aid=415822781&bvid=BV18V411b7VS&cid=272458571&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>

<!--more-->

***

### Part 1 基于深度学习的微表情识别介绍

{% asset_img 1.webp %}

{% asset_img 2.webp %}

相对与macro-expression，micro-expression更加细微，肉眼来看很难判别出来，希望通过算法来识别微表情

{% asset_img 3.webp %}

数据集->预处理->特征提取->微表情识别->分类

微表情识别：宏表情迁移到微表情、基于顶点帧、基于动作单元AU

{% asset_img 4.webp %}

{% asset_img 5.webp %}

利用已有的AU关联

{% asset_img 6.webp %}

输入的两个维度：node feature、adjacency matrix

N代表节点的数量，d代表每个的特征维度

每个GCN layer跟CNN层有异曲同工之妙（这不就跟全连接到卷积，从全局连接到部分连接；卷积到图网络，局部连接到全局自定义连接。更加稀疏了么，而且自由度更高，消除了不必要的节点连接冗余）

{% asset_img 8.webp %}
{% asset_img 7.webp %}

在普通的CNN提取人脸特征下，加入了一个AU检测的GCN，然后concat进行分类

{% asset_img 9.webp %}
{% asset_img 10.webp %}

基于上一篇论文，但思路跟前面有一点点不一样。先汇入一个3D ConvNet作为Backbone，然后经过三层GCN和sagpool，得出有用的节点信息进行分类

{% asset_img 11.webp %}

提出AU-ICGAN去生成微表情的data。attention mask是为了将注意集中在人脸而不是背景，color mask集中注意在rgb这个通道

{% asset_img 12.webp %}

（loss有点多看不太懂，前四个是对单张图片的，后面三个是对于视频序列的）

{% asset_img 13.webp %}
{% asset_img 14.webp %}
{% asset_img 15.webp %}
{% asset_img 16.webp %}
{% asset_img 17.webp %}

人脸存在不对称性，多模态等...看看能不能提升准确度

***

### Part 2 Local Temporal Pattern and Data Augmentation for Micro-Expression Spotting

{% asset_img 18.webp %}

{% asset_img 19.webp %}

样本量很少，5个数据集加起来也就800个左右，对机器学习很有挑战性。

{% asset_img 20.webp %}

2013~2014年发布了CASME有小幅上升，微表情检测的F1-score < 0.1，对于实际应用十分有挑战性。

{% asset_img 21.webp %}

微表情的三个特点：持续时间短、微表情运动强度低、非常局部的面部运动。

主流的两种思路：比较峰值帧之间的特征（相当于传统的图像处理的方法 or 结合机器学习来做一个比较）、

{% asset_img 22.webp %}

特征的局部提取、融合、时域上检测帧与帧之间的差别，检测到一段时间内最大的峰值为微表情，缺点是检测到的不一样是微表情，可能只是当前区间内一个比较显著的特征（并非微表情特征），由于先前已融合特征所以微表情的局部特征有可能已经被忽略掉了，优点是考虑到了微表情时长的变化。

{% asset_img 23.webp %}

{% asset_img 24.webp %}

考虑时域特征、机器学习+数据扩增、提出了一个叫做晚期融合的方法（Late fusion from local to global）

{% asset_img 25.webp %}

1. 局部时空特征
2. 晚期的时空融合
3. 基于Hammerstein模型的数据扩增
4. 一个新的验证方法和度量标准

{% asset_img 26.webp %}

从全局到局部再到全局，预处理、特征提取、融合与检测

{% asset_img 27.webp %}

只选取了12个兴趣区域，眉毛、嘴角还有鼻子，鼻子作为头部运动的整体参考，区域的尺度选择是眼角内部距离的1/5。

{% asset_img 28.webp %}

分析是基于时序的，对每一帧提取ROI，然后对这些ROI序列进行基于时序上的PCA处理，图上每个点代表一帧，点的分布代表每帧视频在时域上的分布。点与点之间的距离越长，变化得越明显；点与点之间的距离越短，变化就比较微弱。

{% asset_img 29.webp %}

通过计算点与点之间的距离，来发现微表情的变化特征。这条曲线也就是局部时域特征

{% asset_img 30.webp %}

{% asset_img 31.webp %}

筛选一下，好的就是S-patterns，能比较好代表微表情特征，坏的就是Non-S-patterns

{% asset_img 32.webp %}

用SVM分类一下

{% asset_img 33.webp %}

这是第二个贡献，在检测到哪些区域发生了类似微表情的活动以后，才做一个晚期的融合。将过长或过短的检测区间去除掉。

{% asset_img 34.webp %}

鼻子作为整体参考，可能是发生了整体的头部运动。

{% asset_img 35.webp %}

{% asset_img 36.webp %}

限制：使用CASME I大概只有78个LTPs被认为是S-patterns，所以需要进行数据扩增（基于Hammerstein模型）

{% asset_img 37.webp %}

Hammerstein模型替代了LTP selection

{% asset_img 38.webp %}

（这玩意啥，没看懂，有点像机器学习，训练两个参数）

{% asset_img 39.webp %}

{% asset_img 40.webp %}

{% asset_img 41.webp %}

{% asset_img 42.webp %}

{% asset_img 43.webp %}

可以把 CASME I 变成 1w 多个样本

{% asset_img 44.webp %}

{% asset_img 45.webp %}

{% asset_img 46.webp %}

{% asset_img 47.webp %}

{% asset_img 48.webp %}

{% asset_img 49.webp %}

{% asset_img 50.webp %}

{% asset_img 51.webp %}

{% asset_img 52.webp %}

{% asset_img 53.webp %}

{% asset_img 54.webp %}

{% asset_img 55.webp %}

{% asset_img 56.webp %}

提出了一个对微表情检测的评判标准

{% asset_img 57.webp %}

{% asset_img 58.webp %}

一些对未来的展望
